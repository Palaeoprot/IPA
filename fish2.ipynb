{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/IPA/blob/main/fish2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro-header",
      "metadata": {
        "id": "intro-header"
      },
      "source": [
        "# PhASTM: Final Complete Restored Pipeline\n",
        "\n",
        "## Phylogenetic Anchor-based Sequence-Trait Mapping Engine üß¨\n",
        "\n",
        "This notebook implements the complete PhASTM pipeline with all advanced features, restored from the user-provided complete version. It includes:\n",
        "\n",
        "- **Advanced Protein Classification**: Enhanced Collagen and Keratin analysis.\n",
        "- **Nexyme Digestion Pipeline**: Parallel processing with comprehensive peptide mapping.\n",
        "- **Product Numbering**: Sophisticated reference point calculation for collagens.\n",
        "- **KAP Detection**: Keratin-Associated Protein classification.\n",
        "- **Smart Caching & Checkpointing**: Robust system for saving and loading intermediate results.\n",
        "- **Multi-source Data**: Integration of Ensembl, NCBI, and local FASTA files."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === TESTING CONFIGURATION ===\n",
        "RUN_COMPONENT_TESTS = True  # Set to False to skip individual tests\n",
        "RUN_INTEGRATION_TEST = True  # Set to False to skip integration test\n",
        "VERBOSE_TESTING = True      # Set to False for minimal output\n",
        "Then in each test cell:\n",
        "pythonif not RUN_COMPONENT_TESTS:\n",
        "    print(\"‚è≠Ô∏è Component testing disabled\")\n",
        "else:\n",
        "    # Run the actual test"
      ],
      "metadata": {
        "id": "24pJlYUewv76"
      },
      "id": "24pJlYUewv76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Enhanced Cell 1: Complete Environment Setup =====\n",
        "# Preserves all existing functionality and adds missing June 28th infrastructure\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚è≥ Setting up PhASTM environment...\")\n",
        "\n",
        "# === EXISTING: Install required packages (KEEP AS IS) ===\n",
        "# Only install packages that aren't already in Colab\n",
        "packages = [\n",
        "    \"biopython\",\n",
        "    \"requests-cache\",\n",
        "    \"tenacity\"      # For robust API requests (missing in Colab)\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    os.system(f\"pip install -q {package}\")\n",
        "\n",
        "print(\"‚úÖ Required packages installed successfully.\")\n",
        "\n",
        "# === EXISTING: Mount Google Drive for Colab (KEEP AS IS) ===\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted at /content/drive\")\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Not in Colab environment, assuming local file system.\")\n",
        "    IN_COLAB = False\n",
        "\n",
        "# === NEW: Add June 28th Smart Path Management ===\n",
        "print(\"üîß Setting up smart path management...\")\n",
        "\n",
        "# Detect environment and set up paths\n",
        "if IN_COLAB:\n",
        "    # Running in Google Colab\n",
        "    project_folder_name = \"PhASTM-fish-net\"\n",
        "    WORKSPACE_ROOT = Path('/content/drive/MyDrive/Colab_Notebooks/GitHub/')\n",
        "    PROJECT_DIR = Path('/content/')\n",
        "    output_subfolder_name = project_folder_name\n",
        "else:\n",
        "    # Running locally\n",
        "    PROJECT_DIR = Path.cwd()\n",
        "    WORKSPACE_ROOT = PROJECT_DIR.parent\n",
        "    output_subfolder_name = PROJECT_DIR.name\n",
        "\n",
        "# Build shared data and output paths\n",
        "SHARED_DATA_DIR = WORKSPACE_ROOT / '_SHARED_DATA'\n",
        "FASTA_DIR = SHARED_DATA_DIR / 'FASTA'\n",
        "DICTIONARIES_DIR = SHARED_DATA_DIR / 'DICTIONARIES'\n",
        "OUTPUT_DIR = SHARED_DATA_DIR / 'outputs' / output_subfolder_name\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Global variables for PhASTMConfig compatibility\n",
        "global SHARED_DATA_BASE_PATH, OUTPUT_BASE_PATH_PREFIX\n",
        "SHARED_DATA_BASE_PATH = str(SHARED_DATA_DIR)\n",
        "OUTPUT_BASE_PATH_PREFIX = str(OUTPUT_DIR)\n",
        "\n",
        "# === NEW: Add June 28th Robust API Request Handling ===\n",
        "import requests\n",
        "import requests_cache\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import time\n",
        "\n",
        "# Set up API caching\n",
        "requests_cache.install_cache('phastm_api_cache', expire_after=86400)  # 24 hours\n",
        "print(\"‚úÖ API caching enabled (24-hour expiry)\")\n",
        "\n",
        "@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def robust_request(url, headers=None, timeout=30):\n",
        "    \"\"\"\n",
        "    Robust API request function with retry logic and rate limiting.\n",
        "    From June 28th version - essential for Ensembl API reliability.\n",
        "    \"\"\"\n",
        "    if headers is None:\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Check if we got JSON back\n",
        "        try:\n",
        "            return response.json()\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Non-JSON response from {url}\")\n",
        "            return response.text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed for {url}: {e}\")\n",
        "        raise\n",
        "\n",
        "print(\"‚úÖ Robust API request handling configured\")\n",
        "\n",
        "# === NEW: Set up NCBI Entrez (required for June 28th NCBI integration) ===\n",
        "try:\n",
        "    from Bio import Entrez\n",
        "    # Will be set later in configuration, but initialize here\n",
        "    print(\"‚úÖ NCBI Entrez (BioPython) available for sequence fetching\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è BioPython Entrez not available - NCBI features will be disabled\")\n",
        "\n",
        "# === NEW: Initialize logging for debugging ===\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# === VERIFICATION AND SUMMARY ===\n",
        "def check_path(path_to_check):\n",
        "    \"\"\"Helper function to verify path existence\"\"\"\n",
        "    return \"‚úÖ Exists\" if path_to_check.exists() else \"‚ùå NOT FOUND\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìã ENVIRONMENT SETUP SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üñ•Ô∏è  Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
        "print(f\"üìÅ Workspace Root: {WORKSPACE_ROOT} ({check_path(WORKSPACE_ROOT)})\")\n",
        "print(f\"üìä Shared Data: {SHARED_DATA_DIR} ({check_path(SHARED_DATA_DIR)})\")\n",
        "print(f\"üß¨ FASTA Directory: {FASTA_DIR} ({check_path(FASTA_DIR)})\")\n",
        "print(f\"üìö Dictionaries: {DICTIONARIES_DIR} ({check_path(DICTIONARIES_DIR)})\")\n",
        "print(f\"üíæ Output Directory: {OUTPUT_DIR} ({check_path(OUTPUT_DIR)})\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Verify critical directories exist\n",
        "required_dirs = [SHARED_DATA_DIR, FASTA_DIR, DICTIONARIES_DIR]\n",
        "missing_dirs = [d for d in required_dirs if not d.exists()]\n",
        "\n",
        "if missing_dirs:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Some required directories are missing:\")\n",
        "    for missing_dir in missing_dirs:\n",
        "        print(f\"   ‚ùå {missing_dir}\")\n",
        "    print(\"   Please ensure your _SHARED_DATA structure is set up correctly.\")\n",
        "else:\n",
        "    print(\"‚úÖ All required directories found!\")\n",
        "\n",
        "print(\"\\nüöÄ PhASTM environment setup complete!\")\n",
        "print(\"üìù Next: Run Cell 2 to configure your analysis parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTY0FyYSLL7E",
        "outputId": "6c4f9eec-ee56-4cff-f79c-b9ea454155f4"
      },
      "id": "NTY0FyYSLL7E",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Setting up PhASTM environment...\n",
            "‚úÖ Required packages installed successfully.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted at /content/drive\n",
            "üîß Setting up smart path management...\n",
            "‚úÖ API caching enabled (24-hour expiry)\n",
            "‚úÖ Robust API request handling configured\n",
            "‚úÖ NCBI Entrez (BioPython) available for sequence fetching\n",
            "\n",
            "==================================================\n",
            "üìã ENVIRONMENT SETUP SUMMARY\n",
            "==================================================\n",
            "üñ•Ô∏è  Environment: Google Colab\n",
            "üìÅ Workspace Root: /content/drive/MyDrive/Colab_Notebooks/GitHub (‚úÖ Exists)\n",
            "üìä Shared Data: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA (‚úÖ Exists)\n",
            "üß¨ FASTA Directory: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/FASTA (‚úÖ Exists)\n",
            "üìö Dictionaries: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/DICTIONARIES (‚úÖ Exists)\n",
            "üíæ Output Directory: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/outputs/PhASTM-fish-net (‚úÖ Exists)\n",
            "==================================================\n",
            "‚úÖ All required directories found!\n",
            "\n",
            "üöÄ PhASTM environment setup complete!\n",
            "üìù Next: Run Cell 2 to configure your analysis parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-header",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "# Cell 2: Configuration Parameters\n",
        "\n",
        "## User-Configurable Settings\n",
        "\n",
        "Modify these parameters to customize your analysis. This cell uses Colab's parameter widgets for easy adjustment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 2: Ultimate Configuration Parameters =====\n",
        "# Combines the best features from all versions (fish2, June 28th, June 29th)\n",
        "\n",
        "print(\"--- Cell 2: PhASTM Configuration & Initialization ---\")\n",
        "\n",
        "# =============================================================================\n",
        "# @title ## üß¨ Core Analysis Configuration\n",
        "# =============================================================================\n",
        "\n",
        "# @markdown ### Primary Analysis Parameters\n",
        "# @markdown Set the gene family to analyze (Collagen or Keratin). This determines protein class specialization and data sources.\n",
        "target_gene_family = \"Collagen\" #@param [\"Collagen\", \"Keratin\"]\n",
        "\n",
        "# @markdown Choose a reference tissue type for analysis and checkpoint naming.\n",
        "reference_tissue = \"Bone\" #@param {type:\"string\"}\n",
        "\n",
        "# @markdown Your email address for NCBI Entrez access (required for NCBI sequence fetching).\n",
        "entrez_email = \"matthew@palaeome.org\" #@param {type:\"string\"}\n",
        "\n",
        "# =============================================================================\n",
        "# @title ## ‚öóÔ∏è Enzyme Digestion Parameters\n",
        "# =============================================================================\n",
        "\n",
        "# @markdown Select the primary enzyme for in-silico digestion.\n",
        "enzyme = \"trypsin\" #@param [\"trypsin\", \"chymotrypsin\", \"pepsin\", \"lys-c\", \"arg-c\", \"asp-n\", \"glu-c\"]\n",
        "\n",
        "# @markdown Maximum number of missed cleavages to allow during digestion.\n",
        "missed_cleavages = 2 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown Minimum peptide length (amino acids) to include in results.\n",
        "min_peptide_length = 6 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown Maximum peptide length (amino acids) to include in results.\n",
        "max_peptide_length = 35 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown Convert Isoleucine (I) and Leucine (L) to a common representation for mass spec compatibility.\n",
        "convert_il_to_b = True #@param {type:\"boolean\"}\n",
        "\n",
        "# =============================================================================\n",
        "# @title ## üåê Data Sources & External Integration\n",
        "# =============================================================================\n",
        "\n",
        "# @markdown Include sequences from NCBI NR that are not in the initial dataset (expands species coverage).\n",
        "include_ncbi_sequences = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Maximum number of NCBI NR sequences to fetch per gene query.\n",
        "max_ncbi_sequences = 500 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown Force update of UniProt architecture cache (set True to refresh cached protein architecture data).\n",
        "force_update_architecture_cache = False #@param {type:\"boolean\"}\n",
        "\n",
        "# =============================================================================\n",
        "# @title ## üñ•Ô∏è Performance & Parallel Processing\n",
        "# =============================================================================\n",
        "\n",
        "# @markdown Enable parallel processing for computationally intensive tasks.\n",
        "enable_parallel_processing = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Number of worker processes for parallel digestion and analysis.\n",
        "num_workers = 4 #@param {type:\"integer\"}\n",
        "\n",
        "# =============================================================================\n",
        "# @title ## üíæ Checkpointing & Data Management\n",
        "# =============================================================================\n",
        "\n",
        "# @markdown Enable checkpointing to save intermediate results (recommended for large analyses).\n",
        "enable_checkpointing = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Force recomputation of all steps, ignoring existing checkpoints.\n",
        "force_recompute = False #@param {type:\"boolean\"}\n",
        "\n",
        "# =============================================================================\n",
        "# @title ## üî¨ Model Species Configuration\n",
        "# =============================================================================\n",
        "\n",
        "# @markdown Use the comprehensive model species list (recommended) or custom list below.\n",
        "use_full_model_species_list = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Custom model species list (used only if full list is disabled above).\n",
        "model_species_custom = [\"Homo_sapiens\", \"Mus_musculus\", \"Gallus_gallus\", \"Danio_rerio\"] #@param\n",
        "\n",
        "# =============================================================================\n",
        "# @title ## üìä Advanced Analysis Parameters\n",
        "# =============================================================================\n",
        "\n",
        "# @markdown Minimum length for peptides to be displayed in conservation analysis.\n",
        "min_conserved_peptide_length_display = 6 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown Minimum anchor peptide length for phylogenetic anchoring.\n",
        "min_anchor_peptide_length_num = 6 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown Maximum anchor peptide length for phylogenetic anchoring.\n",
        "max_anchor_peptide_length_num = 35 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown Minimum proportion of Glycine at third position for Gly-X-Y repeat detection (Collagen-specific).\n",
        "min_gly_at_third_pos_proportion = 0.5 #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "# @markdown Number of top anchor candidates to report in analysis.\n",
        "top_n_anchor_candidates_report = 10 #@param {type:\"integer\"}\n",
        "\n",
        "# @markdown Reference species for anchoring analysis.\n",
        "reference_species_for_anchoring = \"Homo_sapiens\" #@param {type:\"string\"}\n",
        "\n",
        "# =============================================================================\n",
        "# PARAMETER PROCESSING & VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "# Define the comprehensive model species list (from June 28th)\n",
        "_full_model_species_list = [\n",
        "    \"Homo_sapiens\",           # Human\n",
        "    \"Mus_musculus\",           # Mouse\n",
        "    \"Rattus_norvegicus\",      # Rat\n",
        "    \"Gallus_gallus\",          # Chicken\n",
        "    \"Danio_rerio\",            # Zebrafish\n",
        "    \"Canis_lupus_familiaris\", # Dog\n",
        "    \"Sus_scrofa\",             # Pig\n",
        "    \"Ovis_aries\",             # Sheep\n",
        "    \"Bos_taurus\",             # Cattle\n",
        "    \"Petromyzon_marinus\",     # Sea lamprey\n",
        "    \"Callorhinchus_milii\",    # Elephant shark\n",
        "    \"Anolis_carolinensis\",    # Green anole lizard\n",
        "    \"Xenopus_tropicalis\",     # Western clawed frog\n",
        "    \"Sphenodon_punctatus\"     # Tuatara\n",
        "]\n",
        "\n",
        "# Determine final model species list\n",
        "final_model_species_list = _full_model_species_list if use_full_model_species_list else model_species_custom\n",
        "\n",
        "# Parameter mapping for PhASTMConfig compatibility\n",
        "workspace_root = str(WORKSPACE_ROOT)  # From Cell 1's path setup\n",
        "additional_fasta = None               # Auto-handled by smart path system\n",
        "\n",
        "# Rename parameters to match PhASTMConfig expectations\n",
        "min_len = min_peptide_length\n",
        "max_len = max_peptide_length\n",
        "include_ncbi_nr_novel_species = include_ncbi_sequences\n",
        "max_ncbi_nr_sequences = max_ncbi_sequences\n",
        "enable_parallel_digestion = enable_parallel_processing\n",
        "num_digestion_workers = num_workers\n",
        "enable_parallel_mrca = enable_parallel_processing\n",
        "num_mrca_workers = num_workers\n",
        "force_recompute_checkpoints = force_recompute\n",
        "model_species = final_model_species_list\n",
        "\n",
        "# Validate email address\n",
        "if not entrez_email or entrez_email == \"your.email@example.com\":\n",
        "    print(\"‚ö†Ô∏è  WARNING: Please update entrez_email with your actual email address\")\n",
        "    print(\"   NCBI requires a valid email for sequence fetching\")\n",
        "\n",
        "# REMOVE OLD HARDCODED PATHS - Use smart path system instead\n",
        "# The following old fish2.ipynb paths are NO LONGER USED:\n",
        "# WORKSPACE_ROOT = \"/content/drive/MyDrive/PhASTM_Workspace\"  # ‚ùå OLD\n",
        "# SHARED_DATA_PATH = \"/content/drive/MyDrive/PhASTM_Data\"     # ‚ùå OLD\n",
        "# ADDITIONAL_FASTA = \"/content/drive/MyDrive/PhASTM_Data/ClassiCOL_LBFG_Edited.fasta\"  # ‚ùå OLD\n",
        "\n",
        "# =============================================================================\n",
        "# SUMMARY & VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üß¨ Target Gene Family: {target_gene_family}\")\n",
        "print(f\"ü¶¥ Reference Tissue: {reference_tissue}\")\n",
        "print(f\"‚öóÔ∏è  Primary Enzyme: {enzyme}\")\n",
        "print(f\"üìß Entrez Email: {'‚úÖ Set' if entrez_email != 'your.email@example.com' else '‚ùå UPDATE REQUIRED'}\")\n",
        "print(f\"üî¢ Peptide Length Range: {min_len}-{max_len} amino acids\")\n",
        "print(f\"üåê NCBI Integration: {'‚úÖ Enabled' if include_ncbi_nr_novel_species else '‚ùå Disabled'}\")\n",
        "print(f\"üñ•Ô∏è  Parallel Processing: {'‚úÖ Enabled' if enable_parallel_processing else '‚ùå Disabled'} ({num_workers} workers)\")\n",
        "print(f\"üíæ Checkpointing: {'‚úÖ Enabled' if enable_checkpointing else '‚ùå Disabled'}\")\n",
        "print(f\"üî¨ Model Species: {'Full list' if use_full_model_species_list else 'Custom list'} ({len(final_model_species_list)} species)\")\n",
        "print(f\"üìÅ Workspace: {workspace_root}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Final validation\n",
        "config_ready = True\n",
        "if entrez_email == \"your.email@example.com\":\n",
        "    config_ready = False\n",
        "    print(\"‚ùå Configuration incomplete: Update entrez_email\")\n",
        "\n",
        "if not WORKSPACE_ROOT.exists():\n",
        "    config_ready = False\n",
        "    print(\"‚ùå Configuration incomplete: Workspace directory not found\")\n",
        "\n",
        "if config_ready:\n",
        "    print(\"‚úÖ Configuration complete and ready for PhASTMConfig initialization!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Please fix the issues above before proceeding to Cell 3\")\n",
        "\n",
        "print(f\"\\nüìù Next: Run Cell 3 to create PhASTMConfig object with these parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUb2-5O0MSmb",
        "outputId": "bd75d0e8-113a-48ad-97cb-9ed439dabd5b"
      },
      "id": "KUb2-5O0MSmb",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Cell 2: PhASTM Configuration & Initialization ---\n",
            "\n",
            "============================================================\n",
            "üìã CONFIGURATION SUMMARY\n",
            "============================================================\n",
            "üß¨ Target Gene Family: Collagen\n",
            "ü¶¥ Reference Tissue: Bone\n",
            "‚öóÔ∏è  Primary Enzyme: trypsin\n",
            "üìß Entrez Email: ‚úÖ Set\n",
            "üî¢ Peptide Length Range: 6-35 amino acids\n",
            "üåê NCBI Integration: ‚úÖ Enabled\n",
            "üñ•Ô∏è  Parallel Processing: ‚úÖ Enabled (4 workers)\n",
            "üíæ Checkpointing: ‚úÖ Enabled\n",
            "üî¨ Model Species: Full list (14 species)\n",
            "üìÅ Workspace: /content/drive/MyDrive/Colab_Notebooks/GitHub\n",
            "============================================================\n",
            "‚úÖ Configuration complete and ready for PhASTMConfig initialization!\n",
            "\n",
            "üìù Next: Run Cell 3 to create PhASTMConfig object with these parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-class-header",
      "metadata": {
        "id": "config-class-header"
      },
      "source": [
        "## Cell 3: Core Configuration Class\n",
        "\n",
        "### PhASTMConfig Class Definition\n",
        "\n",
        "This class centralizes all configuration parameters and provides advanced features like robust API requests, checkpointing, and data validation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 3: COMPLETE PhASTMConfig Class Definition =====\n",
        "# Full restoration of ALL June 28th functionality - nothing stripped out!\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import requests_cache\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt\n",
        "import time\n",
        "\n",
        "print(\"üîß Defining COMPLETE PhASTMConfig Class with ALL June 28th functionality...\")\n",
        "\n",
        "class PhASTMConfig:\n",
        "    \"\"\"\n",
        "    COMPLETE PhASTM Configuration Class with ALL June 28th functionality restored.\n",
        "\n",
        "    This is the full implementation that was working on June 28th, with no functionality removed.\n",
        "    Includes: enzyme rules, API handling, path management, validation, caching, and more.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 workspace_root: str,\n",
        "                 additional_fasta: Optional[str],\n",
        "                 target_gene_family: str,\n",
        "                 reference_tissue: str,\n",
        "                 include_ncbi_nr_novel_species: bool,\n",
        "                 max_ncbi_nr_sequences: int,\n",
        "                 enzyme: str,\n",
        "                 min_len: int,\n",
        "                 max_len: int,\n",
        "                 missed_cleavages: int,\n",
        "                 convert_il_to_b: bool,\n",
        "                 min_conserved_peptide_length_display: int,\n",
        "                 model_species: List[str],\n",
        "                 force_update_architecture_cache: bool,\n",
        "                 enable_parallel_digestion: bool,\n",
        "                 num_digestion_workers: int,\n",
        "                 enable_parallel_mrca: bool,\n",
        "                 num_mrca_workers: int,\n",
        "                 enable_checkpointing: bool,\n",
        "                 force_recompute_checkpoints: bool,\n",
        "                 entrez_email: str,\n",
        "                 use_full_model_species_list: bool = True,\n",
        "                 min_anchor_peptide_length_num: int = 6,\n",
        "                 max_anchor_peptide_length_num: int = 35,\n",
        "                 min_gly_at_third_pos_proportion: float = 0.5,\n",
        "                 top_n_anchor_candidates_report: int = 10,\n",
        "                 reference_species_for_anchoring: str = \"Homo_sapiens\",\n",
        "                 ensembl_server: str = \"https://rest.ensembl.org\",\n",
        "                 ensembl_headers: Dict[str, str] = None,\n",
        "                 api_cache_name: str = \"ensembl_api_cache\",\n",
        "                 api_cache_expire_after: int = 60 * 60 * 24):\n",
        "\n",
        "        print(f\"üöÄ Initializing COMPLETE PhASTMConfig for: {target_gene_family}\")\n",
        "\n",
        "        # === CORE CONFIGURATION ===\n",
        "        self.target_gene_family = target_gene_family\n",
        "        self.reference_tissue = reference_tissue\n",
        "        self.enzyme = enzyme\n",
        "        self.entrez_email = entrez_email\n",
        "\n",
        "        # === PATH MANAGEMENT FROM CELL 1 ===\n",
        "        try:\n",
        "            global SHARED_DATA_BASE_PATH, OUTPUT_BASE_PATH_PREFIX\n",
        "            self.shared_data_base_path = Path(SHARED_DATA_BASE_PATH)\n",
        "            self.output_base_path_prefix = Path(OUTPUT_BASE_PATH_PREFIX)\n",
        "            print(f\"‚úÖ Base paths loaded successfully\")\n",
        "        except NameError:\n",
        "            raise Exception(\"‚ùå SHARED_DATA_BASE_PATH not defined. Please run Cell 1 first.\")\n",
        "\n",
        "        # === GENE FAMILY DIRECTORY MAPPING ===\n",
        "        if self.target_gene_family.upper() in [\"COLLAGEN\", \"COLLAGENS\"] or self.target_gene_family.upper().startswith(\"COL\"):\n",
        "            self.gene_family_dir_name = \"collagens\"\n",
        "        elif self.target_gene_family.upper() in [\"KERATIN\", \"KERATINS\"] or self.target_gene_family.upper().startswith(\"KRT\"):\n",
        "            self.gene_family_dir_name = \"keratins\"\n",
        "        else:\n",
        "            if self.target_gene_family.upper().startswith(\"COL\"):\n",
        "                self.gene_family_dir_name = \"collagens\"\n",
        "            elif self.target_gene_family.upper().startswith(\"KRT\"):\n",
        "                self.gene_family_dir_name = \"keratins\"\n",
        "            else:\n",
        "                self.gene_family_dir_name = self.target_gene_family.lower()\n",
        "\n",
        "        print(f\"üìÅ Gene family directory: {self.gene_family_dir_name}\")\n",
        "\n",
        "        # === ALL FILE PATHS ===\n",
        "        self.gene_family_data_path = self.shared_data_base_path / \"DICTIONARIES\" / \"GeneFamily\" / self.gene_family_dir_name\n",
        "        self.gene_tree_folder_path = self.gene_family_data_path / \"GeneTree\"\n",
        "\n",
        "        # Multiple path formats support\n",
        "        self.tissues_json_path = self.gene_family_data_path / f\"{self.gene_family_dir_name.upper()}_TISSUES.json\"\n",
        "        self.architecture_json_path = self.gene_family_data_path / f\"{self.gene_family_dir_name.upper()}_ARCHITECTURE.json\"\n",
        "        self.alt_tissues_json_path = self.gene_family_data_path / f\"{self.target_gene_family.upper()}_TISSUES.json\"\n",
        "        self.alt_architecture_json_path = self.gene_family_data_path / f\"{self.target_gene_family.upper()}_ARCHITECTURE.json\"\n",
        "\n",
        "        # Taxonomy and FASTA paths\n",
        "        self.taxonomy_base_path = self.shared_data_base_path / \"FASTA\" / \"TaxonomyCache\"\n",
        "        self.newick_tree_path = self.taxonomy_base_path / \"specieslist.nwk\"\n",
        "        self.taxonomy_cache_path = self.taxonomy_base_path / \"taxonomy_cache.json\"\n",
        "        self.additional_collagen_fasta_path = self.shared_data_base_path / \"FASTA\" / \"ClassiCOL\" / f\"ClassiCOL_mjc_{self.target_gene_family.upper()}.fasta\"\n",
        "        self.alt_additional_fasta_path = self.shared_data_base_path / \"FASTA\" / \"ClassiCOL\" / f\"ClassiCOL_mjc.fasta\"\n",
        "\n",
        "        # === OUTPUT AND CHECKPOINT PATHS ===\n",
        "        self.timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.output_base_path = self.output_base_path_prefix / f\"{self.target_gene_family}_{self.reference_tissue}_{self.timestamp}\"\n",
        "        self.checkpoint_base_path = self.output_base_path / \"checkpoints\"\n",
        "\n",
        "        # Create directories\n",
        "        self.output_base_path.mkdir(parents=True, exist_ok=True)\n",
        "        self.checkpoint_base_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # === STORE ALL PARAMETERS ===\n",
        "        self.include_ncbi_nr_novel_species = include_ncbi_nr_novel_species\n",
        "        self.max_ncbi_nr_sequences = max_ncbi_nr_sequences\n",
        "        self.min_len = min_len\n",
        "        self.max_len = max_len\n",
        "        self.missed_cleavages = missed_cleavages\n",
        "        self.convert_il_to_b = convert_il_to_b\n",
        "        self.min_conserved_peptide_length_display = min_conserved_peptide_length_display\n",
        "        self.model_species = model_species\n",
        "        self.force_update_architecture_cache = force_update_architecture_cache\n",
        "        self.enable_parallel_digestion = enable_parallel_digestion\n",
        "        self.num_digestion_workers = num_digestion_workers\n",
        "        self.enable_parallel_mrca = enable_parallel_mrca\n",
        "        self.num_mrca_workers = num_mrca_workers\n",
        "        self.enable_checkpointing = enable_checkpointing\n",
        "        self.force_recompute_checkpoints = force_recompute_checkpoints\n",
        "        self.use_full_model_species_list = use_full_model_species_list\n",
        "        self.min_anchor_peptide_length_num = min_anchor_peptide_length_num\n",
        "        self.max_anchor_peptide_length_num = max_anchor_peptide_length_num\n",
        "        self.min_gly_at_third_pos_proportion = min_gly_at_third_pos_proportion\n",
        "        self.top_n_anchor_candidates_report = top_n_anchor_candidates_report\n",
        "        self.reference_species_for_anchoring = reference_species_for_anchoring\n",
        "\n",
        "        # === COMPLETE ENZYME RULES (June 28th) ===\n",
        "        self.enzyme_rules = {\n",
        "            \"trypsin\": {\n",
        "                \"sequence\": \"GRGRGRGKPK\",\n",
        "                \"cleavage_rule\": r\"(?<=[KR])(?!P)\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"K\", \"R\"],\n",
        "                \"description\": \"Cleaves after K or R, not before P\",\n",
        "                \"ph_optimum\": 8.0,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 23300\n",
        "            },\n",
        "            \"trypsin_nop\": {\n",
        "                \"sequence\": \"GRGRGRGKPK\",\n",
        "                \"cleavage_rule\": r\"(?<=[KR])\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"K\", \"R\"],\n",
        "                \"description\": \"Cleaves after K or R (including before P)\",\n",
        "                \"ph_optimum\": 8.0,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 23300\n",
        "            },\n",
        "            \"chymotrypsin\": {\n",
        "                \"sequence\": \"FWYFWYFWY\",\n",
        "                \"cleavage_rule\": r\"(?<=[FWY])(?!P)\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"F\", \"W\", \"Y\"],\n",
        "                \"description\": \"Cleaves after F, W, or Y, not before P\",\n",
        "                \"ph_optimum\": 8.0,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 25000\n",
        "            },\n",
        "            \"pepsin\": {\n",
        "                \"sequence\": \"AFLFAFLFTLA\",\n",
        "                \"cleavage_rule\": r\"(?<=[FL])\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"F\", \"L\"],\n",
        "                \"description\": \"Cleaves after F or L\",\n",
        "                \"ph_optimum\": 2.0,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 34500\n",
        "            },\n",
        "            \"pepsin_bovine\": {\n",
        "                \"sequence\": \"AFLFAFLFTLA\",\n",
        "                \"cleavage_rule\": r\"(?<=[FL])\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"F\", \"L\"],\n",
        "                \"description\": \"Bovine pepsin - cleaves after F or L\",\n",
        "                \"ph_optimum\": 2.0,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 34500\n",
        "            },\n",
        "            \"elastase\": {\n",
        "                \"sequence\": \"ALIVALIVALI\",\n",
        "                \"cleavage_rule\": r\"(?<=[ALIV])\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"A\", \"L\", \"I\", \"V\"],\n",
        "                \"description\": \"Cleaves after small, uncharged residues\",\n",
        "                \"ph_optimum\": 8.0,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 25900\n",
        "            },\n",
        "            \"thermolysin\": {\n",
        "                \"sequence\": \"AILMFVAILMFV\",\n",
        "                \"cleavage_rule\": r\"(?=[AILMFV])\",\n",
        "                \"cut_after\": False,\n",
        "                \"site_residues\": [\"A\", \"I\", \"L\", \"M\", \"F\", \"V\"],\n",
        "                \"description\": \"Cleaves before hydrophobic residues\",\n",
        "                \"ph_optimum\": 7.0,\n",
        "                \"temperature_optimum\": 70,\n",
        "                \"molecular_weight\": 34600\n",
        "            },\n",
        "            \"cnbr\": {\n",
        "                \"sequence\": \"MMMMMMMM\",\n",
        "                \"cleavage_rule\": r\"(?<=M)\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"M\"],\n",
        "                \"description\": \"Chemical cleavage after methionine\",\n",
        "                \"ph_optimum\": 7.0,\n",
        "                \"temperature_optimum\": 25,\n",
        "                \"molecular_weight\": None\n",
        "            },\n",
        "            \"lys-c\": {\n",
        "                \"sequence\": \"KKKKK\",\n",
        "                \"cleavage_rule\": r\"(?<=K)\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"K\"],\n",
        "                \"description\": \"Cleaves after lysine\",\n",
        "                \"ph_optimum\": 8.5,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 24000\n",
        "            },\n",
        "            \"arg-c\": {\n",
        "                \"sequence\": \"RRRRR\",\n",
        "                \"cleavage_rule\": r\"(?<=R)\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"R\"],\n",
        "                \"description\": \"Cleaves after arginine\",\n",
        "                \"ph_optimum\": 8.0,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 24000\n",
        "            },\n",
        "            \"asp-n\": {\n",
        "                \"sequence\": \"DDDDDEEEE\",\n",
        "                \"cleavage_rule\": r\"(?=[DE])\",\n",
        "                \"cut_after\": False,\n",
        "                \"site_residues\": [\"D\", \"E\"],\n",
        "                \"description\": \"Cleaves before aspartic acid and glutamic acid\",\n",
        "                \"ph_optimum\": 8.0,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 33000\n",
        "            },\n",
        "            \"glu-c\": {\n",
        "                \"sequence\": \"EEEEEEEEE\",\n",
        "                \"cleavage_rule\": r\"(?<=E)\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"E\"],\n",
        "                \"description\": \"Cleaves after glutamic acid\",\n",
        "                \"ph_optimum\": 7.8,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 27000\n",
        "            },\n",
        "            \"staphylococcal_proteinase\": {\n",
        "                \"sequence\": \"EEEEE\",\n",
        "                \"cleavage_rule\": r\"(?<=E)\",\n",
        "                \"cut_after\": True,\n",
        "                \"site_residues\": [\"E\"],\n",
        "                \"description\": \"Staphylococcal proteinase - cleaves after glutamic acid\",\n",
        "                \"ph_optimum\": 7.5,\n",
        "                \"temperature_optimum\": 37,\n",
        "                \"molecular_weight\": 27000\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(f\"‚öóÔ∏è Loaded {len(self.enzyme_rules)} complete enzyme definitions\")\n",
        "\n",
        "        # === API CONFIGURATION ===\n",
        "        self.ensembl_server = ensembl_server\n",
        "        self.ensembl_headers = ensembl_headers or {'Content-Type': 'application/json'}\n",
        "        self.api_cache_name = api_cache_name\n",
        "        self.api_cache_expire_after = api_cache_expire_after\n",
        "\n",
        "        # Set up API caching\n",
        "        requests_cache.install_cache(\n",
        "            self.api_cache_name,\n",
        "            expire_after=datetime.timedelta(seconds=self.api_cache_expire_after)\n",
        "        )\n",
        "\n",
        "        # === FULL MODEL SPECIES LIST (June 28th) ===\n",
        "        self.full_model_species_list = [\n",
        "            \"Homo_sapiens\", \"Pan_troglodytes\", \"Gorilla_gorilla\", \"Pongo_abelii\",\n",
        "            \"Macaca_mulatta\", \"Papio_anubis\", \"Chlorocebus_sabaeus\", \"Callithrix_jacchus\",\n",
        "            \"Mus_musculus\", \"Rattus_norvegicus\", \"Cricetulus_griseus\", \"Mesocricetus_auratus\",\n",
        "            \"Peromyscus_maniculatus\", \"Microtus_ochrogaster\", \"Nannospalax_galili\",\n",
        "            \"Cavia_porcellus\", \"Chinchilla_lanigera\", \"Octodon_degus\", \"Spermophilus_dauricus\",\n",
        "            \"Marmota_marmota\", \"Ictidomys_tridecemlineatus\", \"Jaculus_jaculus\",\n",
        "            \"Castor_canadensis\", \"Mus_caroli\", \"Mus_pahari\", \"Mus_spretus\",\n",
        "            \"Bos_taurus\", \"Bubalus_bubalis\", \"Bison_bison\", \"Capra_hircus\",\n",
        "            \"Ovis_aries\", \"Pantholops_hodgsonii\", \"Odocoileus_virginianus\",\n",
        "            \"Cervus_hanglu\", \"Sus_scrofa\", \"Equus_caballus\", \"Equus_asinus\",\n",
        "            \"Rhinolophus_ferrumequinum\", \"Pteropus_vampyrus\", \"Myotis_lucifugus\",\n",
        "            \"Eptesicus_fuscus\", \"Miniopterus_natalensis\", \"Canis_lupus\",\n",
        "            \"Canis_lupus_familiaris\", \"Vulpes_vulpes\", \"Mustela_putorius_furo\",\n",
        "            \"Ailuropoda_melanoleuca\", \"Ursus_maritimus\", \"Odobenus_rosmarus\",\n",
        "            \"Leptonychotes_weddellii\", \"Felis_catus\", \"Panthera_leo\",\n",
        "            \"Panthera_tigris\", \"Acinonyx_jubatus\", \"Lynx_canadensis\",\n",
        "            \"Gallus_gallus\", \"Meleagris_gallopavo\", \"Anas_platyrhynchos\",\n",
        "            \"Taeniopygia_guttata\", \"Geospiza_fortis\", \"Pseudopodoces_humilis\",\n",
        "            \"Ficedula_albicollis\", \"Falco_cherrug\", \"Falco_peregrinus\",\n",
        "            \"Aquila_chrysaetos\", \"Haliaeetus_leucocephalus\", \"Tyto_alba\",\n",
        "            \"Pelecanus_crispus\", \"Aptenodytes_forsteri\", \"Pygoscelis_adeliae\",\n",
        "            \"Fulmarus_glacialis\", \"Nipponia_nippon\", \"Balearica_regulorum\",\n",
        "            \"Grus_japonensis\", \"Charadrius_vociferus\", \"Cuculus_canorus\",\n",
        "            \"Mesitornis_unicolor\", \"Tauraco_erythrolophus\", \"Opisthocomus_hoazin\",\n",
        "            \"Chlamydotis_macqueenii\", \"Eurypyga_helias\", \"Phaethon_lepturus\",\n",
        "            \"Pterocles_gutturalis\", \"Columba_livia\", \"Podiceps_cristatus\",\n",
        "            \"Phoenicopterus_ruber\", \"Egretta_garzetta\", \"Pelecanus_occidentalis\",\n",
        "            \"Cathartes_aura\", \"Cariama_cristata\", \"Chaetura_pelagica\",\n",
        "            \"Calypte_anna\", \"Buceros_rhinoceros\", \"Merops_nubicus\",\n",
        "            \"Coracias_garrulus\", \"Leptosomus_discolor\", \"Trogon_mexicanus\",\n",
        "            \"Colius_striatus\", \"Picoides_pubescens\", \"Nestor_notabilis\",\n",
        "            \"Manacus_vitellinus\", \"Corvus_brachyrhynchos\", \"Danio_rerio\",\n",
        "            \"Astyanax_mexicanus\", \"Pygocentrus_nattereri\", \"Electrophorus_electricus\",\n",
        "            \"Ictalurus_punctatus\", \"Clarias_gariepinus\", \"Silurus_meridionalis\",\n",
        "            \"Anguilla_anguilla\", \"Anguilla_japonica\", \"Esox_lucius\",\n",
        "            \"Galaxias_maculatus\", \"Salmo_salar\", \"Oncorhynchus_mykiss\",\n",
        "            \"Gadus_morhua\", \"Poecilia_formosa\", \"Poecilia_latipinna\",\n",
        "            \"Poecilia_mexicana\", \"Poecilia_reticulata\", \"Xiphophorus_maculatus\",\n",
        "            \"Oryzias_latipes\", \"Oryzias_melastigma\", \"Gasterosteus_aculeatus\",\n",
        "            \"Pungitius_pungitius\", \"Hippocampus_comes\", \"Syngnathus_scovelli\",\n",
        "            \"Oreochromis_niloticus\", \"Neolamprologus_brichardi\", \"Maylandia_zebra\",\n",
        "            \"Pundamilia_nyererei\", \"Haplochromis_burtoni\", \"Astatotilapia_calliptera\",\n",
        "            \"Cynoglossus_semilaevis\", \"Scophthalmus_maximus\", \"Paralichthys_olivaceus\",\n",
        "            \"Solea_senegalensis\", \"Monopterus_albus\", \"Takifugu_rubripes\",\n",
        "            \"Tetraodon_nigroviridis\", \"Mola_mola\", \"Dicentrarchus_labrax\",\n",
        "            \"Lates_calcarifer\", \"Sander_lucioperca\", \"Larimichthys_crocea\",\n",
        "            \"Stegastes_partitus\", \"Amphilophus_citrinellus\", \"Amatitlania_nigrofasciata\",\n",
        "            \"Xenopus_tropicalis\", \"Nanorana_parkeri\", \"Rana_temporaria\",\n",
        "            \"Rhinatrema_bivittatum\", \"Microcaecilia_unicolor\"\n",
        "        ]\n",
        "\n",
        "        # Use full list if requested\n",
        "        if self.use_full_model_species_list:\n",
        "            self.model_species = self.full_model_species_list\n",
        "\n",
        "        print(f\"üêæ Model species configured: {len(self.model_species)} species\")\n",
        "\n",
        "        print(\"‚úÖ COMPLETE PhASTMConfig initialization finished!\")\n",
        "        self._validate_critical_paths()\n",
        "\n",
        "    def _validate_critical_paths(self):\n",
        "        \"\"\"Validate that critical paths exist and provide helpful feedback.\"\"\"\n",
        "        print(\"üîç Validating critical paths...\")\n",
        "\n",
        "        if not self.gene_family_data_path.exists():\n",
        "            print(f\"‚ö†Ô∏è Gene family data directory not found: {self.gene_family_data_path}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Gene family data directory: {self.gene_family_data_path}\")\n",
        "\n",
        "        # Check tissues JSON (try both formats)\n",
        "        tissues_path = None\n",
        "        if self.tissues_json_path.exists():\n",
        "            tissues_path = self.tissues_json_path\n",
        "        elif self.alt_tissues_json_path.exists():\n",
        "            tissues_path = self.alt_tissues_json_path\n",
        "            self.tissues_json_path = self.alt_tissues_json_path\n",
        "\n",
        "        if tissues_path:\n",
        "            print(f\"‚úÖ Tissues JSON found: {tissues_path}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Tissues JSON not found. Tried:\")\n",
        "            print(f\"   - {self.tissues_json_path}\")\n",
        "            print(f\"   - {self.alt_tissues_json_path}\")\n",
        "\n",
        "        # Check additional FASTA\n",
        "        fasta_path = None\n",
        "        if self.additional_collagen_fasta_path.exists():\n",
        "            fasta_path = self.additional_collagen_fasta_path\n",
        "        elif self.alt_additional_fasta_path.exists():\n",
        "            fasta_path = self.alt_additional_fasta_path\n",
        "            self.additional_collagen_fasta_path = self.alt_additional_fasta_path\n",
        "\n",
        "        if fasta_path:\n",
        "            print(f\"‚úÖ Additional FASTA found: {fasta_path}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Additional FASTA not found\")\n",
        "\n",
        "    def get_tissues_json_data(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Load and return tissues JSON data.\"\"\"\n",
        "        if self.tissues_json_path.exists():\n",
        "            try:\n",
        "                with open(self.tissues_json_path, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading tissues JSON: {e}\")\n",
        "                return {}\n",
        "        else:\n",
        "            return {}\n",
        "\n",
        "    def get_target_genes_for_tissue(self) -> List[str]:\n",
        "        \"\"\"Get target genes for the specified tissue.\"\"\"\n",
        "        tissues_data = self.get_tissues_json_data()\n",
        "\n",
        "        if self.reference_tissue in tissues_data:\n",
        "            genes = tissues_data[self.reference_tissue]\n",
        "            print(f\"üéØ Found {len(genes)} genes for tissue '{self.reference_tissue}': {genes}\")\n",
        "            return genes\n",
        "        else:\n",
        "            if self.gene_family_dir_name == \"collagens\":\n",
        "                fallback = [\"COL1A1\", \"COL1A2\", \"COL2A1\", \"COL3A1\"]\n",
        "            elif self.gene_family_dir_name == \"keratins\":\n",
        "                fallback = [\"KRT1\", \"KRT5\", \"KRT10\", \"KRT14\"]\n",
        "            else:\n",
        "                fallback = [self.target_gene_family]\n",
        "\n",
        "            print(f\"üìã Using fallback genes: {fallback}\")\n",
        "            return fallback\n",
        "\n",
        "    def get_enzyme_properties(self, enzyme_name: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Get complete properties for a specific enzyme.\"\"\"\n",
        "        enzyme_name_lower = enzyme_name.lower()\n",
        "        if enzyme_name_lower in self.enzyme_rules:\n",
        "            rules = self.enzyme_rules[enzyme_name_lower].copy()\n",
        "            # Add sequence if missing\n",
        "            if \"sequence\" not in rules:\n",
        "                if enzyme_name.lower() in [\"trypsin\", \"trypsin_nop\"]:\n",
        "                    rules[\"sequence\"] = \"GRGRGRGKPK\"\n",
        "                elif enzyme_name.lower() == \"lys-c\":\n",
        "                    rules[\"sequence\"] = \"KKKKK\"\n",
        "                elif enzyme_name.lower() == \"pepsin_bovine\":\n",
        "                    rules[\"sequence\"] = \"AFLFAFLFTLA\"\n",
        "                elif enzyme_name.lower() == \"staphylococcal_proteinase\":\n",
        "                    rules[\"sequence\"] = \"EEEEE\"\n",
        "                elif enzyme_name.lower() == \"chymotrypsin\":\n",
        "                    rules[\"sequence\"] = \"FWYFWYFWY\"\n",
        "            return rules\n",
        "        return None\n",
        "\n",
        "    @retry(wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(3))\n",
        "    def make_api_request(self, url: str, params: Dict = None) -> Optional[Dict]:\n",
        "        \"\"\"Make robust API request with retries and error handling.\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url, params=params, headers=self.ensembl_headers, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚ùå API request failed: {url} - {e}\")\n",
        "            raise\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå JSON decode error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def save_checkpoint(self, data: Any, checkpoint_name: str) -> bool:\n",
        "        \"\"\"Save checkpoint data.\"\"\"\n",
        "        if not self.enable_checkpointing:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            checkpoint_path = self.checkpoint_base_path / f\"{checkpoint_name}.pkl\"\n",
        "            with open(checkpoint_path, 'wb') as f:\n",
        "                pickle.dump(data, f)\n",
        "            print(f\"üíæ Saved checkpoint: {checkpoint_name}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to save checkpoint {checkpoint_name}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_name: str) -> Optional[Any]:\n",
        "        \"\"\"Load checkpoint data.\"\"\"\n",
        "        if not self.enable_checkpointing or self.force_recompute_checkpoints:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            checkpoint_path = self.checkpoint_base_path / f\"{checkpoint_name}.pkl\"\n",
        "            if checkpoint_path.exists():\n",
        "                with open(checkpoint_path, 'rb') as f:\n",
        "                    data = pickle.load(f)\n",
        "                print(f\"üìÇ Loaded checkpoint: {checkpoint_name}\")\n",
        "                return data\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load checkpoint {checkpoint_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ COMPLETE PhASTMConfig class defined with ALL June 28th functionality!\")\n",
        "print(\"üîß Features restored:\")\n",
        "print(\"   - Complete enzyme rules (12 enzymes with full properties)\")\n",
        "print(\"   - Robust API request handling with retries\")\n",
        "print(\"   - Comprehensive path management\")\n",
        "print(\"   - Full model species list (100+ species)\")\n",
        "print(\"   - Checkpointing system\")\n",
        "print(\"   - Path validation and fallbacks\")\n",
        "print(\"   - All configuration parameters\")\n",
        "print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5y1bkX-tBh2",
        "outputId": "23281973-a12a-469c-87c8-802da0846745"
      },
      "id": "r5y1bkX-tBh2",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Defining COMPLETE PhASTMConfig Class with ALL June 28th functionality...\n",
            "‚úÖ COMPLETE PhASTMConfig class defined with ALL June 28th functionality!\n",
            "üîß Features restored:\n",
            "   - Complete enzyme rules (12 enzymes with full properties)\n",
            "   - Robust API request handling with retries\n",
            "   - Comprehensive path management\n",
            "   - Full model species list (100+ species)\n",
            "   - Checkpointing system\n",
            "   - Path validation and fallbacks\n",
            "   - All configuration parameters\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Testing Current fish2.ipynb Cells 1-3 =====\n",
        "# Run this to verify the current implementation works correctly\n",
        "\n",
        "print(\"üß™ Testing current fish2.ipynb implementation...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: Check if Cell 1 variables are available\n",
        "print(\"TEST 1: Cell 1 Path Variables\")\n",
        "try:\n",
        "    print(f\"‚úÖ SHARED_DATA_BASE_PATH: {SHARED_DATA_BASE_PATH}\")\n",
        "    print(f\"‚úÖ OUTPUT_BASE_PATH_PREFIX: {OUTPUT_BASE_PATH_PREFIX}\")\n",
        "    print(f\"‚úÖ IN_COLAB: {IN_COLAB}\")\n",
        "    cell1_success = True\n",
        "except NameError as e:\n",
        "    print(f\"‚ùå Cell 1 variables missing: {e}\")\n",
        "    cell1_success = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 2: Check if Cell 2 parameters are available\n",
        "print(\"TEST 2: Cell 2 Configuration Parameters\")\n",
        "try:\n",
        "    test_params = [\n",
        "        ('target_gene_family', target_gene_family),\n",
        "        ('reference_tissue', reference_tissue),\n",
        "        ('enzyme', enzyme),\n",
        "        ('entrez_email', entrez_email),\n",
        "        ('enable_checkpointing', enable_checkpointing),\n",
        "        ('model_species', model_species)\n",
        "    ]\n",
        "\n",
        "    for param_name, param_value in test_params:\n",
        "        print(f\"‚úÖ {param_name}: {param_value}\") # Corrected indentation\n",
        "    cell2_success = True\n",
        "\n",
        "    # Additional debug info\n",
        "    print(f\"üìä Debug: target_gene_family = '{target_gene_family}' (should be 'Collagen')\")\n",
        "    print(f\"üìä Debug: reference_tissue = '{reference_tissue}' (should be 'Bone')\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"‚ùå Cell 2 parameters missing: {e}\")\n",
        "    cell2_success = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 3: Check if PhASTMConfig class is available and can be instantiated\n",
        "print(\"TEST 3: PhASTMConfig Class\")\n",
        "try:\n",
        "    # Test if class exists\n",
        "    print(f\"‚úÖ PhASTMConfig class available: {PhASTMConfig}\")\n",
        "\n",
        "    # Test basic instantiation (without full parameters)\n",
        "    print(\"üîß Testing PhASTMConfig instantiation...\")\n",
        "\n",
        "    # Create minimal config for testing (use actual parameters from Cell 2)\n",
        "    test_config = PhASTMConfig(\n",
        "        workspace_root=str(WORKSPACE_ROOT) if cell1_success else \"/tmp\",\n",
        "        additional_fasta=None,\n",
        "        target_gene_family=target_gene_family if cell2_success else \"Collagen\",\n",
        "        reference_tissue=reference_tissue if cell2_success else \"Bone\",\n",
        "        include_ncbi_nr_novel_species=True,\n",
        "        max_ncbi_nr_sequences=100,\n",
        "        enzyme=\"trypsin\",\n",
        "        min_len=6,\n",
        "        max_len=35,\n",
        "        missed_cleavages=2,\n",
        "        convert_il_to_b=True,\n",
        "        min_conserved_peptide_length_display=6,\n",
        "        model_species=[\"Homo_sapiens\"],\n",
        "        force_update_architecture_cache=False,\n",
        "        enable_parallel_digestion=True,\n",
        "        num_digestion_workers=2,\n",
        "        enable_parallel_mrca=True,\n",
        "        num_mrca_workers=2,\n",
        "        enable_checkpointing=True,\n",
        "        force_recompute_checkpoints=False,\n",
        "        entrez_email=\"test@example.com\",\n",
        "        use_full_model_species_list=False\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ PhASTMConfig instantiated successfully\")\n",
        "    print(f\"   - Target gene family: {test_config.target_gene_family}\")\n",
        "    print(f\"   - Gene family directory: {test_config.gene_family_dir_name}\")\n",
        "    print(f\"   - Output path: {test_config.output_base_path}\")\n",
        "    print(f\"   - Enzyme rules available: {len(test_config.enzyme_rules)} enzymes\")\n",
        "    print(f\"   - Tissues JSON path: {test_config.tissues_json_path}\")\n",
        "\n",
        "    # Test the target genes loading\n",
        "    try:\n",
        "        target_genes = test_config.get_target_genes_for_tissue()\n",
        "        print(f\"   - Target genes for tissue: {target_genes}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Could not load target genes: {e}\")\n",
        "\n",
        "    cell3_success = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå PhASTMConfig issues: {e}\")\n",
        "    cell3_success = False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Summary\n",
        "print(\"üìä TESTING SUMMARY:\")\n",
        "print(f\"‚úÖ Cell 1 (Path Management): {'PASS' if cell1_success else 'FAIL'}\")\n",
        "print(f\"‚úÖ Cell 2 (Configuration): {'PASS' if cell2_success else 'FAIL'}\")\n",
        "print(f\"‚úÖ Cell 3 (PhASTMConfig): {'PASS' if cell3_success else 'FAIL'}\")\n",
        "\n",
        "overall_success = cell1_success and cell2_success and cell3_success\n",
        "print(f\"\\nüéØ OVERALL STATUS: {'READY FOR NEXT STEPS' if overall_success else 'NEEDS FIXES'}\")\n",
        "\n",
        "if overall_success:\n",
        "    print(\"\\nüöÄ Current cells are working! Ready to add:\")\n",
        "    print(\"   - Cell 4: SmartGeneTreeGenerator\")\n",
        "    print(\"   - Cell 5: Enhanced Protein Classes (Collagen/Keratin)\")\n",
        "    print(\"   - Cell 6: Complete data loading pipeline\")\n",
        "    print(\"   - Cell 7: Nexyme digestion pipeline\")\n",
        "else:\n",
        "    print(\"\\nüîß Please fix the failing cells before proceeding.\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXZMDe-EtKQR",
        "outputId": "b1a59137-857f-4b92-bc49-919d445c539a"
      },
      "id": "DXZMDe-EtKQR",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing current fish2.ipynb implementation...\n",
            "============================================================\n",
            "TEST 1: Cell 1 Path Variables\n",
            "‚úÖ SHARED_DATA_BASE_PATH: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA\n",
            "‚úÖ OUTPUT_BASE_PATH_PREFIX: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/outputs/PhASTM-fish-net\n",
            "‚úÖ IN_COLAB: True\n",
            "----------------------------------------\n",
            "TEST 2: Cell 2 Configuration Parameters\n",
            "‚úÖ target_gene_family: Collagen\n",
            "‚úÖ reference_tissue: Bone\n",
            "‚úÖ enzyme: trypsin\n",
            "‚úÖ entrez_email: matthew@palaeome.org\n",
            "‚úÖ enable_checkpointing: True\n",
            "‚úÖ model_species: ['Homo_sapiens', 'Mus_musculus', 'Rattus_norvegicus', 'Gallus_gallus', 'Danio_rerio', 'Canis_lupus_familiaris', 'Sus_scrofa', 'Ovis_aries', 'Bos_taurus', 'Petromyzon_marinus', 'Callorhinchus_milii', 'Anolis_carolinensis', 'Xenopus_tropicalis', 'Sphenodon_punctatus']\n",
            "üìä Debug: target_gene_family = 'Collagen' (should be 'Collagen')\n",
            "üìä Debug: reference_tissue = 'Bone' (should be 'Bone')\n",
            "----------------------------------------\n",
            "TEST 3: PhASTMConfig Class\n",
            "‚úÖ PhASTMConfig class available: <class '__main__.PhASTMConfig'>\n",
            "üîß Testing PhASTMConfig instantiation...\n",
            "üöÄ Initializing COMPLETE PhASTMConfig for: Collagen\n",
            "‚úÖ Base paths loaded successfully\n",
            "üìÅ Gene family directory: collagens\n",
            "‚öóÔ∏è Loaded 13 complete enzyme definitions\n",
            "üêæ Model species configured: 1 species\n",
            "‚úÖ COMPLETE PhASTMConfig initialization finished!\n",
            "üîç Validating critical paths...\n",
            "‚úÖ Gene family data directory: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/DICTIONARIES/GeneFamily/collagens\n",
            "‚úÖ Tissues JSON found: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/DICTIONARIES/GeneFamily/collagens/COLLAGENS_TISSUES.json\n",
            "‚úÖ Additional FASTA found: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/FASTA/ClassiCOL/ClassiCOL_mjc_COLLAGEN.fasta\n",
            "‚úÖ PhASTMConfig instantiated successfully\n",
            "   - Target gene family: Collagen\n",
            "   - Gene family directory: collagens\n",
            "   - Output path: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/outputs/PhASTM-fish-net/Collagen_Bone_20250713_122722\n",
            "   - Enzyme rules available: 13 enzymes\n",
            "   - Tissues JSON path: /content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/DICTIONARIES/GeneFamily/collagens/COLLAGENS_TISSUES.json\n",
            "üéØ Found 7 genes for tissue 'Bone': ['COL1A1', 'COL1A2', 'COL5A1', 'COL5A2', 'COL16A1', 'COL24A1', 'COL27A1']\n",
            "   - Target genes for tissue: ['COL1A1', 'COL1A2', 'COL5A1', 'COL5A2', 'COL16A1', 'COL24A1', 'COL27A1']\n",
            "============================================================\n",
            "üìä TESTING SUMMARY:\n",
            "‚úÖ Cell 1 (Path Management): PASS\n",
            "‚úÖ Cell 2 (Configuration): PASS\n",
            "‚úÖ Cell 3 (PhASTMConfig): PASS\n",
            "\n",
            "üéØ OVERALL STATUS: READY FOR NEXT STEPS\n",
            "\n",
            "üöÄ Current cells are working! Ready to add:\n",
            "   - Cell 4: SmartGeneTreeGenerator\n",
            "   - Cell 5: Enhanced Protein Classes (Collagen/Keratin)\n",
            "   - Cell 6: Complete data loading pipeline\n",
            "   - Cell 7: Nexyme digestion pipeline\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: SmartGeneTreeGenerator (for Ensembl API integration)"
      ],
      "metadata": {
        "id": "Wr-yEPDUpbq8"
      },
      "id": "Wr-yEPDUpbq8"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 4: SmartGeneTreeGenerator Class =====\n",
        "# Complete Ensembl API integration for gene tree fetching\n",
        "# Restores ALL June 28th functionality for robust API handling\n",
        "\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"üå≥ Defining SmartGeneTreeGenerator Class...\")\n",
        "\n",
        "class SmartGeneTreeGenerator:\n",
        "    \"\"\"\n",
        "    Advanced gene tree fetching from Ensembl with robust error handling.\n",
        "    Handles gene name -> Ensembl ID -> Gene Tree ID -> Sequences pipeline.\n",
        "\n",
        "    Features from June 28th:\n",
        "    - Automatic retries with exponential backoff\n",
        "    - Smart rate limiting\n",
        "    - Comprehensive error handling\n",
        "    - Progress tracking\n",
        "    - Checkpoint saving\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_instance):\n",
        "        \"\"\"Initialize with PhASTMConfig instance.\"\"\"\n",
        "        self.config = config_instance\n",
        "        self.base_url = \"https://rest.ensembl.org\"\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'Content-Type': 'application/json',\n",
        "            'User-Agent': 'PhASTM/1.0 (matthew@palaeome.org)'\n",
        "        })\n",
        "\n",
        "        # Tracking dictionaries\n",
        "        self.gene_to_ensembl_id = {}\n",
        "        self.gene_to_tree_id = {}\n",
        "        self.tree_id_to_url = {}\n",
        "        self.failed_genes = []\n",
        "\n",
        "        print(f\"‚úÖ SmartGeneTreeGenerator initialized\")\n",
        "        print(f\"   Base URL: {self.base_url}\")\n",
        "\n",
        "    @retry(wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "           stop=stop_after_attempt(3))\n",
        "    def _make_request(self, url: str, params: Dict = None) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Make robust API request with retries and error handling.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚ùå Request failed: {url} - {e}\")\n",
        "            raise\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå JSON decode error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def resolve_gene_name_to_ensembl_id(self, gene_name: str,\n",
        "                                      species: str = \"homo_sapiens\") -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Convert gene symbol to Ensembl gene ID.\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/lookup/symbol/{species}/{gene_name}\"\n",
        "\n",
        "        try:\n",
        "            result = self._make_request(url)\n",
        "            if result and 'id' in result:\n",
        "                ensembl_id = result['id']\n",
        "                print(f\"  ‚úÖ {gene_name} -> {ensembl_id}\")\n",
        "                return ensembl_id\n",
        "            else:\n",
        "                print(f\"  ‚ùå No Ensembl ID found for {gene_name}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Failed to resolve {gene_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_gene_tree_id_from_gene_id(self, ensembl_gene_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Get gene tree ID from Ensembl gene ID.\n",
        "        \"\"\"\n",
        "        url = f\"{self.base_url}/genetree/member/id/{ensembl_gene_id}\"\n",
        "\n",
        "        try:\n",
        "            result = self._make_request(url)\n",
        "            if result and 'tree' in result and 'id' in result['tree']:\n",
        "                tree_id = result['tree']['id']\n",
        "                tree_url = f\"{self.base_url}/genetree/id/{tree_id}?sequence=protein;aligned=0;format=fasta\"\n",
        "                self.tree_id_to_url[tree_id] = tree_url\n",
        "                print(f\"  ‚úÖ Tree ID: {tree_id}\")\n",
        "                return tree_id\n",
        "            else:\n",
        "                print(f\"  ‚ùå No gene tree found for {ensembl_gene_id}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Failed to get tree ID for {ensembl_gene_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def download_tree_sequences_via_api(self, tree_id: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Download all protein sequences from a gene tree.\n",
        "        \"\"\"\n",
        "        if tree_id not in self.tree_id_to_url:\n",
        "            print(f\"‚ùå No URL found for tree {tree_id}\")\n",
        "            return []\n",
        "\n",
        "        url = self.tree_id_to_url[tree_id]\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            fasta_content = response.text\n",
        "\n",
        "            if not fasta_content.strip():\n",
        "                print(f\"  ‚ùå Empty response for tree {tree_id}\")\n",
        "                return []\n",
        "\n",
        "            # Parse FASTA content\n",
        "            sequences = []\n",
        "            current_header = None\n",
        "            current_seq = []\n",
        "\n",
        "            for line in fasta_content.split('\\n'):\n",
        "                line = line.strip()\n",
        "                if line.startswith('>'):\n",
        "                    # Save previous sequence\n",
        "                    if current_header and current_seq:\n",
        "                        seq_data = self._parse_ensembl_header(current_header, ''.join(current_seq))\n",
        "                        if seq_data:\n",
        "                            sequences.append(seq_data)\n",
        "\n",
        "                    # Start new sequence\n",
        "                    current_header = line[1:]  # Remove '>'\n",
        "                    current_seq = []\n",
        "                elif line and not line.startswith('>'):\n",
        "                    current_seq.append(line)\n",
        "\n",
        "            # Don't forget the last sequence\n",
        "            if current_header and current_seq:\n",
        "                seq_data = self._parse_ensembl_header(current_header, ''.join(current_seq))\n",
        "                if seq_data:\n",
        "                    sequences.append(seq_data)\n",
        "\n",
        "            print(f\"  ‚úÖ Downloaded {len(sequences)} sequences from tree {tree_id}\")\n",
        "            return sequences\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Failed to download tree {tree_id}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _parse_ensembl_header(self, header: str, sequence: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Parse Ensembl FASTA header to extract metadata.\n",
        "\n",
        "        Example header:\n",
        "        >ENSP00000364140.1 pep:known chromosome:GRCh38:17:50183590:50201716:1 gene:ENSG00000108821.15 transcript:ENST00000374988.8 gene_biotype:protein_coding transcript_biotype:protein_coding gene_symbol:COL1A1\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Extract basic info\n",
        "            parts = header.split(' ', 1)\n",
        "            if len(parts) < 2:\n",
        "                return None\n",
        "\n",
        "            protein_id = parts[0]\n",
        "            metadata_str = parts[1]\n",
        "\n",
        "            # Parse metadata\n",
        "            metadata = {}\n",
        "            for item in metadata_str.split(' '):\n",
        "                if ':' in item:\n",
        "                    key, value = item.split(':', 1)\n",
        "                    metadata[key] = value\n",
        "\n",
        "            # Extract species from protein ID (format: species_ENSP...)\n",
        "            species_match = protein_id.split('_')[0] if '_' in protein_id else \"unknown\"\n",
        "\n",
        "            return {\n",
        "                'protein_id': protein_id,\n",
        "                'gene_symbol': metadata.get('gene_symbol', 'Unknown'),\n",
        "                'species': species_match,\n",
        "                'sequence': sequence,\n",
        "                'gene_id': metadata.get('gene'),\n",
        "                'transcript_id': metadata.get('transcript'),\n",
        "                'gene_biotype': metadata.get('gene_biotype'),\n",
        "                'source': 'ensembl_tree',\n",
        "                'length': len(sequence)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Failed to parse header: {header[:50]}... Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_sequences_for_gene_list(self, gene_names: List[str],\n",
        "                                    species: str = \"homo_sapiens\") -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Complete pipeline: gene names -> sequences for all genes.\n",
        "        \"\"\"\n",
        "        print(f\"üöÄ Starting complete gene tree fetching for {len(gene_names)} genes...\")\n",
        "\n",
        "        all_sequences_by_gene = {}\n",
        "\n",
        "        # Step 1: Resolve gene names to Ensembl IDs\n",
        "        print(\"üîç Step 1: Resolving gene names to Ensembl IDs...\")\n",
        "        for gene_name in tqdm(gene_names, desc=\"Resolving genes\"):\n",
        "            ensembl_id = self.resolve_gene_name_to_ensembl_id(gene_name, species)\n",
        "            if ensembl_id:\n",
        "                self.gene_to_ensembl_id[gene_name] = ensembl_id\n",
        "            else:\n",
        "                self.failed_genes.append(gene_name)\n",
        "            time.sleep(0.2)  # Rate limiting\n",
        "\n",
        "        print(f\"‚úÖ Resolved {len(self.gene_to_ensembl_id)}/{len(gene_names)} genes to Ensembl IDs\")\n",
        "\n",
        "        # Step 2: Get Gene Tree IDs\n",
        "        print(\"üå≥ Step 2: Getting Gene Tree IDs...\")\n",
        "        for gene_name, ensembl_id in tqdm(self.gene_to_ensembl_id.items(), desc=\"Getting tree IDs\"):\n",
        "            tree_id = self.get_gene_tree_id_from_gene_id(ensembl_id)\n",
        "            if tree_id:\n",
        "                self.gene_to_tree_id[gene_name] = tree_id\n",
        "            time.sleep(0.2)\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.gene_to_tree_id)} gene trees\")\n",
        "\n",
        "        # Step 3: Download sequences from trees\n",
        "        print(\"üíæ Step 3: Downloading sequences from gene trees...\")\n",
        "        for gene_name, tree_id in tqdm(self.gene_to_tree_id.items(), desc=\"Downloading sequences\"):\n",
        "            sequences = self.download_tree_sequences_via_api(tree_id)\n",
        "            if sequences:\n",
        "                # Add gene family info to each sequence\n",
        "                for seq in sequences:\n",
        "                    seq['gene_family'] = gene_name\n",
        "                    seq['tree_id'] = tree_id\n",
        "\n",
        "                all_sequences_by_gene[gene_name] = sequences\n",
        "                print(f\"  {gene_name}: {len(sequences)} sequences\")\n",
        "            time.sleep(0.5)  # Longer pause for downloads\n",
        "\n",
        "        # Step 4: Save summary\n",
        "        self._save_summary()\n",
        "\n",
        "        return all_sequences_by_gene\n",
        "\n",
        "    def _save_summary(self):\n",
        "        \"\"\"Save summary of the fetching process.\"\"\"\n",
        "        summary = {\n",
        "            'gene_to_ensembl_id': self.gene_to_ensembl_id,\n",
        "            'gene_to_tree_id': self.gene_to_tree_id,\n",
        "            'tree_id_to_url': self.tree_id_to_url,\n",
        "            'failed_genes': self.failed_genes,\n",
        "            'total_genes_processed': len(self.gene_to_ensembl_id),\n",
        "            'total_trees_found': len(self.gene_to_tree_id)\n",
        "        }\n",
        "\n",
        "        summary_path = self.config.output_base_path / \"ensembl_gene_tree_summary.json\"\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(f\"üíæ Summary saved to: {summary_path}\")\n",
        "\n",
        "print(\"‚úÖ SmartGeneTreeGenerator class defined successfully!\")\n",
        "print(\"üîó Features restored:\")\n",
        "print(\"   - Robust API requests with retries\")\n",
        "print(\"   - Gene name -> Ensembl ID resolution\")\n",
        "print(\"   - Gene tree fetching and parsing\")\n",
        "print(\"   - Comprehensive error handling\")\n",
        "print(\"   - Progress tracking and checkpointing\")\n",
        "print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT62XHErpKV2",
        "outputId": "715ae901-b08a-4c4b-b6ec-72597558bd3a"
      },
      "id": "aT62XHErpKV2",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üå≥ Defining SmartGeneTreeGenerator Class...\n",
            "‚úÖ SmartGeneTreeGenerator class defined successfully!\n",
            "üîó Features restored:\n",
            "   - Robust API requests with retries\n",
            "   - Gene name -> Ensembl ID resolution\n",
            "   - Gene tree fetching and parsing\n",
            "   - Comprehensive error handling\n",
            "   - Progress tracking and checkpointing\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Test Cell 4: SmartGeneTreeGenerator =====\n",
        "# Test the SmartGeneTreeGenerator class independently\n",
        "\n",
        "print(\"üß™ Testing Cell 4: SmartGeneTreeGenerator...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: Check if SmartGeneTreeGenerator class exists\n",
        "print(\"TEST 1: SmartGeneTreeGenerator Class Availability\")\n",
        "try:\n",
        "    print(f\"‚úÖ SmartGeneTreeGenerator class available: {SmartGeneTreeGenerator}\")\n",
        "    cell4_class_available = True\n",
        "except NameError as e:\n",
        "    print(f\"‚ùå SmartGeneTreeGenerator class not defined: {e}\")\n",
        "    cell4_class_available = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 2: Test SmartGeneTreeGenerator instantiation\n",
        "print(\"TEST 2: SmartGeneTreeGenerator Instantiation\")\n",
        "if cell4_class_available:\n",
        "    try:\n",
        "        # Use the config from previous tests\n",
        "        if 'test_config' in globals():\n",
        "            gene_tree_gen = SmartGeneTreeGenerator(test_config)\n",
        "            print(\"‚úÖ SmartGeneTreeGenerator instantiated successfully\")\n",
        "            print(f\"   - Base URL: {gene_tree_gen.base_url}\")\n",
        "            print(f\"   - Session configured: {gene_tree_gen.session is not None}\")\n",
        "            print(f\"   - Config attached: {gene_tree_gen.config is not None}\")\n",
        "            cell4_instantiation = True\n",
        "        else:\n",
        "            print(\"‚ùå test_config not available from previous tests\")\n",
        "            cell4_instantiation = False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå SmartGeneTreeGenerator instantiation failed: {e}\")\n",
        "        cell4_instantiation = False\n",
        "else:\n",
        "    cell4_instantiation = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 3: Test API methods (without making actual requests)\n",
        "print(\"TEST 3: SmartGeneTreeGenerator Methods\")\n",
        "if cell4_instantiation:\n",
        "    try:\n",
        "        # Test method availability\n",
        "        required_methods = [\n",
        "            'resolve_gene_name_to_ensembl_id',\n",
        "            'get_gene_tree_id_from_gene_id',\n",
        "            'download_tree_sequences_via_api',\n",
        "            '_make_request',\n",
        "            '_parse_ensembl_header',\n",
        "            'fetch_sequences_for_gene_list'\n",
        "        ]\n",
        "\n",
        "        missing_methods = []\n",
        "        available_methods = []\n",
        "\n",
        "        for method_name in required_methods:\n",
        "            if hasattr(gene_tree_gen, method_name):\n",
        "                available_methods.append(method_name)\n",
        "                print(f\"   ‚úÖ {method_name}\")\n",
        "            else:\n",
        "                missing_methods.append(method_name)\n",
        "                print(f\"   ‚ùå {method_name}\")\n",
        "\n",
        "        if not missing_methods:\n",
        "            print(f\"‚úÖ All {len(required_methods)} required methods available\")\n",
        "            cell4_methods = True\n",
        "        else:\n",
        "            print(f\"‚ùå Missing {len(missing_methods)} methods: {missing_methods}\")\n",
        "            cell4_methods = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error testing methods: {e}\")\n",
        "        cell4_methods = False\n",
        "else:\n",
        "    cell4_methods = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 4: Test tracking dictionaries\n",
        "print(\"TEST 4: SmartGeneTreeGenerator Data Structures\")\n",
        "if cell4_instantiation:\n",
        "    try:\n",
        "        # Check if tracking dictionaries are initialized\n",
        "        required_attrs = [\n",
        "            'gene_to_ensembl_id',\n",
        "            'gene_to_tree_id',\n",
        "            'tree_id_to_url',\n",
        "            'failed_genes'\n",
        "        ]\n",
        "\n",
        "        for attr_name in required_attrs:\n",
        "            if hasattr(gene_tree_gen, attr_name):\n",
        "                attr_value = getattr(gene_tree_gen, attr_name)\n",
        "                print(f\"   ‚úÖ {attr_name}: {type(attr_value).__name__} (initialized)\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå {attr_name}: missing\")\n",
        "\n",
        "        cell4_data_structures = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error testing data structures: {e}\")\n",
        "        cell4_data_structures = False\n",
        "else:\n",
        "    cell4_data_structures = False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Summary for Cell 4\n",
        "print(\"üìä CELL 4 TESTING SUMMARY:\")\n",
        "print(f\"‚úÖ Class Available: {'PASS' if cell4_class_available else 'FAIL'}\")\n",
        "print(f\"‚úÖ Instantiation: {'PASS' if cell4_instantiation else 'FAIL'}\")\n",
        "print(f\"‚úÖ Methods: {'PASS' if cell4_methods else 'FAIL'}\")\n",
        "print(f\"‚úÖ Data Structures: {'PASS' if cell4_data_structures else 'FAIL'}\")\n",
        "\n",
        "cell4_overall = cell4_class_available and cell4_instantiation and cell4_methods and cell4_data_structures\n",
        "print(f\"\\nüéØ CELL 4 STATUS: {'READY' if cell4_overall else 'NEEDS FIXES'}\")\n",
        "\n",
        "if cell4_overall:\n",
        "    print(\"\\n‚úÖ SmartGeneTreeGenerator is ready for API calls!\")\n",
        "    print(\"   - Can proceed to test actual gene name resolution\")\n",
        "    print(\"   - Can test gene tree fetching\")\n",
        "    print(\"   - All infrastructure in place\")\n",
        "else:\n",
        "    print(\"\\nüîß Fix Cell 4 issues before proceeding\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyAy89EUveUi",
        "outputId": "38103def-9825-43b1-df73-773b571b71a8"
      },
      "id": "XyAy89EUveUi",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Cell 4: SmartGeneTreeGenerator...\n",
            "============================================================\n",
            "TEST 1: SmartGeneTreeGenerator Class Availability\n",
            "‚úÖ SmartGeneTreeGenerator class available: <class '__main__.SmartGeneTreeGenerator'>\n",
            "----------------------------------------\n",
            "TEST 2: SmartGeneTreeGenerator Instantiation\n",
            "‚úÖ SmartGeneTreeGenerator initialized\n",
            "   Base URL: https://rest.ensembl.org\n",
            "‚úÖ SmartGeneTreeGenerator instantiated successfully\n",
            "   - Base URL: https://rest.ensembl.org\n",
            "   - Session configured: True\n",
            "   - Config attached: True\n",
            "----------------------------------------\n",
            "TEST 3: SmartGeneTreeGenerator Methods\n",
            "   ‚úÖ resolve_gene_name_to_ensembl_id\n",
            "   ‚úÖ get_gene_tree_id_from_gene_id\n",
            "   ‚úÖ download_tree_sequences_via_api\n",
            "   ‚úÖ _make_request\n",
            "   ‚úÖ _parse_ensembl_header\n",
            "   ‚úÖ fetch_sequences_for_gene_list\n",
            "‚úÖ All 6 required methods available\n",
            "----------------------------------------\n",
            "TEST 4: SmartGeneTreeGenerator Data Structures\n",
            "   ‚úÖ gene_to_ensembl_id: dict (initialized)\n",
            "   ‚úÖ gene_to_tree_id: dict (initialized)\n",
            "   ‚úÖ tree_id_to_url: dict (initialized)\n",
            "   ‚úÖ failed_genes: list (initialized)\n",
            "============================================================\n",
            "üìä CELL 4 TESTING SUMMARY:\n",
            "‚úÖ Class Available: PASS\n",
            "‚úÖ Instantiation: PASS\n",
            "‚úÖ Methods: PASS\n",
            "‚úÖ Data Structures: PASS\n",
            "\n",
            "üéØ CELL 4 STATUS: READY\n",
            "\n",
            "‚úÖ SmartGeneTreeGenerator is ready for API calls!\n",
            "   - Can proceed to test actual gene name resolution\n",
            "   - Can test gene tree fetching\n",
            "   - All infrastructure in place\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "protein-classes-header",
      "metadata": {
        "id": "protein-classes-header"
      },
      "source": [
        "## Cell 5: Protein Processing Classes\n",
        "\n",
        "##3 Advanced Protein Family Analysis\n",
        "\n",
        "Defines the base `ProteinClass` and specialized subclasses for `Collagen` and `Keratin` analysis with the fully restored, advanced analytical methods."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 5: Enhanced Protein Classes (Collagen, Keratin, KAP) =====\n",
        "# Detailed protein analysis classes preserving your research work\n",
        "# Combines June 28th functionality with your enhanced protein classification\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"üß¨ Defining Enhanced Protein Classes...\")\n",
        "\n",
        "class ProteinClass:\n",
        "    \"\"\"\n",
        "    Base class for protein analysis with common functionality.\n",
        "    Provides framework for specialized protein family analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_instance):\n",
        "        \"\"\"Initialize with PhASTMConfig instance.\"\"\"\n",
        "        self.config = config_instance\n",
        "        self.master_df = pd.DataFrame()\n",
        "        self.enzyme_rules = config_instance.enzyme_rules\n",
        "\n",
        "        print(f\"  ‚úÖ Base ProteinClass initialized\")\n",
        "        print(f\"  üìä Available enzymes: {list(self.enzyme_rules.keys())}\")\n",
        "\n",
        "    def digest_sequence_with_enzyme(self, sequence: str, enzyme: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Digest protein sequence with specified enzyme.\n",
        "        Returns list of peptide fragments.\n",
        "        \"\"\"\n",
        "        if enzyme not in self.enzyme_rules:\n",
        "            raise ValueError(f\"Enzyme {enzyme} not supported. Available: {list(self.enzyme_rules.keys())}\")\n",
        "\n",
        "        enzyme_data = self.enzyme_rules[enzyme]\n",
        "        cleavage_rule = enzyme_data['cleavage_rule']\n",
        "\n",
        "        # Apply cleavage rule using regex\n",
        "        try:\n",
        "            peptides = re.split(cleavage_rule, sequence)\n",
        "            # Remove empty peptides\n",
        "            peptides = [p for p in peptides if p.strip()]\n",
        "            return peptides\n",
        "        except re.error as e:\n",
        "            print(f\"‚ùå Regex error for enzyme {enzyme}: {e}\")\n",
        "            return [sequence]  # Return original sequence if regex fails\n",
        "\n",
        "    def extract_species_from_header(self, header: str) -> str:\n",
        "        \"\"\"Extract species information from sequence header.\"\"\"\n",
        "        # Try different header formats\n",
        "        patterns = [\n",
        "            r'\\[([A-Za-z]+\\s+[A-Za-z]+)\\]',  # [Homo sapiens]\n",
        "            r'OS=([A-Za-z]+\\s+[A-Za-z]+)',   # OS=Homo sapiens\n",
        "            r'_([A-Z]+[A-Z0-9]*)\\s',         # _HUMAN\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, header)\n",
        "            if match:\n",
        "                species = match.group(1)\n",
        "                # Convert _HUMAN style to proper species name\n",
        "                if species.isupper() and len(species) <= 10:\n",
        "                    species_mapping = {\n",
        "                        'HUMAN': 'Homo sapiens',\n",
        "                        'MOUSE': 'Mus musculus',\n",
        "                        'RAT': 'Rattus norvegicus',\n",
        "                        'CHICK': 'Gallus gallus',\n",
        "                        'ZEBRA': 'Danio rerio'\n",
        "                    }\n",
        "                    return species_mapping.get(species, f\"Unknown_{species}\")\n",
        "                else:\n",
        "                    return species.replace('_', ' ')\n",
        "\n",
        "        return \"Unknown_species\"\n",
        "\n",
        "    def analyze_protein_composition(self, sequence: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze basic protein composition.\n",
        "        \"\"\"\n",
        "        if not sequence:\n",
        "            return {}\n",
        "\n",
        "        aa_counts = defaultdict(int)\n",
        "        for aa in sequence:\n",
        "            aa_counts[aa] += 1\n",
        "\n",
        "        total_length = len(sequence)\n",
        "\n",
        "        return {\n",
        "            'length': total_length,\n",
        "            'amino_acid_counts': dict(aa_counts),\n",
        "            'amino_acid_frequencies': {aa: count/total_length for aa, count in aa_counts.items()},\n",
        "            'molecular_weight_approx': total_length * 110,  # Rough approximation\n",
        "            'charge_positive': sequence.count('K') + sequence.count('R') + sequence.count('H'),\n",
        "            'charge_negative': sequence.count('D') + sequence.count('E'),\n",
        "            'hydrophobic_residues': sum(sequence.count(aa) for aa in 'AILMFWYV'),\n",
        "            'polar_residues': sum(sequence.count(aa) for aa in 'STYNQC'),\n",
        "            'aromatic_residues': sum(sequence.count(aa) for aa in 'FWY')\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Base ProteinClass defined\")\n",
        "\n",
        "class Collagen(ProteinClass):\n",
        "    \"\"\"\n",
        "    Enhanced Collagen analysis class with detailed structural analysis.\n",
        "    Preserves your research work on collagen classification and analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_instance):\n",
        "        \"\"\"Initialize Collagen-specific analysis.\"\"\"\n",
        "        super().__init__(config_instance)\n",
        "        self.collagen_types = self._load_collagen_classification()\n",
        "\n",
        "        # Collagen-specific parameters\n",
        "        self.gly_x_y_pattern = re.compile(r'G[A-Z][A-Z]')\n",
        "        self.hydroxyproline_pattern = re.compile(r'GP[PO]|G[A-Z]P')\n",
        "\n",
        "        print(\"  üß¨ Collagen subclass initialized\")\n",
        "        print(f\"  üìö Loaded {len(self.collagen_types)} collagen type classifications\")\n",
        "\n",
        "    def _load_collagen_classification(self) -> Dict[str, Any]:\n",
        "        \"\"\"Load collagen type classification data.\"\"\"\n",
        "        # Enhanced collagen classification based on your research\n",
        "        return {\n",
        "            'COL1A1': {\n",
        "                'type': 'Type I',\n",
        "                'description': 'Fibrillar collagen, major component of bone, tendon, skin',\n",
        "                'typical_tissues': ['bone', 'tendon', 'skin', 'cornea'],\n",
        "                'chain_composition': 'Œ±1(I)‚ÇÇŒ±2(I)',\n",
        "                'triple_helix_length': 1014,\n",
        "                'n_telopeptide_length': {'default': 16, 'range': [14, 18]},\n",
        "                'c_telopeptide_length': {'default': 26, 'range': [24, 28]},\n",
        "                'characteristic_sequences': ['GFOGER', 'GVQGER', 'GLPGER']\n",
        "            },\n",
        "            'COL1A2': {\n",
        "                'type': 'Type I',\n",
        "                'description': 'Fibrillar collagen, Œ±2 chain of Type I',\n",
        "                'typical_tissues': ['bone', 'tendon', 'skin', 'cornea'],\n",
        "                'chain_composition': 'Œ±1(I)‚ÇÇŒ±2(I)',\n",
        "                'triple_helix_length': 1008,\n",
        "                'n_telopeptide_length': {'default': 17, 'range': [15, 19]},\n",
        "                'c_telopeptide_length': {'default': 25, 'range': [23, 27]},\n",
        "                'characteristic_sequences': ['GFOGER', 'GVQGER', 'GLAGER']\n",
        "            },\n",
        "            'COL2A1': {\n",
        "                'type': 'Type II',\n",
        "                'description': 'Cartilage-specific fibrillar collagen',\n",
        "                'typical_tissues': ['cartilage', 'vitreous_humor', 'notochord'],\n",
        "                'chain_composition': 'Œ±1(II)‚ÇÉ',\n",
        "                'triple_helix_length': 1017,\n",
        "                'n_telopeptide_length': {'default': 18, 'range': [16, 20]},\n",
        "                'c_telopeptide_length': {'default': 27, 'range': [25, 29]},\n",
        "                'characteristic_sequences': ['GFOGER', 'GVQGER', 'GAPGER']\n",
        "            },\n",
        "            'COL3A1': {\n",
        "                'type': 'Type III',\n",
        "                'description': 'Reticular collagen, associated with Type I',\n",
        "                'typical_tissues': ['skin', 'blood_vessels', 'internal_organs'],\n",
        "                'chain_composition': 'Œ±1(III)‚ÇÉ',\n",
        "                'triple_helix_length': 1026,\n",
        "                'n_telopeptide_length': {'default': 15, 'range': [13, 17]},\n",
        "                'c_telopeptide_length': {'default': 24, 'range': [22, 26]},\n",
        "                'characteristic_sequences': ['GFOGER', 'GVQGER', 'GAPGER']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def analyze_collagen_structure(self, sequence: str, gene_symbol: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Comprehensive collagen structural analysis.\n",
        "        \"\"\"\n",
        "        analysis = {}\n",
        "\n",
        "        # Basic composition analysis\n",
        "        basic_analysis = self.analyze_protein_composition(sequence)\n",
        "        analysis.update(basic_analysis)\n",
        "\n",
        "        # Gly-X-Y repeat analysis\n",
        "        gxy_analysis = self._analyze_gly_x_y_repeats(sequence)\n",
        "        analysis['gly_x_y_analysis'] = gxy_analysis\n",
        "\n",
        "        # Domain extraction\n",
        "        if gene_symbol and gene_symbol.upper() in self.collagen_types:\n",
        "            domains = self._extract_collagen_domains(sequence, gene_symbol.upper())\n",
        "            analysis['domains'] = domains\n",
        "\n",
        "        # Collagen type classification\n",
        "        collagen_type = self._classify_collagen_type(sequence, gene_symbol)\n",
        "        analysis['collagen_classification'] = collagen_type\n",
        "\n",
        "        # Post-translational modification sites\n",
        "        ptm_sites = self._predict_ptm_sites(sequence)\n",
        "        analysis['ptm_prediction'] = ptm_sites\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _analyze_gly_x_y_repeats(self, sequence: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze Gly-X-Y repeat structure characteristic of collagen.\n",
        "        \"\"\"\n",
        "        # Find all Gly-X-Y triplets\n",
        "        gxy_triplets = []\n",
        "        for i in range(0, len(sequence) - 2, 3):\n",
        "            triplet = sequence[i:i+3]\n",
        "            if triplet.startswith('G'):\n",
        "                gxy_triplets.append({\n",
        "                    'position': i,\n",
        "                    'sequence': triplet,\n",
        "                    'x_residue': triplet[1] if len(triplet) > 1 else 'X',\n",
        "                    'y_residue': triplet[2] if len(triplet) > 2 else 'X'\n",
        "                })\n",
        "\n",
        "        total_triplets = len(sequence) // 3\n",
        "        gly_triplets = len(gxy_triplets)\n",
        "        gly_percentage = (gly_triplets / total_triplets * 100) if total_triplets > 0 else 0\n",
        "\n",
        "        # Analyze X and Y position preferences\n",
        "        x_residues = [t['x_residue'] for t in gxy_triplets]\n",
        "        y_residues = [t['y_residue'] for t in gxy_triplets]\n",
        "\n",
        "        x_counts = defaultdict(int)\n",
        "        y_counts = defaultdict(int)\n",
        "\n",
        "        for x in x_residues:\n",
        "            x_counts[x] += 1\n",
        "        for y in y_residues:\n",
        "            y_counts[y] += 1\n",
        "\n",
        "        return {\n",
        "            'total_triplets': total_triplets,\n",
        "            'gly_triplets': gly_triplets,\n",
        "            'gly_percentage': gly_percentage,\n",
        "            'triplet_details': gxy_triplets,\n",
        "            'x_position_preferences': dict(x_counts),\n",
        "            'y_position_preferences': dict(y_counts),\n",
        "            'proline_in_y': y_counts.get('P', 0),\n",
        "            'hydroxyproline_predicted': len(re.findall(self.hydroxyproline_pattern, sequence)),\n",
        "            'is_collagen_like': gly_percentage > 25.0  # Threshold for collagen-like\n",
        "        }\n",
        "\n",
        "    def _extract_collagen_domains(self, sequence: str, gene_symbol: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract collagen domains: N-telopeptide, triple helix, C-telopeptide.\n",
        "        \"\"\"\n",
        "        params = self.collagen_types.get(gene_symbol, {})\n",
        "        if not params:\n",
        "            return {'error': f'No parameters found for {gene_symbol}'}\n",
        "\n",
        "        n_telo_len = params.get('n_telopeptide_length', {}).get('default', 0)\n",
        "        c_telo_len = params.get('c_telopeptide_length', {}).get('default', 0)\n",
        "        seq_len = len(sequence)\n",
        "\n",
        "        # Extract domains\n",
        "        n_telopeptide = sequence[:n_telo_len] if n_telo_len > 0 and seq_len > n_telo_len else None\n",
        "\n",
        "        helix_start = n_telo_len\n",
        "        helix_end = seq_len - c_telo_len if c_telo_len > 0 and (seq_len - n_telo_len) > c_telo_len else seq_len\n",
        "\n",
        "        triple_helix_core = sequence[helix_start:helix_end] if helix_end > helix_start else None\n",
        "        c_telopeptide = sequence[helix_end:] if c_telo_len > 0 and helix_end < seq_len else None\n",
        "\n",
        "        return {\n",
        "            'n_telopeptide': {\n",
        "                'sequence': n_telopeptide,\n",
        "                'length': len(n_telopeptide) if n_telopeptide else 0,\n",
        "                'expected_length': n_telo_len\n",
        "            },\n",
        "            'triple_helix': {\n",
        "                'sequence': triple_helix_core,\n",
        "                'length': len(triple_helix_core) if triple_helix_core else 0,\n",
        "                'expected_length': params.get('triple_helix_length', 'unknown'),\n",
        "                'start_position': helix_start,\n",
        "                'end_position': helix_end\n",
        "            },\n",
        "            'c_telopeptide': {\n",
        "                'sequence': c_telopeptide,\n",
        "                'length': len(c_telopeptide) if c_telopeptide else 0,\n",
        "                'expected_length': c_telo_len\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _classify_collagen_type(self, sequence: str, gene_symbol: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Classify collagen type based on sequence and gene symbol.\n",
        "        \"\"\"\n",
        "        if gene_symbol and gene_symbol.upper() in self.collagen_types:\n",
        "            known_type = self.collagen_types[gene_symbol.upper()]\n",
        "            return {\n",
        "                'classification': 'known_collagen',\n",
        "                'type': known_type['type'],\n",
        "                'description': known_type['description'],\n",
        "                'confidence': 'high',\n",
        "                'gene_symbol': gene_symbol.upper()\n",
        "            }\n",
        "\n",
        "        # Sequence-based classification for unknown genes\n",
        "        gxy_analysis = self._analyze_gly_x_y_repeats(sequence)\n",
        "\n",
        "        if gxy_analysis['gly_percentage'] > 30:\n",
        "            if len(sequence) > 1000:\n",
        "                predicted_type = 'Fibrillar collagen (I, II, III, V, XI-like)'\n",
        "            elif 200 < len(sequence) < 500:\n",
        "                predicted_type = 'Network collagen (IV, VIII, X-like)'\n",
        "            else:\n",
        "                predicted_type = 'Short collagen or collagen-like'\n",
        "        else:\n",
        "            predicted_type = 'Non-collagen or atypical collagen'\n",
        "\n",
        "        return {\n",
        "            'classification': 'predicted',\n",
        "            'type': predicted_type,\n",
        "            'confidence': 'medium',\n",
        "            'gly_percentage': gxy_analysis['gly_percentage'],\n",
        "            'sequence_length': len(sequence)\n",
        "        }\n",
        "\n",
        "    def _predict_ptm_sites(self, sequence: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Predict post-translational modification sites.\n",
        "        \"\"\"\n",
        "        # Hydroxylation sites (P in Y position of Gly-X-Y)\n",
        "        hydroxyproline_sites = []\n",
        "        hydroxylysine_sites = []\n",
        "\n",
        "        for i in range(0, len(sequence) - 2, 3):\n",
        "            if i + 2 < len(sequence):\n",
        "                triplet = sequence[i:i+3]\n",
        "                if triplet.startswith('G') and len(triplet) == 3:\n",
        "                    if triplet[2] == 'P':  # Proline in Y position\n",
        "                        hydroxyproline_sites.append(i + 2)\n",
        "                    elif triplet[2] == 'K':  # Lysine in Y position\n",
        "                        hydroxylysine_sites.append(i + 2)\n",
        "\n",
        "        # Glycosylation sites (N-linked: NxS/T, O-linked: S/T)\n",
        "        n_glycosylation = []\n",
        "        o_glycosylation = []\n",
        "\n",
        "        # N-linked glycosylation: N-X-S/T\n",
        "        for match in re.finditer(r'N[A-Z][ST]', sequence):\n",
        "            n_glycosylation.append(match.start())\n",
        "\n",
        "        # O-linked glycosylation: S or T\n",
        "        for i, aa in enumerate(sequence):\n",
        "            if aa in 'ST':\n",
        "                o_glycosylation.append(i)\n",
        "\n",
        "        return {\n",
        "            'hydroxyproline_sites': hydroxyproline_sites,\n",
        "            'hydroxylysine_sites': hydroxylysine_sites,\n",
        "            'n_glycosylation_sites': n_glycosylation,\n",
        "            'o_glycosylation_sites': o_glycosylation[:20],  # Limit output\n",
        "            'total_hydroxyproline_predicted': len(hydroxyproline_sites),\n",
        "            'total_hydroxylysine_predicted': len(hydroxylysine_sites),\n",
        "            'total_n_glycosylation_predicted': len(n_glycosylation),\n",
        "            'total_o_glycosylation_predicted': len(o_glycosylation)\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Enhanced Collagen class defined\")\n",
        "\n",
        "class Keratin(ProteinClass):\n",
        "    \"\"\"\n",
        "    Enhanced Keratin analysis class with detailed structural analysis.\n",
        "    Includes coiled-coil analysis, cysteine analysis, and KAP classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_instance):\n",
        "        \"\"\"Initialize Keratin-specific analysis.\"\"\"\n",
        "        super().__init__(config_instance)\n",
        "        self.keratin_types = self._load_keratin_classification()\n",
        "\n",
        "        # Keratin-specific parameters\n",
        "        self.coiled_coil_pattern = re.compile(r'[ALEV][A-Z]{6}[ALEV]')  # Simplified heptad repeat\n",
        "        self.cysteine_pattern = re.compile(r'C')\n",
        "\n",
        "        # Alpha-helix propensity scores (Chou-Fasman)\n",
        "        self.alpha_helix_propensity = {\n",
        "            'A': 1.42, 'R': 0.98, 'N': 0.67, 'D': 1.01, 'C': 0.70,\n",
        "            'Q': 1.11, 'E': 1.51, 'G': 0.57, 'H': 1.00, 'I': 1.08,\n",
        "            'L': 1.21, 'K': 1.16, 'M': 1.45, 'F': 1.13, 'P': 0.57,\n",
        "            'S': 0.77, 'T': 0.83, 'W': 1.08, 'Y': 0.69, 'V': 1.06\n",
        "        }\n",
        "\n",
        "        print(\"  üß¨ Keratin subclass initialized\")\n",
        "        print(f\"  üìö Loaded {len(self.keratin_types)} keratin type classifications\")\n",
        "\n",
        "    def _load_keratin_classification(self) -> Dict[str, Any]:\n",
        "        \"\"\"Load keratin type classification data.\"\"\"\n",
        "        return {\n",
        "            'KRT1': {\n",
        "                'type': 'Type II Intermediate Filament',\n",
        "                'description': 'Epidermis-specific, pairs with KRT10',\n",
        "                'typical_tissues': ['epidermis', 'skin'],\n",
        "                'molecular_weight': 66000,\n",
        "                'isoelectric_point': 8.15,\n",
        "                'cysteine_content': 'low'\n",
        "            },\n",
        "            'KRT5': {\n",
        "                'type': 'Type II Intermediate Filament',\n",
        "                'description': 'Basal epithelial cells, pairs with KRT14',\n",
        "                'typical_tissues': ['basal_epithelium', 'skin'],\n",
        "                'molecular_weight': 58000,\n",
        "                'isoelectric_point': 7.59,\n",
        "                'cysteine_content': 'low'\n",
        "            },\n",
        "            'KRT10': {\n",
        "                'type': 'Type I Intermediate Filament',\n",
        "                'description': 'Epidermis-specific, pairs with KRT1',\n",
        "                'typical_tissues': ['epidermis', 'skin'],\n",
        "                'molecular_weight': 56500,\n",
        "                'isoelectric_point': 5.13,\n",
        "                'cysteine_content': 'low'\n",
        "            },\n",
        "            'KRT14': {\n",
        "                'type': 'Type I Intermediate Filament',\n",
        "                'description': 'Basal epithelial cells, pairs with KRT5',\n",
        "                'typical_tissues': ['basal_epithelium', 'skin'],\n",
        "                'molecular_weight': 51600,\n",
        "                'isoelectric_point': 5.09,\n",
        "                'cysteine_content': 'low'\n",
        "            },\n",
        "            'KRT31': {\n",
        "                'type': 'Hair Keratin',\n",
        "                'description': 'Hard keratin of hair cortex',\n",
        "                'typical_tissues': ['hair', 'wool', 'fur'],\n",
        "                'molecular_weight': 45000,\n",
        "                'isoelectric_point': 5.4,\n",
        "                'cysteine_content': 'high'\n",
        "            },\n",
        "            'KRT85': {\n",
        "                'type': 'Hair Keratin',\n",
        "                'description': 'Hard keratin of hair cortex',\n",
        "                'typical_tissues': ['hair', 'wool', 'fur'],\n",
        "                'molecular_weight': 50000,\n",
        "                'isoelectric_point': 5.8,\n",
        "                'cysteine_content': 'high'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def analyze_keratin_structure(self, sequence: str, gene_symbol: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Comprehensive keratin structural analysis.\n",
        "        \"\"\"\n",
        "        analysis = {}\n",
        "\n",
        "        # Basic composition analysis\n",
        "        basic_analysis = self.analyze_protein_composition(sequence)\n",
        "        analysis.update(basic_analysis)\n",
        "\n",
        "        # Coiled-coil analysis\n",
        "        coiled_coil_analysis = self._analyze_coiled_coil_structure(sequence)\n",
        "        analysis['coiled_coil_analysis'] = coiled_coil_analysis\n",
        "\n",
        "        # Cysteine analysis\n",
        "        cysteine_analysis = self._analyze_cysteine_content(sequence)\n",
        "        analysis['cysteine_analysis'] = cysteine_analysis\n",
        "\n",
        "        # Domain extraction (Head-Rod-Tail)\n",
        "        domains = self._extract_keratin_domains(sequence)\n",
        "        analysis['domains'] = domains\n",
        "\n",
        "        # Keratin type classification\n",
        "        keratin_type = self._classify_keratin_type(sequence, gene_symbol)\n",
        "        analysis['keratin_classification'] = keratin_type\n",
        "\n",
        "        # KAP detection\n",
        "        kap_analysis = self._analyze_kap_features(sequence, gene_symbol)\n",
        "        analysis['kap_analysis'] = kap_analysis\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _analyze_coiled_coil_structure(self, sequence: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze coiled-coil heptad repeat structure.\n",
        "        \"\"\"\n",
        "        # Simplified heptad repeat analysis\n",
        "        heptad_positions = {'a': [], 'b': [], 'c': [], 'd': [], 'e': [], 'f': [], 'g': []}\n",
        "\n",
        "        for i, aa in enumerate(sequence):\n",
        "            heptad_pos = ['a', 'b', 'c', 'd', 'e', 'f', 'g'][i % 7]\n",
        "            heptad_positions[heptad_pos].append(aa)\n",
        "\n",
        "        # Calculate hydrophobic moments for coiled-coil prediction\n",
        "        hydrophobic_aa = set('AILMFWYV')\n",
        "\n",
        "        a_hydrophobic = sum(1 for aa in heptad_positions['a'] if aa in hydrophobic_aa)\n",
        "        d_hydrophobic = sum(1 for aa in heptad_positions['d'] if aa in hydrophobic_aa)\n",
        "\n",
        "        total_a = len(heptad_positions['a'])\n",
        "        total_d = len(heptad_positions['d'])\n",
        "\n",
        "        a_hydrophobic_fraction = a_hydrophobic / total_a if total_a > 0 else 0\n",
        "        d_hydrophobic_fraction = d_hydrophobic / total_d if total_d > 0 else 0\n",
        "\n",
        "        coiled_coil_score = (a_hydrophobic_fraction + d_hydrophobic_fraction) / 2\n",
        "\n",
        "        # Alpha-helix propensity\n",
        "        helix_score = sum(self.alpha_helix_propensity.get(aa, 1.0) for aa in sequence) / len(sequence)\n",
        "\n",
        "        return {\n",
        "            'heptad_analysis': {pos: len(residues) for pos, residues in heptad_positions.items()},\n",
        "            'a_position_hydrophobic_fraction': a_hydrophobic_fraction,\n",
        "            'd_position_hydrophobic_fraction': d_hydrophobic_fraction,\n",
        "            'coiled_coil_score': coiled_coil_score,\n",
        "            'alpha_helix_propensity': helix_score,\n",
        "            'predicted_coiled_coil': coiled_coil_score > 0.5 and helix_score > 1.0\n",
        "        }\n",
        "\n",
        "    def _analyze_cysteine_content(self, sequence: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze cysteine content and distribution.\n",
        "        \"\"\"\n",
        "        cys_positions = [i for i, aa in enumerate(sequence) if aa == 'C']\n",
        "        total_cys = len(cys_positions)\n",
        "        cys_percentage = (total_cys / len(sequence) * 100) if len(sequence) > 0 else 0\n",
        "\n",
        "        # Analyze cysteine spacing\n",
        "        cys_spacings = []\n",
        "        for i in range(len(cys_positions) - 1):\n",
        "            spacing = cys_positions[i + 1] - cys_positions[i]\n",
        "            cys_spacings.append(spacing)\n",
        "\n",
        "        avg_spacing = sum(cys_spacings) / len(cys_spacings) if cys_spacings else 0\n",
        "\n",
        "        # Classify based on cysteine content\n",
        "        if cys_percentage > 15:\n",
        "            cys_class = 'Ultra-high cysteine (>15%)'\n",
        "        elif cys_percentage > 8:\n",
        "            cys_class = 'High cysteine (8-15%)'\n",
        "        elif cys_percentage > 3:\n",
        "            cys_class = 'Moderate cysteine (3-8%)'\n",
        "        else:\n",
        "            cys_class = 'Low cysteine (<3%)'\n",
        "\n",
        "        return {\n",
        "            'total_cysteines': total_cys,\n",
        "            'cysteine_percentage': cys_percentage,\n",
        "            'cysteine_positions': cys_positions,\n",
        "            'cysteine_spacings': cys_spacings,\n",
        "            'average_spacing': avg_spacing,\n",
        "            'cysteine_classification': cys_class,\n",
        "            'is_high_cysteine': cys_percentage > 8\n",
        "        }\n",
        "\n",
        "    def _extract_keratin_domains(self, sequence: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract keratin domains: Head, Rod, Tail.\n",
        "        Simplified domain prediction based on sequence features.\n",
        "        \"\"\"\n",
        "        seq_len = len(sequence)\n",
        "\n",
        "        # Rough domain boundaries (these would need refinement with real data)\n",
        "        # Head domain: typically first 100-150 residues\n",
        "        # Rod domain: central coiled-coil region\n",
        "        # Tail domain: C-terminal region\n",
        "\n",
        "        head_end = min(150, seq_len // 4)\n",
        "        tail_start = max(seq_len - 100, seq_len * 3 // 4)\n",
        "\n",
        "        head_domain = sequence[:head_end]\n",
        "        rod_domain = sequence[head_end:tail_start]\n",
        "        tail_domain = sequence[tail_start:]\n",
        "\n",
        "        return {\n",
        "            'head': {\n",
        "                'sequence': head_domain,\n",
        "                'length': len(head_domain),\n",
        "                'region': f'1-{head_end}'\n",
        "            },\n",
        "            'rod': {\n",
        "                'sequence': rod_domain,\n",
        "                'length': len(rod_domain),\n",
        "                'region': f'{head_end + 1}-{tail_start}'\n",
        "            },\n",
        "            'tail': {\n",
        "                'sequence': tail_domain,\n",
        "                'length': len(tail_domain),\n",
        "                'region': f'{tail_start + 1}-{seq_len}'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _classify_keratin_type(self, sequence: str, gene_symbol: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Classify keratin type based on sequence and gene symbol.\n",
        "        \"\"\"\n",
        "        if gene_symbol and gene_symbol.upper() in self.keratin_types:\n",
        "            known_type = self.keratin_types[gene_symbol.upper()]\n",
        "            return {\n",
        "                'classification': 'known_keratin',\n",
        "                'type': known_type['type'],\n",
        "                'description': known_type['description'],\n",
        "                'confidence': 'high',\n",
        "                'gene_symbol': gene_symbol.upper()\n",
        "            }\n",
        "\n",
        "        # Sequence-based classification\n",
        "        cys_analysis = self._analyze_cysteine_content(sequence)\n",
        "        coiled_coil_analysis = self._analyze_coiled_coil_structure(sequence)\n",
        "\n",
        "        seq_length = len(sequence)\n",
        "        cys_percentage = cys_analysis['cysteine_percentage']\n",
        "        coiled_coil_score = coiled_coil_analysis['coiled_coil_score']\n",
        "\n",
        "        if cys_percentage > 8.0:\n",
        "            predicted_type = \"Hard keratin (high cysteine)\"\n",
        "        elif (coiled_coil_score > 0.5 and\n",
        "              coiled_coil_analysis['alpha_helix_propensity'] > 1.0 and\n",
        "              seq_length > 400):\n",
        "            predicted_type = \"Intermediate filament keratin\"\n",
        "        elif seq_length < 200:\n",
        "            predicted_type = \"Keratin-like protein (short)\"\n",
        "        else:\n",
        "            predicted_type = \"Keratin-like protein (atypical)\"\n",
        "\n",
        "        return {\n",
        "            'classification': 'predicted',\n",
        "            'type': predicted_type,\n",
        "            'confidence': 'medium',\n",
        "            'cysteine_percentage': cys_percentage,\n",
        "            'coiled_coil_score': coiled_coil_score,\n",
        "            'sequence_length': seq_length\n",
        "        }\n",
        "\n",
        "    def _analyze_kap_features(self, sequence: str, gene_symbol: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze Keratin-Associated Protein (KAP) features.\n",
        "        \"\"\"\n",
        "        # KAP detection based on your research\n",
        "        is_kap_candidate = False\n",
        "        kap_type = \"Not KAP\"\n",
        "\n",
        "        cys_analysis = self._analyze_cysteine_content(sequence)\n",
        "        cys_percentage = cys_analysis['cysteine_percentage']\n",
        "        seq_length = len(sequence)\n",
        "\n",
        "        # KAP classification criteria\n",
        "        if gene_symbol and 'KRTAP' in gene_symbol.upper():\n",
        "            is_kap_candidate = True\n",
        "            if cys_percentage > 20:\n",
        "                kap_type = \"High-sulfur KAP\"\n",
        "            elif cys_percentage > 10:\n",
        "                kap_type = \"High-glycine-tyrosine KAP\"\n",
        "            else:\n",
        "                kap_type = \"KAP (low cysteine)\"\n",
        "        elif cys_percentage > 15 and seq_length < 300:\n",
        "            is_kap_candidate = True\n",
        "            kap_type = \"KAP-like (high cysteine, small)\"\n",
        "\n",
        "        # Analyze characteristic amino acid patterns\n",
        "        gly_percentage = (sequence.count('G') / len(sequence) * 100) if len(sequence) > 0 else 0\n",
        "        tyr_percentage = (sequence.count('Y') / len(sequence) * 100) if len(sequence) > 0 else 0\n",
        "        ser_percentage = (sequence.count('S') / len(sequence) * 100) if len(sequence) > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'is_kap_candidate': is_kap_candidate,\n",
        "            'kap_type': kap_type,\n",
        "            'cysteine_percentage': cys_percentage,\n",
        "            'glycine_percentage': gly_percentage,\n",
        "            'tyrosine_percentage': tyr_percentage,\n",
        "            'serine_percentage': ser_percentage,\n",
        "            'sequence_length': seq_length,\n",
        "            'characteristic_composition': {\n",
        "                'high_cysteine': cys_percentage > 15,\n",
        "                'high_glycine': gly_percentage > 15,\n",
        "                'high_tyrosine': tyr_percentage > 8,\n",
        "                'small_protein': seq_length < 300\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Enhanced Keratin class defined\")\n",
        "print(\"‚úÖ All enhanced protein classes ready!\")\n",
        "print(\"üî¨ Features added:\")\n",
        "print(\"   - Detailed Collagen analysis (Gly-X-Y, domains, PTM)\")\n",
        "print(\"   - Comprehensive Keratin analysis (coiled-coil, cysteine)\")\n",
        "print(\"   - KAP (Keratin-Associated Protein) detection\")\n",
        "print(\"   - Advanced structural domain extraction\")\n",
        "print(\"   - Sequence-based classification algorithms\")\n",
        "print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUCpEukhqJ5F",
        "outputId": "32ae57d3-e1b2-41aa-fb8d-fda9b1b6f9b2"
      },
      "id": "jUCpEukhqJ5F",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß¨ Defining Enhanced Protein Classes...\n",
            "‚úÖ Base ProteinClass defined\n",
            "‚úÖ Enhanced Collagen class defined\n",
            "‚úÖ Enhanced Keratin class defined\n",
            "‚úÖ All enhanced protein classes ready!\n",
            "üî¨ Features added:\n",
            "   - Detailed Collagen analysis (Gly-X-Y, domains, PTM)\n",
            "   - Comprehensive Keratin analysis (coiled-coil, cysteine)\n",
            "   - KAP (Keratin-Associated Protein) detection\n",
            "   - Advanced structural domain extraction\n",
            "   - Sequence-based classification algorithms\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data-fetcher-header",
      "metadata": {
        "id": "data-fetcher-header"
      },
      "source": [
        "## Cell 6: Data Fetching & Processing Infrastructure\n",
        "\n",
        "### Smart Data Fetchers and Main Processor\n",
        "\n",
        "This cell defines the `DataFetcher` for handling external API calls and the main `ProteinProcessor` which orchestrates the entire workflow, including data loading, analysis, and the full Nexyme digestion pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 6: Data Fetching & Processing Infrastructure =====\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from Bio import SeqIO\n",
        "from typing import List, Dict, Any\n",
        "import concurrent.futures\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"--- Cell 6: Data Fetching & Processing Infrastructure ---\")\n",
        "\n",
        "class DataFetcher:\n",
        "    \"\"\"Handles all data fetching operations from various sources.\"\"\"\n",
        "\n",
        "    def __init__(self, config: PhASTMConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def get_fasta_sequences(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Load sequences from FASTA files.\"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        # Load from additional FASTA if specified\n",
        "        if hasattr(self.config, 'additional_fasta') and self.config.additional_fasta:\n",
        "            fasta_path = Path(self.config.additional_fasta)\n",
        "            if fasta_path.exists():\n",
        "                print(f\"Loading sequences from: {fasta_path}\")\n",
        "                try:\n",
        "                    with open(fasta_path, 'r') as handle:\n",
        "                        for record in SeqIO.parse(handle, \"fasta\"):\n",
        "                            taxonomy = self._extract_taxonomy_from_header(record.description)\n",
        "                            sequences.append({\n",
        "                                'sequence': str(record.seq),\n",
        "                                'protein_id': record.id,\n",
        "                                'description': record.description,\n",
        "                                'gene': taxonomy.get('gene', 'Unknown'),\n",
        "                                'species': taxonomy.get('species', 'Unknown_species'),\n",
        "                                'accession': taxonomy.get('accession', record.id),\n",
        "                                'source': 'additional_fasta'\n",
        "                            })\n",
        "                    print(f\"‚úÖ Loaded {len(sequences)} sequences from additional FASTA\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error loading FASTA file {fasta_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Additional FASTA file not found: {fasta_path}\")\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def _extract_taxonomy_from_header(self, header: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract taxonomy information from FASTA header\"\"\"\n",
        "        try:\n",
        "            # Common patterns for extracting gene and species from headers\n",
        "            if '|' in header:\n",
        "                parts = header.split('|')\n",
        "                gene = parts[0].strip('>')\n",
        "                species = parts[1] if len(parts) > 1 else 'Unknown_species'\n",
        "            elif '_' in header:\n",
        "                parts = header.split('_', 1)\n",
        "                gene = parts[0].strip('>')\n",
        "                species = parts[1] if len(parts) > 1 else 'Unknown_species'\n",
        "            else:\n",
        "                # Fallback\n",
        "                gene = header.strip('>').split()[0]\n",
        "                species = 'Unknown_species'\n",
        "\n",
        "            return {\n",
        "                'gene': gene,\n",
        "                'species': species.replace(' ', '_'),\n",
        "                'accession': gene\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not extract taxonomy from header '{header}': {e}\")\n",
        "            return {\n",
        "                'gene': 'Unknown',\n",
        "                'species': 'Unknown_species',\n",
        "                'accession': 'Unknown'\n",
        "            }\n",
        "\n",
        "class ProteinProcessor:\n",
        "    \"\"\"Main orchestrator of the PhASTM pipeline.\"\"\"\n",
        "\n",
        "    def __init__(self, config: PhASTMConfig, protein_class_instance):\n",
        "        self.config = config\n",
        "        self.protein_class = protein_class_instance\n",
        "        self.fetcher = DataFetcher(config)\n",
        "        self.master_df = pd.DataFrame()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load sequence data from all configured sources.\"\"\"\n",
        "        print(\"Loading data from configured sources...\")\n",
        "\n",
        "        # Load from FASTA\n",
        "        fasta_seqs = self.fetcher.get_fasta_sequences()\n",
        "        if fasta_seqs:\n",
        "            self.master_df = pd.DataFrame(fasta_seqs)\n",
        "            print(f\"‚úÖ Loaded {len(self.master_df)} total sequences.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No sequences loaded from FASTA sources.\")\n",
        "            self.master_df = pd.DataFrame()\n",
        "\n",
        "    def analyze_sequences(self):\n",
        "        \"\"\"Run protein-family specific analysis on loaded sequences.\"\"\"\n",
        "        if self.master_df.empty:\n",
        "            print(\"‚ö†Ô∏è No sequences to analyze.\")\n",
        "            return\n",
        "\n",
        "        print(\"Running advanced analysis on sequences...\")\n",
        "\n",
        "        # Apply protein-family specific analysis\n",
        "        if isinstance(self.protein_class, Keratin):\n",
        "            print(\"Applying Keratin-specific analysis...\")\n",
        "            self.master_df['subtype'] = self.master_df.apply(\n",
        "                lambda row: self.protein_class.classify_keratin_subtype(\n",
        "                    row['sequence'], row.get('protein_id', '')\n",
        "                ), axis=1\n",
        "            )\n",
        "        elif isinstance(self.protein_class, Collagen):\n",
        "            print(\"Applying Collagen-specific analysis...\")\n",
        "            # Add collagen-specific analysis here\n",
        "            pass\n",
        "\n",
        "        print(f\"‚úÖ Analysis complete for {len(self.master_df)} sequences.\")\n",
        "\n",
        "    def run_digestion_pipeline(self) -> pd.DataFrame:\n",
        "        \"\"\"Run the complete digestion pipeline and return peptide results.\"\"\"\n",
        "        if self.master_df.empty:\n",
        "            print(\"‚ö†Ô∏è No sequences available for digestion.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        print(f\"Starting digestion pipeline with {self.config.enzyme} enzyme...\")\n",
        "\n",
        "        # Simple digestion implementation\n",
        "        all_peptides = []\n",
        "\n",
        "        for idx, row in self.master_df.iterrows():\n",
        "            sequence = row['sequence']\n",
        "\n",
        "            # Simple trypsin-like digestion (cleave after K or R, not before P)\n",
        "            if self.config.enzyme.lower() == 'trypsin':\n",
        "                sites = [0]\n",
        "                for i, aa in enumerate(sequence):\n",
        "                    if aa in ['K', 'R'] and i + 1 < len(sequence) and sequence[i + 1] != 'P':\n",
        "                        sites.append(i + 1)\n",
        "                sites.append(len(sequence))\n",
        "\n",
        "                # Generate peptides\n",
        "                for i in range(len(sites) - 1):\n",
        "                    start, end = sites[i], sites[i + 1]\n",
        "                    peptide_seq = sequence[start:end]\n",
        "\n",
        "                    # Filter by length\n",
        "                    if self.config.min_len <= len(peptide_seq) <= self.config.max_len:\n",
        "                        all_peptides.append({\n",
        "                            'peptide_sequence': peptide_seq,\n",
        "                            'protein_id': row['protein_id'],\n",
        "                            'gene': row.get('gene', 'Unknown'),\n",
        "                            'species': row.get('species', 'Unknown'),\n",
        "                            'start_pos': start,\n",
        "                            'end_pos': end,\n",
        "                            'length': len(peptide_seq)\n",
        "                        })\n",
        "\n",
        "        peptide_df = pd.DataFrame(all_peptides)\n",
        "        print(f\"‚úÖ Generated {len(peptide_df)} peptides from digestion.\")\n",
        "\n",
        "        return peptide_df\n",
        "\n",
        "print(\"‚úÖ DataFetcher and ProteinProcessor classes defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiYvlSEPCltT",
        "outputId": "94d13525-33d6-4d04-efbb-6da8bbc5cd2f"
      },
      "id": "jiYvlSEPCltT",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Cell 6: Data Fetching & Processing Infrastructure ---\n",
            "‚úÖ DataFetcher and ProteinProcessor classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Test Cell 6: Data Loading Infrastructure =====\n",
        "# Test the data loading pipeline and DataFetcher components\n",
        "\n",
        "print(\"üß™ Testing Cell 6: Data Loading Infrastructure...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: Check if data loading functions exist\n",
        "print(\"TEST 1: Data Loading Functions Availability\")\n",
        "required_functions = [\n",
        "    'initialize_data_loading_system',\n",
        "    'load_target_genes_from_tissues',\n",
        "    'load_sequences_from_ensembl_trees',\n",
        "    'load_sequences_from_additional_fasta',\n",
        "    'extract_species_from_fasta_header'\n",
        "]\n",
        "\n",
        "missing_functions = []\n",
        "available_functions = []\n",
        "\n",
        "for func_name in required_functions:\n",
        "    try:\n",
        "        func = globals()[func_name]\n",
        "        available_functions.append(func_name)\n",
        "        print(f\"   ‚úÖ {func_name}\")\n",
        "    except KeyError:\n",
        "        missing_functions.append(func_name)\n",
        "        print(f\"   ‚ùå {func_name}\")\n",
        "\n",
        "cell6_functions = len(missing_functions) == 0\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 2: Test initialize_data_loading_system\n",
        "print(\"TEST 2: Data Loading System Initialization\")\n",
        "if 'initialize_data_loading_system' in available_functions and 'test_config' in globals():\n",
        "    try:\n",
        "        gene_tree_generator, protein_processor = initialize_data_loading_system(test_config)\n",
        "\n",
        "        print(\"‚úÖ Data loading system initialized successfully\")\n",
        "        print(f\"   - Gene tree generator type: {type(gene_tree_generator).__name__}\")\n",
        "        print(f\"   - Protein processor type: {type(protein_processor).__name__}\")\n",
        "        print(f\"   - Entrez email configured: {test_config.entrez_email}\")\n",
        "\n",
        "        # Store for later tests\n",
        "        globals()['test_gene_tree_gen'] = gene_tree_generator\n",
        "        globals()['test_protein_proc'] = protein_processor\n",
        "\n",
        "        cell6_initialization = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Data loading system initialization failed: {e}\")\n",
        "        cell6_initialization = False\n",
        "else:\n",
        "    print(\"‚ùå initialize_data_loading_system function or test_config not available\")\n",
        "    cell6_initialization = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 3: Test target genes loading\n",
        "print(\"TEST 3: Target Genes Loading\")\n",
        "if 'load_target_genes_from_tissues' in available_functions and 'test_config' in globals():\n",
        "    try:\n",
        "        target_genes = load_target_genes_from_tissues(test_config)\n",
        "\n",
        "        print(f\"‚úÖ Target genes loaded: {len(target_genes)} genes\")\n",
        "        print(f\"   - Genes: {target_genes}\")\n",
        "\n",
        "        # Store for later tests\n",
        "        globals()['test_target_genes'] = target_genes\n",
        "\n",
        "        cell6_target_genes = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Target genes loading failed: {e}\")\n",
        "        cell6_target_genes = False\n",
        "else:\n",
        "    print(\"‚ùå load_target_genes_from_tissues function or test_config not available\")\n",
        "    cell6_target_genes = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 4: Test FASTA header parsing\n",
        "print(\"TEST 4: FASTA Header Parsing\")\n",
        "if 'extract_species_from_fasta_header' in available_functions:\n",
        "    try:\n",
        "        test_headers = [\n",
        "            \"COL1A1_HUMAN Collagen alpha-1(I) chain [Homo sapiens]\",\n",
        "            \"sp|P02452|CO1A1_HUMAN Collagen alpha-1(I) chain OS=Homo sapiens\",\n",
        "            \"COL1A1|Homo_sapiens\",\n",
        "            \"gi|123456|ref|NP_000089.3| collagen alpha-1(I) [Homo sapiens]\"\n",
        "        ]\n",
        "\n",
        "        parsed_species = []\n",
        "        for header in test_headers:\n",
        "            species = extract_species_from_fasta_header(header)\n",
        "            parsed_species.append(species)\n",
        "            print(f\"   ‚úÖ '{header[:30]}...' -> '{species}'\")\n",
        "\n",
        "        # Check if we got reasonable species names\n",
        "        valid_parses = sum(1 for s in parsed_species if s != \"Unknown_species\")\n",
        "\n",
        "        if valid_parses >= len(test_headers) // 2:  # At least half should parse correctly\n",
        "            print(f\"‚úÖ FASTA header parsing: {valid_parses}/{len(test_headers)} parsed correctly\")\n",
        "            cell6_header_parsing = True\n",
        "        else:\n",
        "            print(f\"‚ùå FASTA header parsing: only {valid_parses}/{len(test_headers)} parsed correctly\")\n",
        "            cell6_header_parsing = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå FASTA header parsing test failed: {e}\")\n",
        "        cell6_header_parsing = False\n",
        "else:\n",
        "    print(\"‚ùå extract_species_from_fasta_header function not available\")\n",
        "    cell6_header_parsing = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 5: Test data loading functions (without actual file access)\n",
        "print(\"TEST 5: Data Loading Function Structure\")\n",
        "if 'load_sequences_from_additional_fasta' in available_functions and 'test_config' in globals():\n",
        "    try:\n",
        "        # Test the function with our config (will likely return empty since files don't exist)\n",
        "        fasta_sequences = load_sequences_from_additional_fasta(test_config)\n",
        "\n",
        "        print(f\"‚úÖ Additional FASTA loading completed\")\n",
        "        print(f\"   - Sequences loaded: {len(fasta_sequences)}\")\n",
        "        print(f\"   - Function executed without errors\")\n",
        "\n",
        "        cell6_fasta_loading = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Additional FASTA loading test failed: {e}\")\n",
        "        cell6_fasta_loading = False\n",
        "else:\n",
        "    print(\"‚ùå load_sequences_from_additional_fasta function or test_config not available\")\n",
        "    cell6_fasta_loading = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 6: Test checkpoint loading functions (if available)\n",
        "print(\"TEST 6: Data Integration Pipeline Structure\")\n",
        "if cell6_initialization and 'test_protein_proc' in globals():\n",
        "    try:\n",
        "        # Check if protein processor has required methods\n",
        "        required_methods = ['load_data', 'analyze_sequences', 'run_digestion_pipeline']\n",
        "\n",
        "        available_methods = []\n",
        "        for method_name in required_methods:\n",
        "            if hasattr(test_protein_proc, method_name):\n",
        "                available_methods.append(method_name)\n",
        "                print(f\"   ‚úÖ {method_name}\")\n",
        "            else:\n",
        "                print(f\"   ‚ùå {method_name}\")\n",
        "\n",
        "        if len(available_methods) >= 2:  # At least 2 core methods should be available\n",
        "            print(f\"‚úÖ Data integration pipeline: {len(available_methods)}/{len(required_methods)} methods available\")\n",
        "            cell6_integration = True\n",
        "        else:\n",
        "            print(f\"‚ùå Data integration pipeline: only {len(available_methods)}/{len(required_methods)} methods available\")\n",
        "            cell6_integration = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Data integration pipeline test failed: {e}\")\n",
        "        cell6_integration = False\n",
        "else:\n",
        "    print(\"‚ùå Data integration pipeline test skipped (processor not available)\")\n",
        "    cell6_integration = False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Summary for Cell 6\n",
        "print(\"üìä CELL 6 TESTING SUMMARY:\")\n",
        "print(f\"‚úÖ Functions Available: {'PASS' if cell6_functions else 'FAIL'}\")\n",
        "print(f\"‚úÖ System Initialization: {'PASS' if cell6_initialization else 'FAIL'}\")\n",
        "print(f\"‚úÖ Target Genes Loading: {'PASS' if cell6_target_genes else 'FAIL'}\")\n",
        "print(f\"‚úÖ Header Parsing: {'PASS' if cell6_header_parsing else 'FAIL'}\")\n",
        "print(f\"‚úÖ FASTA Loading: {'PASS' if cell6_fasta_loading else 'FAIL'}\")\n",
        "print(f\"‚úÖ Integration Pipeline: {'PASS' if cell6_integration else 'FAIL'}\")\n",
        "\n",
        "cell6_overall = (cell6_functions and cell6_initialization and cell6_target_genes and\n",
        "                cell6_header_parsing and cell6_fasta_loading)\n",
        "print(f\"\\nüéØ CELL 6 STATUS: {'READY' if cell6_overall else 'NEEDS FIXES'}\")\n",
        "\n",
        "if cell6_overall:\n",
        "    print(\"\\n‚úÖ Data Loading Infrastructure is ready!\")\n",
        "    print(\"   - All data loading functions available\")\n",
        "    print(\"   - System initialization working\")\n",
        "    print(\"   - Target gene resolution functional\")\n",
        "    print(\"   - FASTA parsing capabilities verified\")\n",
        "    print(\"   - Ready for actual data loading\")\n",
        "else:\n",
        "    print(\"\\nüîß Fix Cell 6 issues before proceeding\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpriisknvtwo",
        "outputId": "6f9e3f7f-64f8-4dc3-baa7-2067e7859513"
      },
      "id": "zpriisknvtwo",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing Cell 6: Data Loading Infrastructure...\n",
            "============================================================\n",
            "TEST 1: Data Loading Functions Availability\n",
            "   ‚ùå initialize_data_loading_system\n",
            "   ‚ùå load_target_genes_from_tissues\n",
            "   ‚ùå load_sequences_from_ensembl_trees\n",
            "   ‚ùå load_sequences_from_additional_fasta\n",
            "   ‚ùå extract_species_from_fasta_header\n",
            "----------------------------------------\n",
            "TEST 2: Data Loading System Initialization\n",
            "‚ùå initialize_data_loading_system function or test_config not available\n",
            "----------------------------------------\n",
            "TEST 3: Target Genes Loading\n",
            "‚ùå load_target_genes_from_tissues function or test_config not available\n",
            "----------------------------------------\n",
            "TEST 4: FASTA Header Parsing\n",
            "‚ùå extract_species_from_fasta_header function not available\n",
            "----------------------------------------\n",
            "TEST 5: Data Loading Function Structure\n",
            "‚ùå load_sequences_from_additional_fasta function or test_config not available\n",
            "----------------------------------------\n",
            "TEST 6: Data Integration Pipeline Structure\n",
            "‚ùå Data integration pipeline test skipped (processor not available)\n",
            "============================================================\n",
            "üìä CELL 6 TESTING SUMMARY:\n",
            "‚úÖ Functions Available: FAIL\n",
            "‚úÖ System Initialization: FAIL\n",
            "‚úÖ Target Genes Loading: FAIL\n",
            "‚úÖ Header Parsing: FAIL\n",
            "‚úÖ FASTA Loading: FAIL\n",
            "‚úÖ Integration Pipeline: FAIL\n",
            "\n",
            "üéØ CELL 6 STATUS: NEEDS FIXES\n",
            "\n",
            "üîß Fix Cell 6 issues before proceeding\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pipeline-execution-header",
      "metadata": {
        "id": "pipeline-execution-header"
      },
      "source": [
        "## Cell 7: Pipeline Construction\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 7: Complete Data Loading Pipeline =====\n",
        "# Multi-source data integration: Ensembl, FASTA, NCBI\n",
        "# Restores ALL June 28th data loading functionality\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "from Bio import SeqIO, Entrez\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any, Optional\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "print(\"üìä Defining Complete Data Loading Pipeline...\")\n",
        "\n",
        "def initialize_data_loading_system(config):\n",
        "    \"\"\"\n",
        "    Initialize the complete data loading system.\n",
        "    Sets up all required components and validates paths.\n",
        "    \"\"\"\n",
        "    print(\"üîß Initializing data loading system...\")\n",
        "\n",
        "    # Set up Entrez for NCBI access\n",
        "    Entrez.email = config.entrez_email\n",
        "    print(f\"‚úÖ Entrez email set: {config.entrez_email}\")\n",
        "\n",
        "    # Initialize SmartGeneTreeGenerator\n",
        "    gene_tree_generator = SmartGeneTreeGenerator(config)\n",
        "    print(\"‚úÖ SmartGeneTreeGenerator initialized\")\n",
        "\n",
        "    # Initialize appropriate protein processor\n",
        "    if config.target_gene_family.upper().startswith('COL'):\n",
        "        protein_processor = Collagen(config)\n",
        "        print(\"‚úÖ Collagen processor initialized\")\n",
        "    elif config.target_gene_family.upper().startswith('KRT') or config.target_gene_family.upper().startswith('KRTAP'):\n",
        "        protein_processor = Keratin(config)\n",
        "        print(\"‚úÖ Keratin processor initialized\")\n",
        "    else:\n",
        "        protein_processor = ProteinClass(config)\n",
        "        print(\"‚úÖ Generic protein processor initialized\")\n",
        "\n",
        "    return gene_tree_generator, protein_processor\n",
        "\n",
        "def load_target_genes_from_tissues(config) -> List[str]:\n",
        "    \"\"\"\n",
        "    Load target genes based on tissue selection.\n",
        "    \"\"\"\n",
        "    print(f\"üéØ Loading target genes for tissue: {config.reference_tissue}\")\n",
        "\n",
        "    try:\n",
        "        with open(config.tissues_json_path, 'r') as f:\n",
        "            tissues_data = json.load(f)\n",
        "\n",
        "        if config.reference_tissue in tissues_data:\n",
        "            target_genes = tissues_data[config.reference_tissue]\n",
        "            print(f\"‚úÖ Found {len(target_genes)} target genes: {target_genes}\")\n",
        "            return target_genes\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Tissue '{config.reference_tissue}' not found in tissues file\")\n",
        "            # Fallback to common collagen genes\n",
        "            fallback_genes = [\"COL1A1\", \"COL1A2\", \"COL2A1\", \"COL3A1\"]\n",
        "            print(f\"üìã Using fallback genes: {fallback_genes}\")\n",
        "            return fallback_genes\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Tissues file not found: {config.tissues_json_path}\")\n",
        "        fallback_genes = [\"COL1A1\", \"COL1A2\"]\n",
        "        print(f\"üìã Using minimal fallback genes: {fallback_genes}\")\n",
        "        return fallback_genes\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Error parsing tissues JSON: {e}\")\n",
        "        return [\"COL1A1\"]\n",
        "\n",
        "def load_sequences_from_ensembl_trees(config, gene_tree_generator, target_genes) -> Dict[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Load sequences from Ensembl gene trees for target genes.\n",
        "    \"\"\"\n",
        "    print(\"üå≥ Loading sequences from Ensembl gene trees...\")\n",
        "\n",
        "    # Check for checkpoint\n",
        "    checkpoint_path = config.checkpoint_base_path / \"ensembl_tree_sequences.pkl\"\n",
        "\n",
        "    if (config.enable_checkpointing and\n",
        "        checkpoint_path.exists() and\n",
        "        not config.force_recompute_checkpoints):\n",
        "\n",
        "        print(\"üìÇ Loading Ensembl sequences from checkpoint...\")\n",
        "        try:\n",
        "            with open(checkpoint_path, 'rb') as f:\n",
        "                ensembl_sequences = pickle.load(f)\n",
        "            print(f\"‚úÖ Loaded checkpoint: {sum(len(seqs) for seqs in ensembl_sequences.values())} sequences\")\n",
        "            return ensembl_sequences\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load checkpoint: {e}\")\n",
        "\n",
        "    # Fetch sequences from Ensembl\n",
        "    print(\"üîç Fetching sequences from Ensembl API...\")\n",
        "    ensembl_sequences = gene_tree_generator.fetch_sequences_for_gene_list(target_genes)\n",
        "\n",
        "    # Save checkpoint\n",
        "    if config.enable_checkpointing and ensembl_sequences:\n",
        "        try:\n",
        "            with open(checkpoint_path, 'wb') as f:\n",
        "                pickle.dump(ensembl_sequences, f)\n",
        "            print(f\"üíæ Saved Ensembl sequences checkpoint\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to save checkpoint: {e}\")\n",
        "\n",
        "    total_sequences = sum(len(seqs) for seqs in ensembl_sequences.values())\n",
        "    print(f\"‚úÖ Ensembl loading complete: {total_sequences} sequences from {len(ensembl_sequences)} genes\")\n",
        "\n",
        "    return ensembl_sequences\n",
        "\n",
        "def load_sequences_from_additional_fasta(config) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load sequences from additional FASTA files.\n",
        "    \"\"\"\n",
        "    print(\"üìÑ Loading sequences from additional FASTA files...\")\n",
        "\n",
        "    fasta_sequences = []\n",
        "\n",
        "    # Try the configured additional FASTA path\n",
        "    if config.additional_collagen_fasta_path and config.additional_collagen_fasta_path.exists():\n",
        "        print(f\"üìÇ Loading from: {config.additional_collagen_fasta_path}\")\n",
        "\n",
        "        try:\n",
        "            with open(config.additional_collagen_fasta_path, 'r') as f:\n",
        "                for record in SeqIO.parse(f, 'fasta'):\n",
        "                    sequence_data = {\n",
        "                        'protein_id': record.id,\n",
        "                        'description': record.description,\n",
        "                        'sequence': str(record.seq),\n",
        "                        'length': len(record.seq),\n",
        "                        'source': 'additional_fasta',\n",
        "                        'gene_family': config.target_gene_family,\n",
        "                        'species': extract_species_from_fasta_header(record.description)\n",
        "                    }\n",
        "                    fasta_sequences.append(sequence_data)\n",
        "\n",
        "            print(f\"‚úÖ Loaded {len(fasta_sequences)} sequences from additional FASTA\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading additional FASTA: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No additional FASTA file specified or file not found\")\n",
        "\n",
        "    return fasta_sequences\n",
        "\n",
        "def extract_species_from_fasta_header(header: str) -> str:\n",
        "    \"\"\"Extract species name from FASTA header.\"\"\"\n",
        "    # Try different patterns for species extraction\n",
        "    patterns = [\n",
        "        r'\\[([A-Za-z]+\\s+[A-Za-z]+)\\]',  # [Homo sapiens]\n",
        "        r'OS=([A-Za-z]+\\s+[A-Za-z]+)',   # OS=Homo sapiens\n",
        "        r'_([A-Z]{3,5})\\s',              # _HUMAN\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, header)\n",
        "        if match:\n",
        "            species = match.group(1)\n",
        "            # Convert organism codes to species names\n",
        "            if species.isupper():\n",
        "                organism_codes = {\n",
        "                    'HUMAN': 'Homo sapiens',\n",
        "                    'MOUSE': 'Mus musculus',\n",
        "                    'RAT': 'Rattus norvegicus',\n",
        "                    'CHICK': 'Gallus gallus',\n",
        "                    'ZEBRA': 'Danio rerio',\n",
        "                    'BOVIN': 'Bos taurus'\n",
        "                }\n",
        "                return organism_codes.get(species, f\"Unknown_{species}\")\n",
        "            else:\n",
        "                return species.replace('_', ' ')\n",
        "\n",
        "    return \"Unknown_species\"\n",
        "\n",
        "def load_sequences_from_ncbi_nr(config, target_genes) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load additional sequences from NCBI NR database.\n",
        "    \"\"\"\n",
        "    if not config.include_ncbi_nr_novel_species:\n",
        "        print(\"‚è≠Ô∏è NCBI NR fetching disabled\")\n",
        "        return []\n",
        "\n",
        "    print(\"üî¨ Loading sequences from NCBI NR...\")\n",
        "\n",
        "    # Check for checkpoint\n",
        "    checkpoint_path = config.checkpoint_base_path / \"ncbi_nr_sequences.pkl\"\n",
        "\n",
        "    if (config.enable_checkpointing and\n",
        "        checkpoint_path.exists() and\n",
        "        not config.force_recompute_checkpoints):\n",
        "\n",
        "        print(\"üìÇ Loading NCBI sequences from checkpoint...\")\n",
        "        try:\n",
        "            with open(checkpoint_path, 'rb') as f:\n",
        "                ncbi_sequences = pickle.load(f)\n",
        "            print(f\"‚úÖ Loaded checkpoint: {len(ncbi_sequences)} sequences\")\n",
        "            return ncbi_sequences\n",
        "        except Exception"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "avnXalY-uaI9",
        "outputId": "ed21fb50-3a30-44cd-d068-1a3dc0b9b48c"
      },
      "id": "avnXalY-uaI9",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (ipython-input-18-2923471440.py, line 201)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-18-2923471440.py\"\u001b[0;36m, line \u001b[0;32m201\u001b[0m\n\u001b[0;31m    except Exception\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Test Cell 7: Digestion Pipeline =====\n",
        "# Test the complete digestion pipeline and related functions\n",
        "\n",
        "print(\"üß™ Testing Cell 7: Digestion Pipeline...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: Check if digestion pipeline functions exist\n",
        "print(\"TEST 1: Digestion Pipeline Functions Availability\")\n",
        "expected_functions = [\n",
        "    'run_complete_digestion_pipeline',\n",
        "    'generate_peptide_library',\n",
        "    'analyze_peptide_conservation',\n",
        "    'generate_final_reports'\n",
        "]\n",
        "\n",
        "# Note: Some functions might be methods of classes instead of standalone functions\n",
        "available_pipeline_functions = []\n",
        "missing_pipeline_functions = []\n",
        "\n",
        "for func_name in expected_functions:\n",
        "    try:\n",
        "        func = globals()[func_name]\n",
        "        available_pipeline_functions.append(func_name)\n",
        "        print(f\"   ‚úÖ {func_name}\")\n",
        "    except KeyError:\n",
        "        missing_pipeline_functions.append(func_name)\n",
        "        print(f\"   ‚ùå {func_name} (may be implemented as class method)\")\n",
        "\n",
        "# Don't fail if some are missing - they might be class methods\n",
        "cell7_functions = len(available_pipeline_functions) >= 1\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 2: Test enzyme digestion capabilities\n",
        "print(\"TEST 2: Enzyme Digestion Capabilities\")\n",
        "if 'test_config' in globals():\n",
        "    try:\n",
        "        # Test enzyme rules access\n",
        "        available_enzymes = list(test_config.enzyme_rules.keys())\n",
        "        print(f\"‚úÖ Available enzymes: {len(available_enzymes)}\")\n",
        "        print(f\"   - Enzymes: {available_enzymes[:5]}{'...' if len(available_enzymes) > 5 else ''}\")\n",
        "\n",
        "        # Test enzyme property access\n",
        "        test_enzyme = \"trypsin\"\n",
        "        if test_enzyme in test_config.enzyme_rules:\n",
        "            enzyme_props = test_config.get_enzyme_properties(test_enzyme)\n",
        "            if enzyme_props:\n",
        "                print(f\"‚úÖ Enzyme properties accessible for {test_enzyme}\")\n",
        "                print(f\"   - Cleavage rule: {enzyme_props.get('cleavage_rule', 'N/A')}\")\n",
        "                print(f\"   - Description: {enzyme_props.get('description', 'N/A')}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Could not get enzyme properties for {test_enzyme}\")\n",
        "\n",
        "        cell7_enzymes = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Enzyme digestion test failed: {e}\")\n",
        "        cell7_enzymes = False\n",
        "else:\n",
        "    print(\"‚ùå test_config not available\")\n",
        "    cell7_enzymes = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 3: Test digestion with protein processor\n",
        "print(\"TEST 3: Protein Processor Digestion\")\n",
        "if 'test_protein_proc' in globals():\n",
        "    try:\n",
        "        # Test if the protein processor has digestion capabilities\n",
        "        if hasattr(test_protein_proc, 'run_digestion_pipeline'):\n",
        "            print(\"‚úÖ run_digestion_pipeline method available\")\n",
        "        else:\n",
        "            print(\"‚ùå run_digestion_pipeline method not found\")\n",
        "\n",
        "        # Test basic sequence digestion\n",
        "        if hasattr(test_protein_proc, 'digest_sequence_with_enzyme'):\n",
        "            test_sequence = \"MKRGKRGKRGPGPGPGPGPGPG\"\n",
        "            peptides = test_protein_proc.digest_sequence_with_enzyme(test_sequence, \"trypsin\")\n",
        "            print(f\"‚úÖ Sequence digestion working: {len(peptides)} peptides generated\")\n",
        "            print(f\"   - Example peptides: {peptides[:3] if len(peptides) >= 3 else peptides}\")\n",
        "        else:\n",
        "            print(\"‚ùå digest_sequence_with_enzyme method not found\")\n",
        "\n",
        "        cell7_processor_digestion = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Protein processor digestion test failed: {e}\")\n",
        "        cell7_processor_digestion = False\n",
        "else:\n",
        "    print(\"‚ùå test_protein_proc not available from previous tests\")\n",
        "    cell7_processor_digestion = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 4: Test checkpoint system for digestion\n",
        "print(\"TEST 4: Digestion Checkpoint System\")\n",
        "if 'test_config' in globals():\n",
        "    try:\n",
        "        # Test checkpoint configuration\n",
        "        print(f\"‚úÖ Checkpointing enabled: {test_config.enable_checkpointing}\")\n",
        "        print(f\"‚úÖ Force recompute: {test_config.force_recompute_checkpoints}\")\n",
        "        print(f\"‚úÖ Checkpoint path: {test_config.checkpoint_base_path}\")\n",
        "\n",
        "        # Test checkpoint methods\n",
        "        if hasattr(test_config, 'save_checkpoint') and hasattr(test_config, 'load_checkpoint'):\n",
        "            print(\"‚úÖ Checkpoint save/load methods available\")\n",
        "\n",
        "            # Test checkpoint functionality with dummy data\n",
        "            test_data = {\"test\": \"data\", \"peptides\": [\"PEPTIDE1\", \"PEPTIDE2\"]}\n",
        "            checkpoint_name = \"test_digestion_checkpoint\"\n",
        "\n",
        "            # Try to save and load\n",
        "            saved = test_config.save_checkpoint(test_data, checkpoint_name)\n",
        "            if saved:\n",
        "                loaded_data = test_config.load_checkpoint(checkpoint_name)\n",
        "                if loaded_data and loaded_data.get(\"test\") == \"data\":\n",
        "                    print(\"‚úÖ Checkpoint save/load functional\")\n",
        "                else:\n",
        "                    print(\"‚ùå Checkpoint load failed or data corrupted\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Checkpoint save disabled or failed\")\n",
        "        else:\n",
        "            print(\"‚ùå Checkpoint methods not available\")\n",
        "\n",
        "        cell7_checkpoints = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Checkpoint system test failed: {e}\")\n",
        "        cell7_checkpoints = False\n",
        "else:\n",
        "    print(\"‚ùå test_config not available\")\n",
        "    cell7_checkpoints = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 5: Test parallel processing configuration\n",
        "print(\"TEST 5: Parallel Processing Configuration\")\n",
        "if 'test_config' in globals():\n",
        "    try:\n",
        "        # Test parallel processing settings\n",
        "        print(f\"‚úÖ Parallel digestion enabled: {test_config.enable_parallel_digestion}\")\n",
        "        print(f\"‚úÖ Digestion workers: {test_config.num_digestion_workers}\")\n",
        "        print(f\"‚úÖ Parallel MRCA enabled: {test_config.enable_parallel_mrca}\")\n",
        "        print(f\"‚úÖ MRCA workers: {test_config.num_mrca_workers}\")\n",
        "\n",
        "        # Check if concurrent.futures is available for parallel processing\n",
        "        try:\n",
        "            import concurrent.futures\n",
        "            print(\"‚úÖ concurrent.futures available for parallel processing\")\n",
        "        except ImportError:\n",
        "            print(\"‚ùå concurrent.futures not available\")\n",
        "\n",
        "        cell7_parallel = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Parallel processing configuration test failed: {e}\")\n",
        "        cell7_parallel = False\n",
        "else:\n",
        "    print(\"‚ùå test_config not available\")\n",
        "    cell7_parallel = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 6: Test peptide filtering parameters\n",
        "print(\"TEST 6: Peptide Filtering Parameters\")\n",
        "if 'test_config' in globals():\n",
        "    try:\n",
        "        # Test peptide length filtering\n",
        "        print(f\"‚úÖ Min peptide length: {test_config.min_len}\")\n",
        "        print(f\"‚úÖ Max peptide length: {test_config.max_len}\")\n",
        "        print(f\"‚úÖ Missed cleavages allowed: {test_config.missed_cleavages}\")\n",
        "        print(f\"‚úÖ Convert I/L to B: {test_config.convert_il_to_b}\")\n",
        "\n",
        "        # Test display parameters\n",
        "        print(f\"‚úÖ Min conserved peptide length (display): {test_config.min_conserved_peptide_length_display}\")\n",
        "\n",
        "        # Test anchor peptide parameters\n",
        "        if hasattr(test_config, 'min_anchor_peptide_length_num'):\n",
        "            print(f\"‚úÖ Min anchor peptide length: {test_config.min_anchor_peptide_length_num}\")\n",
        "            print(f\"‚úÖ Max anchor peptide length: {test_config.max_anchor_peptide_length_num}\")\n",
        "            print(f\"‚úÖ Min Gly proportion at 3rd pos: {test_config.min_gly_at_third_pos_proportion}\")\n",
        "\n",
        "        # Validate parameter ranges\n",
        "        valid_params = (\n",
        "            6 <= test_config.min_len <= test_config.max_len <= 50 and\n",
        "            0 <= test_config.missed_cleavages <= 5 and\n",
        "            0 <= test_config.min_conserved_peptide_length_display <= 20\n",
        "        )\n",
        "\n",
        "        if valid_params:\n",
        "            print(\"‚úÖ All peptide filtering parameters in valid ranges\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Some peptide filtering parameters may be outside expected ranges\")\n",
        "\n",
        "        cell7_filtering = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Peptide filtering parameters test failed: {e}\")\n",
        "        cell7_filtering = False\n",
        "else:\n",
        "    print(\"‚ùå test_config not available\")\n",
        "    cell7_filtering = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 7: Test complete pipeline integration readiness\n",
        "print(\"TEST 7: Pipeline Integration Readiness\")\n",
        "integration_ready = True\n",
        "integration_issues = []\n",
        "\n",
        "# Check if all prerequisite components are available\n",
        "prerequisites = [\n",
        "    ('test_config', 'Configuration'),\n",
        "    ('test_protein_proc', 'Protein Processor'),\n",
        "    ('test_gene_tree_gen', 'Gene Tree Generator'),\n",
        "    ('test_target_genes', 'Target Genes')\n",
        "]\n",
        "\n",
        "for var_name, component_name in prerequisites:\n",
        "    if var_name in globals():\n",
        "        print(f\"   ‚úÖ {component_name} available\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {component_name} missing\")\n",
        "        integration_ready = False\n",
        "        integration_issues.append(component_name)\n",
        "\n",
        "# Check if protein processor has loaded data capability\n",
        "if 'test_protein_proc' in globals():\n",
        "    if hasattr(test_protein_proc, 'master_df'):\n",
        "        print(\"   ‚úÖ Master DataFrame initialized\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Master DataFrame not found\")\n",
        "        integration_ready = False\n",
        "        integration_issues.append(\"Master DataFrame\")\n",
        "\n",
        "if integration_ready:\n",
        "    print(\"‚úÖ All components ready for full pipeline execution\")\n",
        "    cell7_integration = True\n",
        "else:\n",
        "    print(f\"‚ùå Integration issues: {', '.join(integration_issues)}\")\n",
        "    cell7_integration = False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Summary for Cell 7\n",
        "print(\"üìä CELL 7 TESTING SUMMARY:\")\n",
        "print(f\"‚úÖ Pipeline Functions: {'PASS' if cell7_functions else 'FAIL'}\")\n",
        "print(f\"‚úÖ Enzyme Capabilities: {'PASS' if cell7_enzymes else 'FAIL'}\")\n",
        "print(f\"‚úÖ Processor Digestion: {'PASS' if cell7_processor_digestion else 'FAIL'}\")\n",
        "print(f\"‚úÖ Checkpoint System: {'PASS' if cell7_checkpoints else 'FAIL'}\")\n",
        "print(f\"‚úÖ Parallel Processing: {'PASS' if cell7_parallel else 'FAIL'}\")\n",
        "print(f\"‚úÖ Filtering Parameters: {'PASS' if cell7_filtering else 'FAIL'}\")\n",
        "print(f\"‚úÖ Integration Ready: {'PASS' if cell7_integration else 'FAIL'}\")\n",
        "\n",
        "cell7_overall = (cell7_functions and cell7_enzymes and cell7_processor_digestion and\n",
        "                cell7_checkpoints and cell7_parallel and cell7_filtering and cell7_integration)\n",
        "\n",
        "print(f\"\\nüéØ CELL 7 STATUS: {'READY' if cell7_overall else 'NEEDS FIXES'}\")\n",
        "\n",
        "if cell7_overall:\n",
        "    print(\"\\n‚úÖ Digestion Pipeline is ready!\")\n",
        "    print(\"   - All enzyme digestion capabilities functional\")\n",
        "    print(\"   - Checkpoint system operational\")\n",
        "    print(\"   - Parallel processing configured\")\n",
        "    print(\"   - Peptide filtering parameters validated\")\n",
        "    print(\"   - Integration with previous components verified\")\n",
        "    print(\"   - Ready for full pipeline execution\")\n",
        "else:\n",
        "    print(\"\\nüîß Fix Cell 7 issues before running full pipeline\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "lEpYwXkbwKva"
      },
      "id": "lEpYwXkbwKva",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Test Cell 7: Digestion Pipeline =====\n",
        "# Test the complete digestion pipeline and related functions\n",
        "\n",
        "print(\"üß™ Testing Cell 7: Digestion Pipeline...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test 1: Check if digestion pipeline functions exist\n",
        "print(\"TEST 1: Digestion Pipeline Functions Availability\")\n",
        "expected_functions = [\n",
        "    'run_complete_digestion_pipeline',\n",
        "    'generate_peptide_library',\n",
        "    'analyze_peptide_conservation',\n",
        "    'generate_final_reports'\n",
        "]\n",
        "\n",
        "# Note: Some functions might be methods of classes instead of standalone functions\n",
        "available_pipeline_functions = []\n",
        "missing_pipeline_functions = []\n",
        "\n",
        "for func_name in expected_functions:\n",
        "    try:\n",
        "        func = globals()[func_name]\n",
        "        available_pipeline_functions.append(func_name)\n",
        "        print(f\"   ‚úÖ {func_name}\")\n",
        "    except KeyError:\n",
        "        missing_pipeline_functions.append(func_name)\n",
        "        print(f\"   ‚ùå {func_name} (may be implemented as class method)\")\n",
        "\n",
        "# Don't fail if some are missing - they might be class methods\n",
        "cell7_functions = len(available_pipeline_functions) >= 1\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 2: Test enzyme digestion capabilities\n",
        "print(\"TEST 2: Enzyme Digestion Capabilities\")\n",
        "if 'test_config' in globals():\n",
        "    try:\n",
        "        # Test enzyme rules access\n",
        "        available_enzymes = list(test_config.enzyme_rules.keys())\n",
        "        print(f\"‚úÖ Available enzymes: {len(available_enzymes)}\")\n",
        "        print(f\"   - Enzymes: {available_enzymes[:5]}{'...' if len(available_enzymes) > 5 else ''}\")\n",
        "\n",
        "        # Test enzyme property access\n",
        "        test_enzyme = \"trypsin\"\n",
        "        if test_enzyme in test_config.enzyme_rules:\n",
        "            enzyme_props = test_config.get_enzyme_properties(test_enzyme)\n",
        "            if enzyme_props:\n",
        "                print(f\"‚úÖ Enzyme properties accessible for {test_enzyme}\")\n",
        "                print(f\"   - Cleavage rule: {enzyme_props.get('cleavage_rule', 'N/A')}\")\n",
        "                print(f\"   - Description: {enzyme_props.get('description', 'N/A')}\")\n",
        "            else:\n",
        "                print(f\"‚ùå Could not get enzyme properties for {test_enzyme}\")\n",
        "\n",
        "        cell7_enzymes = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Enzyme digestion test failed: {e}\")\n",
        "        cell7_enzymes = False\n",
        "else:\n",
        "    print(\"‚ùå test_config not available\")\n",
        "    cell7_enzymes = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 3: Test digestion with protein processor\n",
        "print(\"TEST 3: Protein Processor Digestion\")\n",
        "if 'test_protein_proc' in globals():\n",
        "    try:\n",
        "        # Test if the protein processor has digestion capabilities\n",
        "        if hasattr(test_protein_proc, 'run_digestion_pipeline'):\n",
        "            print(\"‚úÖ run_digestion_pipeline method available\")\n",
        "        else:\n",
        "            print(\"‚ùå run_digestion_pipeline method not found\")\n",
        "\n",
        "        # Test basic sequence digestion\n",
        "        if hasattr(test_protein_proc, 'digest_sequence_with_enzyme'):\n",
        "            test_sequence = \"MKRGKRGKRGPGPGPGPGPGPG\"\n",
        "            peptides = test_protein_proc.digest_sequence_with_enzyme(test_sequence, \"trypsin\")\n",
        "            print(f\"‚úÖ Sequence digestion working: {len(peptides)} peptides generated\")\n",
        "            print(f\"   - Example peptides: {peptides[:3] if len(peptides) >= 3 else peptides}\")\n",
        "        else:\n",
        "            print(\"‚ùå digest_sequence_with_enzyme method not found\")\n",
        "\n",
        "        cell7_processor_digestion = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Protein processor digestion test failed: {e}\")\n",
        "        cell7_processor_digestion = False\n",
        "else:\n",
        "    print(\"‚ùå test_protein_proc not available from previous tests\")\n",
        "    cell7_processor_digestion = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 4: Test checkpoint system for digestion\n",
        "print(\"TEST 4: Digestion Checkpoint System\")\n",
        "if 'test_config' in globals():\n",
        "    try:\n",
        "        # Test checkpoint configuration\n",
        "        print(f\"‚úÖ Checkpointing enabled: {test_config.enable_checkpointing}\")\n",
        "        print(f\"‚úÖ Force recompute: {test_config.force_recompute_checkpoints}\")\n",
        "        print(f\"‚úÖ Checkpoint path: {test_config.checkpoint_base_path}\")\n",
        "\n",
        "        # Test checkpoint methods\n",
        "        if hasattr(test_config, 'save_checkpoint') and hasattr(test_config, 'load_checkpoint'):\n",
        "            print(\"‚úÖ Checkpoint save/load methods available\")\n",
        "\n",
        "            # Test checkpoint functionality with dummy data\n",
        "            test_data = {\"test\": \"data\", \"peptides\": [\"PEPTIDE1\", \"PEPTIDE2\"]}\n",
        "            checkpoint_name = \"test_digestion_checkpoint\"\n",
        "\n",
        "            # Try to save and load\n",
        "            saved = test_config.save_checkpoint(test_data, checkpoint_name)\n",
        "            if saved:\n",
        "                loaded_data = test_config.load_checkpoint(checkpoint_name)\n",
        "                if loaded_data and loaded_data.get(\"test\") == \"data\":\n",
        "                    print(\"‚úÖ Checkpoint save/load functional\")\n",
        "                else:\n",
        "                    print(\"‚ùå Checkpoint load failed or data corrupted\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Checkpoint save disabled or failed\")\n",
        "        else:\n",
        "            print(\"‚ùå Checkpoint methods not available\")\n",
        "\n",
        "        cell7_checkpoints = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Checkpoint system test failed: {e}\")\n",
        "        cell7_checkpoints = False\n",
        "else:\n",
        "    print(\"‚ùå test_config not available\")\n",
        "    cell7_checkpoints = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 5: Test parallel processing configuration\n",
        "print(\"TEST 5: Parallel Processing Configuration\")\n",
        "if 'test_config' in globals():\n",
        "    try:\n",
        "        # Test parallel processing settings\n",
        "        print(f\"‚úÖ Parallel digestion enabled: {test_config.enable_parallel_digestion}\")\n",
        "        print(f\"‚úÖ Digestion workers: {test_config.num_digestion_workers}\")\n",
        "        print(f\"‚úÖ Parallel MRCA enabled: {test_config.enable_parallel_mrca}\")\n",
        "        print(f\"‚úÖ MRCA workers: {test_config.num_mrca_workers}\")\n",
        "\n",
        "        # Check if concurrent.futures is available for parallel processing\n",
        "        try:\n",
        "            import concurrent.futures\n",
        "            print(\"‚úÖ concurrent.futures available for parallel processing\")\n",
        "        except ImportError:\n",
        "            print(\"‚ùå concurrent.futures not available\")\n",
        "\n",
        "        cell7_parallel = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Parallel processing configuration test failed: {e}\")\n",
        "        cell7_parallel = False\n",
        "else:\n",
        "    print(\"‚ùå test_config not available\")\n",
        "    cell7_parallel = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 6: Test peptide filtering parameters\n",
        "print(\"TEST 6: Peptide Filtering Parameters\")\n",
        "if 'test_config' in globals():\n",
        "    try:\n",
        "        # Test peptide length filtering\n",
        "        print(f\"‚úÖ Min peptide length: {test_config.min_len}\")\n",
        "        print(f\"‚úÖ Max peptide length: {test_config.max_len}\")\n",
        "        print(f\"‚úÖ Missed cleavages allowed: {test_config.missed_cleavages}\")\n",
        "        print(f\"‚úÖ Convert I/L to B: {test_config.convert_il_to_b}\")\n",
        "\n",
        "        # Test display parameters\n",
        "        print(f\"‚úÖ Min conserved peptide length (display): {test_config.min_conserved_peptide_length_display}\")\n",
        "\n",
        "        # Test anchor peptide parameters\n",
        "        if hasattr(test_config, 'min_anchor_peptide_length_num'):\n",
        "            print(f\"‚úÖ Min anchor peptide length: {test_config.min_anchor_peptide_length_num}\")\n",
        "            print(f\"‚úÖ Max anchor peptide length: {test_config.max_anchor_peptide_length_num}\")\n",
        "            print(f\"‚úÖ Min Gly proportion at 3rd pos: {test_config.min_gly_at_third_pos_proportion}\")\n",
        "\n",
        "        # Validate parameter ranges\n",
        "        valid_params = (\n",
        "            6 <= test_config.min_len <= test_config.max_len <= 50 and\n",
        "            0 <= test_config.missed_cleavages <= 5 and\n",
        "            0 <= test_config.min_conserved_peptide_length_display <= 20\n",
        "        )\n",
        "\n",
        "        if valid_params:\n",
        "            print(\"‚úÖ All peptide filtering parameters in valid ranges\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Some peptide filtering parameters may be outside expected ranges\")\n",
        "\n",
        "        cell7_filtering = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Peptide filtering parameters test failed: {e}\")\n",
        "        cell7_filtering = False\n",
        "else:\n",
        "    print(\"‚ùå test_config not available\")\n",
        "    cell7_filtering = False\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test 7: Test complete pipeline integration readiness\n",
        "print(\"TEST 7: Pipeline Integration Readiness\")\n",
        "integration_ready = True\n",
        "integration_issues = []\n",
        "\n",
        "# Check if all prerequisite components are available\n",
        "prerequisites = [\n",
        "    ('test_config', 'Configuration'),\n",
        "    ('test_protein_proc', 'Protein Processor'),\n",
        "    ('test_gene_tree_gen', 'Gene Tree Generator'),\n",
        "    ('test_target_genes', 'Target Genes')\n",
        "]\n",
        "\n",
        "for var_name, component_name in prerequisites:\n",
        "    if var_name in globals():\n",
        "        print(f\"   ‚úÖ {component_name} available\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {component_name} missing\")\n",
        "        integration_ready = False\n",
        "        integration_issues.append(component_name)\n",
        "\n",
        "# Check if protein processor has loaded data capability\n",
        "if 'test_protein_proc' in globals():\n",
        "    if hasattr(test_protein_proc, 'master_df'):\n",
        "        print(\"   ‚úÖ Master DataFrame initialized\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Master DataFrame not found\")\n",
        "        integration_ready = False\n",
        "        integration_issues.append(\"Master DataFrame\")\n",
        "\n",
        "if integration_ready:\n",
        "    print(\"‚úÖ All components ready for full pipeline execution\")\n",
        "    cell7_integration = True\n",
        "else:\n",
        "    print(f\"‚ùå Integration issues: {', '.join(integration_issues)}\")\n",
        "    cell7_integration = False\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Summary for Cell 7\n",
        "print(\"üìä CELL 7 TESTING SUMMARY:\")\n",
        "print(f\"‚úÖ Pipeline Functions: {'PASS' if cell7_functions else 'FAIL'}\")\n",
        "print(f\"‚úÖ Enzyme Capabilities: {'PASS' if cell7_enzymes else 'FAIL'}\")\n",
        "print(f\"‚úÖ Processor Digestion: {'PASS' if cell7_processor_digestion else 'FAIL'}\")\n",
        "print(f\"‚úÖ Checkpoint System: {'PASS' if cell7_checkpoints else 'FAIL'}\")\n",
        "print(f\"‚úÖ Parallel Processing: {'PASS' if cell7_parallel else 'FAIL'}\")\n",
        "print(f\"‚úÖ Filtering Parameters: {'PASS' if cell7_filtering else 'FAIL'}\")\n",
        "print(f\"‚úÖ Integration Ready: {'PASS' if cell7_integration else 'FAIL'}\")\n",
        "\n",
        "cell7_overall = (cell7_functions and cell7_enzymes and cell7_processor_digestion and\n",
        "                cell7_checkpoints and cell7_parallel and cell7_filtering and cell7_integration)\n",
        "\n",
        "print(f\"\\nüéØ CELL 7 STATUS: {'READY' if cell7_overall else 'NEEDS FIXES'}\")\n",
        "\n",
        "if cell7_overall:\n",
        "    print(\"\\n‚úÖ Digestion Pipeline is ready!\")\n",
        "    print(\"   - All enzyme digestion capabilities functional\")\n",
        "    print(\"   - Checkpoint system operational\")\n",
        "    print(\"   - Parallel processing configured\")\n",
        "    print(\"   - Peptide filtering parameters validated\")\n",
        "    print(\"   - Integration with previous components verified\")\n",
        "    print(\"   - Ready for full pipeline execution\")\n",
        "else:\n",
        "    print(\"\\nüîß Fix Cell 7 issues before running full pipeline\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "ejY_TNRUwPt3"
      },
      "id": "ejY_TNRUwPt3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 8 Pipeline Exection\n",
        "## Running the PhASTM Workflow\n",
        "\n",
        "This final cell initializes all the classes and runs the complete, restored pipeline from data loading through to advanced analysis and digestion."
      ],
      "metadata": {
        "id": "Gj26W4Whuj5Q"
      },
      "id": "Gj26W4Whuj5Q"
    },
    {
      "cell_type": "code",
      "id": "pipeline-execution",
      "metadata": {
        "id": "pipeline-execution"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "def run_full_pipeline():\n",
        "    print(\"üöÄ Initializing PhASTM System...\")\n",
        "    try:\n",
        "        # Instead of using globals(), explicitly pass the required parameters\n",
        "        config_params = {\n",
        "            'workspace_root': workspace_root,\n",
        "            'additional_fasta': additional_fasta,  # Add the missing additional_fasta parameter\n",
        "            'target_gene_family': target_gene_family,\n",
        "            'reference_tissue': reference_tissue,\n",
        "            'enzyme': enzyme,\n",
        "            'entrez_email': entrez_email,\n",
        "            'enable_checkpointing': enable_checkpointing,\n",
        "            'force_recompute_checkpoints': force_recompute_checkpoints,\n",
        "            'force_update_architecture_cache': force_update_architecture_cache,\n",
        "            'include_ncbi_nr_novel_species': include_ncbi_nr_novel_species,\n",
        "            'max_ncbi_nr_sequences': max_ncbi_nr_sequences,\n",
        "            'missed_cleavages': missed_cleavages,\n",
        "            'min_len': min_len,\n",
        "            'max_len': max_len,\n",
        "            'convert_il_to_b': convert_il_to_b,\n",
        "            'enable_parallel_digestion': enable_parallel_digestion,\n",
        "            'num_digestion_workers': num_digestion_workers,\n",
        "            'enable_parallel_mrca': enable_parallel_mrca,\n",
        "            'num_mrca_workers': num_mrca_workers,\n",
        "            'use_full_model_species_list': use_full_model_species_list,\n",
        "            'model_species': model_species,\n",
        "            'min_anchor_peptide_length_num': min_anchor_peptide_length_num,\n",
        "            'max_anchor_peptide_length_num': max_anchor_peptide_length_num,\n",
        "            'min_gly_at_third_pos_proportion': min_gly_at_third_pos_proportion,\n",
        "            'top_n_anchor_candidates_report': top_n_anchor_candidates_report,\n",
        "            'reference_species_for_anchoring': reference_species_for_anchoring,\n",
        "            'min_conserved_peptide_length_display': min_conserved_peptide_length_display\n",
        "        }\n",
        "\n",
        "        config = PhASTMConfig(**config_params)\n",
        "\n",
        "        if config.target_gene_family == 'Collagen':\n",
        "            protein_class_instance = Collagen(config)\n",
        "        elif config.target_gene_family == 'Keratin':\n",
        "            protein_class_instance = Keratin(config)\n",
        "        else:\n",
        "            protein_class_instance = ProteinClass(config)\n",
        "\n",
        "        processor = ProteinProcessor(config, protein_class_instance)\n",
        "\n",
        "        # --- Run Workflow ---\n",
        "        print(\"\\n--- Starting Data Loading ---\")\n",
        "        processor.load_data()\n",
        "\n",
        "        print(\"\\n--- Starting Sequence Analysis ---\")\n",
        "        processor.analyze_sequences()\n",
        "\n",
        "        print(\"\\n--- Starting Digestion Pipeline ---\")\n",
        "        peptide_map = processor.run_digestion_pipeline()\n",
        "\n",
        "        print(\"\\nüéâ PhASTM Pipeline Completed Successfully! üéâ\")\n",
        "        if peptide_map is not None:\n",
        "            print(f\"Generated {len(peptide_map)} peptides.\")\n",
        "            display(peptide_map.head())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå A fatal error occurred in the pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# --- Execute the pipeline ---\n",
        "if __name__ == '__main__' and 'get_ipython' in locals():\n",
        "    if entrez_email == \"your.email@example.com\":\n",
        "        print(\"‚ÄºÔ∏è ACTION REQUIRED: Please update 'entrez_email' in Cell 2.\")\n",
        "    else:\n",
        "        run_full_pipeline()"
      ]
    }
  ]
}