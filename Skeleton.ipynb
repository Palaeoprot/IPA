{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "150uYW4avYEQJuPFex6ZBaN_8ps--UuTQ",
      "authorship_tag": "ABX9TyM4ysXBhPhnzl2itJkvMYFq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/IPA/blob/main/Skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Import Libraries and Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.cm as cm\n",
        "import math\n",
        "import json\n",
        "import gspread\n",
        "from datetime import datetime\n",
        "import random\n",
        "from scipy.optimize import differential_evolution, curve_fit\n",
        "from scipy.interpolate import interp1d\n",
        "import logging\n",
        "from typing import List, Dict, Tuple\n",
        "import configparser\n",
        "import argparse"
      ],
      "metadata": {
        "id": "nGfTzmkY_b37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded6e32e-b3ef-4196-89ce-ce1ffbbf4c43"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class ModulAAR:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.data = None\n",
        "        self.processed_data = None\n",
        "        self.logger = self._setup_logger()\n",
        "\n",
        "    def _setup_logger(self):\n",
        "        from datetime import datetime\n",
        "        return lambda msg, level=\"INFO\": print(f\"{datetime.now()} - {level} - {msg}\")\n",
        "\n",
        "    def load_data(self, source, is_gsheet=False):\n",
        "        \"\"\"Load data from a CSV file or Google Sheets.\"\"\"\n",
        "        try:\n",
        "            if is_gsheet:\n",
        "                sheet_id = source['sheet_id']\n",
        "                gid = source['gid']\n",
        "                export_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}\"\n",
        "                self.data = pd.read_csv(export_url)\n",
        "                self.logger(f\"Data loaded successfully from Google Sheet\")\n",
        "            else:\n",
        "                self.data = pd.read_csv(source)\n",
        "                self.logger(f\"Data loaded successfully from {source}\")\n",
        "        except Exception as e:\n",
        "            self.logger(f\"Error loading data: {str(e)}\", \"ERROR\")\n",
        "            self.data = None\n",
        "        return self.data\n",
        "\n",
        "    def process_data(self, df=None):\n",
        "        \"\"\"Process the loaded data based on its type.\"\"\"\n",
        "        if df is None:\n",
        "            df = self.data\n",
        "        if df is None:\n",
        "            self.logger(\"No data available for processing.\", \"ERROR\")\n",
        "            return None\n",
        "\n",
        "        df = df.dropna()\n",
        "\n",
        "        # Flexible column renaming\n",
        "        rename_mapping = {\n",
        "            'time (h)': 'time',\n",
        "            'temp': 'temp (°C)',\n",
        "            'Pre-heat bleach time': 'Pre-heat bleach time (h)'\n",
        "            # Add other potential column mappings here if needed\n",
        "        }\n",
        "        df.rename(columns=rename_mapping, inplace=True)\n",
        "\n",
        "        expected_eggshell_columns = [\n",
        "            'Pre-heat bleach time (h)', 'temp (°C)', 'pH', 'sample', 'time',\n",
        "            '[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]', '[Phe]', '[Ile]',\n",
        "            'Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L', 'Phe D/L', 'Ile D/L'\n",
        "        ]\n",
        "        expected_betalactoglobulin_columns = ['Condition', 'Fraction', 'm']\n",
        "\n",
        "        if all(col in df.columns for col in expected_eggshell_columns):\n",
        "            self.logger(\"Processing eggshell data\")\n",
        "            return self._process_eggshell_data(df)\n",
        "        elif all(col in df.columns for col in expected_betalactoglobulin_columns):\n",
        "            self.logger(\"Processing beta-lactoglobulin data\")\n",
        "            return self._process_betalactoglobulin_data(df)\n",
        "        else:\n",
        "            self.logger(\n",
        "                f\"Unknown data format. Expected columns for eggshell: {expected_eggshell_columns}, \"\n",
        "                f\"or beta-lactoglobulin: {expected_betalactoglobulin_columns}. Found columns: {list(df.columns)}\",\n",
        "                \"ERROR\"\n",
        "            )\n",
        "            return None\n",
        "\n",
        "    def _process_eggshell_data(self, df):\n",
        "        \"\"\"Process eggshell data.\"\"\"\n",
        "        df = self.clean_data(df)\n",
        "        grouped_faa, grouped_thaa = self.calculate_stats(df)\n",
        "\n",
        "        processed_df = pd.merge(grouped_faa, grouped_thaa, on=['temp (°C)', 'time'], how='outer')\n",
        "\n",
        "        # Add 'temp (K)' column\n",
        "        if 'temp (°C)' in processed_df.columns:\n",
        "            processed_df['temp (K)'] = processed_df['temp (°C)'] + 273.15\n",
        "\n",
        "        self.logger(\"Eggshell data processed successfully\")\n",
        "        return processed_df\n",
        "\n",
        "    def _process_betalactoglobulin_data(self, df):\n",
        "        \"\"\"Process beta-lactoglobulin data.\"\"\"\n",
        "        relevant_columns = ['Condition', 'Fraction', 'm', '[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]',\n",
        "                            'Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L']\n",
        "        df = df[relevant_columns]\n",
        "\n",
        "        # Convert concentration and D/L columns to numeric\n",
        "        for col in df.columns:\n",
        "            if col.startswith('[') or col.endswith('D/L'):\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Group by condition and calculate statistics\n",
        "        grouped = df.groupby(['Condition', 'Fraction', 'm']).agg({\n",
        "            col: ['mean', 'std'] for col in df.columns if col not in ['Condition', 'Fraction', 'm']\n",
        "        }).reset_index()\n",
        "\n",
        "        # Flatten column names\n",
        "        grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
        "\n",
        "        self.logger(\"Beta-lactoglobulin data processed successfully\")\n",
        "        return grouped\n",
        "\n",
        "    def clean_data(self, df):\n",
        "        \"\"\"Clean the eggshell data by converting to numeric and filtering.\"\"\"\n",
        "        concentration_columns = ['[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]', '[Phe]', '[Ile]']\n",
        "        dl_columns = ['Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L', 'Phe D/L', 'Ile D/L']\n",
        "\n",
        "        for col in concentration_columns + dl_columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        df = df.dropna(subset=concentration_columns + dl_columns)\n",
        "        return df\n",
        "\n",
        "    def calculate_stats(self, df):\n",
        "        \"\"\"Calculate statistics for FAA and THAA samples.\"\"\"\n",
        "        faa_df = df[df['sample'] == 'FAA']\n",
        "        thaa_df = df[df['sample'] == 'THAA']\n",
        "\n",
        "        grouped_faa = faa_df.groupby(['temp (°C)', 'time']).agg({\n",
        "            '[Asx]': ['mean', 'std', 'count'],\n",
        "            '[Glx]': ['mean', 'std', 'count'],\n",
        "            '[Ser]': ['mean', 'std', 'count'],\n",
        "            '[Ala]': ['mean', 'std', 'count'],\n",
        "            '[Val]': ['mean', 'std', 'count'],\n",
        "            '[Phe]': ['mean', 'std', 'count'],\n",
        "            '[Ile]': ['mean', 'std', 'count'],\n",
        "            'Asx D/L': ['mean', 'std', 'count'],\n",
        "            'Glx D/L': ['mean', 'std', 'count'],\n",
        "            'Ser D/L': ['mean', 'std', 'count'],\n",
        "            'Ala D/L': ['mean', 'std', 'count'],\n",
        "            'Val D/L': ['mean', 'std', 'count'],\n",
        "            'Phe D/L': ['mean', 'std', 'count'],\n",
        "            'Ile D/L': ['mean', 'std', 'count']\n",
        "        }).reset_index()\n",
        "\n",
        "        grouped_thaa = thaa_df.groupby(['temp (°C)', 'time']).agg({\n",
        "            '[Asx]': ['mean', 'std', 'count'],\n",
        "            '[Glx]': ['mean', 'std', 'count'],\n",
        "            '[Ser]': ['mean', 'std', 'count'],\n",
        "            '[Ala]': ['mean', 'std', 'count'],\n",
        "            '[Val]': ['mean', 'std', 'count'],\n",
        "            '[Phe]': ['mean', 'std', 'count'],\n",
        "            '[Ile]': ['mean', 'std', 'count'],\n",
        "            'Asx D/L': ['mean', 'std', 'count'],\n",
        "            'Glx D/L': ['mean', 'std', 'count'],\n",
        "            'Ser D/L': ['mean', 'std', 'count'],\n",
        "            'Ala D/L': ['mean', 'std', 'count'],\n",
        "            'Val D/L': ['mean', 'std', 'count'],\n",
        "            'Phe D/L': ['mean', 'std', 'count'],\n",
        "            'Ile D/L': ['mean', 'std', 'count']\n",
        "        }).reset_index()\n",
        "\n",
        "        new_columns_faa = ['temp (°C)', 'time'] + [\n",
        "            f'{aa}_Conc_FAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']] + [\n",
        "            f'{aa}_D/L_FAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']]\n",
        "        new_columns_thaa = ['temp (°C)', 'time'] + [\n",
        "            f'{aa}_Conc_THAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']] + [\n",
        "            f'{aa}_D/L_THAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']]\n",
        "\n",
        "        grouped_faa.columns = new_columns_faa\n",
        "        grouped_thaa.columns = new_columns_thaa\n",
        "\n",
        "        for col in new_columns_faa:\n",
        "            if '_Std' in col:\n",
        "                count_col = col.replace('_Std', '_Count')\n",
        "                grouped_faa.loc[grouped_faa[count_col] == 1, col] = 0\n",
        "\n",
        "        for col in new_columns_thaa:\n",
        "            if '_Std' in col:\n",
        "                count_col = col.replace('_Std', '_Count')\n",
        "                grouped_thaa.loc[grouped_thaa[count_col] == 1, col] = 0\n",
        "\n",
        "        return grouped_faa, grouped_thaa\n",
        "\n",
        "    def calculate_amino_acid_distribution(self, df, amino_acid):\n",
        "        \"\"\"Calculate the distribution of a given amino acid.\"\"\"\n",
        "        if df is None:\n",
        "            self.logger(\"No processed data available. Please process data first.\", \"ERROR\")\n",
        "            return None\n",
        "\n",
        "        amino_acid_distributions = []\n",
        "\n",
        "        initial_thaa = df[f\"{amino_acid}_Conc_THAA_Mean\"].iloc[0]\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            thaa = row[f\"{amino_acid}_Conc_THAA_Mean\"]\n",
        "            faa = row[f\"{amino_acid}_Conc_FAA_Mean\"]\n",
        "            thaa_dl = row[f\"{amino_acid}_D/L_THAA_Mean\"]\n",
        "            faa_dl = row[f\"{amino_acid}_D/L_FAA_Mean\"]\n",
        "            time_point = row[\"time\"]\n",
        "\n",
        "            # Calculate BAA, FAA, and losses\n",
        "            baa = thaa - faa\n",
        "            baa_d = baa * thaa_dl / (1 + thaa_dl)\n",
        "            baa_l = baa - baa_d\n",
        "            faa_d = faa * faa_dl / (1 + faa_dl)\n",
        "            faa_l = faa - faa_d\n",
        "            total_loss = max(initial_thaa - thaa, 0)\n",
        "            loss_d = total_loss * faa_dl / (1 + faa_dl) if total_loss > 0 else 0\n",
        "            loss_l = total_loss - loss_d if total_loss > 0 else 0\n",
        "\n",
        "            # Calculate standard deviations\n",
        "            thaa_std = row[f\"{amino_acid}_Conc_THAA_Std\"]\n",
        "            faa_std = row[f\"{amino_acid}_Conc_FAA_Std\"]\n",
        "            baa_std = np.sqrt(thaa_std**2 + faa_std**2)\n",
        "            baa_d_std = baa_std * thaa_dl / (1 + thaa_dl)\n",
        "            baa_l_std = baa_std - baa_d_std\n",
        "            faa_d_std = faa_std * faa_dl / (1 + faa_dl)\n",
        "            faa_l_std = faa_std - faa_d_std\n",
        "            loss_std = np.sqrt(2 * thaa_std**2)\n",
        "            loss_d_std = loss_std * faa_dl / (1 + faa_dl)\n",
        "            loss_l_std = loss_std - loss_d_std\n",
        "\n",
        "            amino_acid_distributions.append({\n",
        "                \"time\": time_point,\n",
        "                \"temp (K)\": row[\"temp (K)\"],\n",
        "                \"BAA_D\": baa_d, \"BAA_L\": baa_l,\n",
        "                \"BAA_D_Std\": baa_d_std, \"BAA_L_Std\": baa_l_std,\n",
        "                \"FAA_D\": faa_d, \"FAA_L\": faa_l,\n",
        "                \"FAA_D_Std\": faa_d_std, \"FAA_L_Std\": faa_l_std,\n",
        "                \"FAA_D_loss\": loss_d, \"FAA_L_loss\": loss_l,\n",
        "                \"FAA_D_loss_Std\": loss_d_std, \"FAA_L_loss_Std\": loss_l_std,\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(amino_acid_distributions)\n",
        "\n",
        "\n",
        "# Usage\n",
        "config = {\n",
        "    'N': 50000,\n",
        "    'fold_water': 8,\n",
        "    'temperature_kelvin': 353.15,\n",
        "    'amino_acid': 'Asx',\n",
        "    'initial_length': 1500,\n",
        "    'rate_params': {\n",
        "        # Add rate parameters here\n",
        "    }\n",
        "}\n",
        "\n",
        "model = ModulAAR(config)\n",
        "\n",
        "# Load data from Google Sheets\n",
        "sheet_info = {'sheet_id': \"1nA6jSAkAf1Ud-kHdaYTMtBPgKhe9nBg_IjM9idLlj8E\", 'gid': \"1259514505\"}\n",
        "df1 = model.load_data(sheet_info, is_gsheet=True)\n",
        "\n",
        "# Process data\n",
        "if df1 is not None:\n",
        "    processed_df1 = model.process_data(df1)\n",
        "    if processed_df1 is not None:\n",
        "        print(processed_df1.head())\n",
        "\n",
        "        # Calculate amino acid distribution for Asx\n",
        "        asx_distribution = model.calculate_amino_acid_distribution(processed_df1, 'Asx')\n",
        "        print(asx_distribution.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIw0LW5qHmCV",
        "outputId": "fa8bd12e-d9d5-43fb-9ed7-e13aecea0b21"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-05 18:22:05.963365 - INFO - Data loaded successfully from Google Sheet\n",
            "2024-07-05 18:22:05.968569 - INFO - Processing eggshell data\n",
            "2024-07-05 18:22:06.058944 - INFO - Eggshell data processed successfully\n",
            "   temp (°C)   time  Asx_Conc_FAA_Mean  Asx_Conc_FAA_Std  Asx_Conc_FAA_Count  \\\n",
            "0         80    0.0          43.333333          8.082904                   3   \n",
            "1         80   24.0          83.250000         35.453021                   4   \n",
            "2         80   96.0         161.000000          0.000000                   1   \n",
            "3         80   97.3          67.333333         15.044379                   3   \n",
            "4         80  120.0         171.714286         18.034359                   7   \n",
            "\n",
            "   Glx_Conc_FAA_Mean  Glx_Conc_FAA_Std  Glx_Conc_FAA_Count  Ser_Conc_FAA_Mean  \\\n",
            "0          25.666667          5.033223                   3         166.000000   \n",
            "1          52.000000         21.400935                   4         163.250000   \n",
            "2          95.000000          0.000000                   1         107.000000   \n",
            "3          40.333333          8.020806                   3         174.000000   \n",
            "4         106.142857         17.218484                   7         153.142857   \n",
            "\n",
            "   Ser_Conc_FAA_Std  ...  Val_D/L_THAA_Mean  Val_D/L_THAA_Std  \\\n",
            "0         21.000000  ...           0.030000          0.000000   \n",
            "1         11.324752  ...           0.028571          0.013452   \n",
            "2          0.000000  ...           0.037500          0.005000   \n",
            "3         34.510868  ...           0.040000          0.000000   \n",
            "4         41.969944  ...           0.037143          0.004880   \n",
            "\n",
            "   Val_D/L_THAA_Count  Phe_D/L_THAA_Mean  Phe_D/L_THAA_Std  \\\n",
            "0                 3.0           0.040000          0.000000   \n",
            "1                 7.0           0.072857          0.009512   \n",
            "2                 4.0           0.082500          0.009574   \n",
            "3                 3.0           0.080000          0.000000   \n",
            "4                 7.0           0.081429          0.008997   \n",
            "\n",
            "   Phe_D/L_THAA_Count  Ile_D/L_THAA_Mean  Ile_D/L_THAA_Std  \\\n",
            "0                 3.0           0.023333          0.005774   \n",
            "1                 7.0           0.025714          0.018127   \n",
            "2                 4.0           0.027500          0.005000   \n",
            "3                 3.0           0.026667          0.005774   \n",
            "4                 7.0           0.032857          0.004880   \n",
            "\n",
            "   Ile_D/L_THAA_Count  temp (K)  \n",
            "0                 3.0    353.15  \n",
            "1                 7.0    353.15  \n",
            "2                 4.0    353.15  \n",
            "3                 3.0    353.15  \n",
            "4                 7.0    353.15  \n",
            "\n",
            "[5 rows x 87 columns]\n",
            "    time  temp (K)        BAA_D        BAA_L   BAA_D_Std    BAA_L_Std  \\\n",
            "0    0.0    353.15   577.294118  4329.705882  197.172633  1478.794749   \n",
            "1   24.0    353.15   775.396299  4175.210843   91.599870   493.230070   \n",
            "2   96.0    353.15  1034.784254  4650.715746  160.848957   722.916661   \n",
            "3   97.3    353.15   950.366577  4015.633423  147.575636   623.559026   \n",
            "4  120.0    353.15   952.239973  4114.617169  169.593176   732.810018   \n",
            "\n",
            "       FAA_D       FAA_L  FAA_D_Std  FAA_L_Std  FAA_D_loss  FAA_L_loss  \\\n",
            "0   4.058409   39.274924   0.757009   7.325895         0.0         0.0   \n",
            "1  12.096154   71.153846   5.151294  30.301727         0.0         0.0   \n",
            "2  35.218750  125.781250   0.000000   0.000000         0.0         0.0   \n",
            "3  11.685950   55.647383   2.611008  12.433371         0.0         0.0   \n",
            "4  39.335431  132.378855   4.131219  13.903140         0.0         0.0   \n",
            "\n",
            "   FAA_D_loss_Std  FAA_L_loss_Std  \n",
            "0      221.977629     2148.170608  \n",
            "1      119.952192      705.601129  \n",
            "2      273.401039      976.432283  \n",
            "3      189.232829      901.108708  \n",
            "4      292.284889      983.651070  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-b6148bff994b>:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.rename(columns=rename_mapping, inplace=True)\n",
            "<ipython-input-28-b6148bff994b>:114: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = pd.to_numeric(df[col], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ModulAAR Core\n",
        "class ModulAAR:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.data = None\n",
        "        self.processed_data = None\n",
        "        self.logger = self._setup_logger()\n",
        "\n",
        "    def _setup_logger(self):\n",
        "        from datetime import datetime\n",
        "        return lambda msg, level=\"INFO\": print(f\"{datetime.now()} - {level} - {msg}\")\n",
        "\n",
        "    def load_data(self, source, is_gsheet=False):\n",
        "        \"\"\"Load data from a CSV file or Google Sheets.\"\"\"\n",
        "        try:\n",
        "            if is_gsheet:\n",
        "                sheet_id = source['sheet_id']\n",
        "                gid = source['gid']\n",
        "                export_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}\"\n",
        "                self.data = pd.read_csv(export_url)\n",
        "                self.logger(f\"Data loaded successfully from Google Sheet\")\n",
        "            else:\n",
        "                self.data = pd.read_csv(source)\n",
        "                self.logger(f\"Data loaded successfully from {source}\")\n",
        "        except Exception as e:\n",
        "            self.logger(f\"Error loading data: {str(e)}\", \"ERROR\")\n",
        "            self.data = None\n",
        "        return self.data\n",
        "\n",
        "    def process_data(self, df=None):\n",
        "        \"\"\"Process the loaded data based on its type.\"\"\"\n",
        "        if df is None:\n",
        "            df = self.data\n",
        "        if df is None:\n",
        "            self.logger(\"No data available for processing.\", \"ERROR\")\n",
        "            return None\n",
        "\n",
        "        df = df.dropna()\n",
        "\n",
        "        expected_eggshell_columns = [\n",
        "            'Pre-heat bleach time (h)', 'temp (°C)', 'pH', 'sample', 'time',\n",
        "            '[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]', '[Phe]', '[Ile]',\n",
        "            'Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L', 'Phe D/L', 'Ile D/L'\n",
        "        ]\n",
        "        expected_betalactoglobulin_columns = ['Condition', 'Fraction', 'm']\n",
        "\n",
        "        if all(col in df.columns for col in expected_eggshell_columns):\n",
        "            self.logger(\"Processing eggshell data\")\n",
        "            return self._process_eggshell_data(df)\n",
        "        elif all(col in df.columns for col in expected_betalactoglobulin_columns):\n",
        "            self.logger(\"Processing beta-lactoglobulin data\")\n",
        "            return self._process_betalactoglobulin_data(df)\n",
        "        else:\n",
        "            self.logger(\n",
        "                f\"Unknown data format. Expected columns for eggshell: {expected_eggshell_columns}, \"\n",
        "                f\"or beta-lactoglobulin: {expected_betalactoglobulin_columns}. Found columns: {list(df.columns)}\",\n",
        "                \"ERROR\"\n",
        "            )\n",
        "            return None\n",
        "\n",
        "    def _process_eggshell_data(self, df):\n",
        "        \"\"\"Process eggshell data.\"\"\"\n",
        "        df = self.clean_data(df)\n",
        "        grouped_faa, grouped_thaa = self.calculate_stats(df)\n",
        "\n",
        "        processed_df = pd.merge(grouped_faa, grouped_thaa, on=['temp (°C)', 'time'], how='outer')\n",
        "\n",
        "        processed_df['temp (K)'] = processed_df['temp (°C)'] + 273.15\n",
        "\n",
        "        self.logger(\"Eggshell data processed successfully\")\n",
        "        return processed_df\n",
        "\n",
        "    def _process_betalactoglobulin_data(self, df):\n",
        "        \"\"\"Process beta-lactoglobulin data.\"\"\"\n",
        "        relevant_columns = ['Condition', 'Fraction', 'm', '[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]',\n",
        "                            'Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L']\n",
        "        df = df[relevant_columns]\n",
        "\n",
        "        # Convert concentration and D/L columns to numeric\n",
        "        for col in df.columns:\n",
        "            if col.startswith('[') or col.endswith('D/L'):\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Group by condition and calculate statistics\n",
        "        grouped = df.groupby(['Condition', 'Fraction', 'm']).agg({\n",
        "            col: ['mean', 'std'] for col in df.columns if col not in ['Condition', 'Fraction', 'm']\n",
        "        }).reset_index()\n",
        "\n",
        "        # Flatten column names\n",
        "        grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
        "\n",
        "        self.logger(\"Beta-lactoglobulin data processed successfully\")\n",
        "        return grouped\n",
        "\n",
        "    def clean_data(self, df):\n",
        "        \"\"\"Clean the eggshell data by converting to numeric and filtering.\"\"\"\n",
        "        concentration_columns = ['[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]', '[Phe]', '[Ile]']\n",
        "        dl_columns = ['Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L', 'Phe D/L', 'Ile D/L']\n",
        "\n",
        "        for col in concentration_columns + dl_columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        df = df.dropna(subset=concentration_columns + dl_columns)\n",
        "        return df\n",
        "\n",
        "    def calculate_stats(self, df):\n",
        "        \"\"\"Calculate statistics for FAA and THAA samples.\"\"\"\n",
        "        faa_df = df[df['sample'] == 'FAA']\n",
        "        thaa_df = df[df['sample'] == 'THAA']\n",
        "\n",
        "        grouped_faa = faa_df.groupby(['temp (°C)', 'time']).agg({\n",
        "            '[Asx]': ['mean', 'std', 'count'],\n",
        "            '[Glx]': ['mean', 'std', 'count'],\n",
        "            '[Ser]': ['mean', 'std', 'count'],\n",
        "            '[Ala]': ['mean', 'std', 'count'],\n",
        "            '[Val]': ['mean', 'std', 'count'],\n",
        "            '[Phe]': ['mean', 'std', 'count'],\n",
        "            '[Ile]': ['mean', 'std', 'count'],\n",
        "            'Asx D/L': ['mean', 'std', 'count'],\n",
        "            'Glx D/L': ['mean', 'std', 'count'],\n",
        "            'Ser D/L': ['mean', 'std', 'count'],\n",
        "            'Ala D/L': ['mean', 'std', 'count'],\n",
        "            'Val D/L': ['mean', 'std', 'count'],\n",
        "            'Phe D/L': ['mean', 'std', 'count'],\n",
        "            'Ile D/L': ['mean', 'std', 'count']\n",
        "        }).reset_index()\n",
        "\n",
        "        grouped_thaa = thaa_df.groupby(['temp (°C)', 'time']).agg({\n",
        "            '[Asx]': ['mean', 'std', 'count'],\n",
        "            '[Glx]': ['mean', 'std', 'count'],\n",
        "            '[Ser]': ['mean', 'std', 'count'],\n",
        "            '[Ala]': ['mean', 'std', 'count'],\n",
        "            '[Val]': ['mean', 'std', 'count'],\n",
        "            '[Phe]': ['mean', 'std', 'count'],\n",
        "            '[Ile]': ['mean', 'std', 'count'],\n",
        "            'Asx D/L': ['mean', 'std', 'count'],\n",
        "            'Glx D/L': ['mean', 'std', 'count'],\n",
        "            'Ser D/L': ['mean', 'std', 'count'],\n",
        "            'Ala D/L': ['mean', 'std', 'count'],\n",
        "            'Val D/L': ['mean', 'std', 'count'],\n",
        "            'Phe D/L': ['mean', 'std', 'count'],\n",
        "            'Ile D/L': ['mean', 'std', 'count']\n",
        "        }).reset_index()\n",
        "\n",
        "        new_columns_faa = ['temp (°C)', 'time'] + [\n",
        "            f'{aa}_Conc_FAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']] + [\n",
        "            f'{aa}_D/L_FAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']]\n",
        "        new_columns_thaa = ['temp (°C)', 'time'] + [\n",
        "            f'{aa}_Conc_THAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']] + [\n",
        "            f'{aa}_D/L_THAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']]\n",
        "\n",
        "        grouped_faa.columns = new_columns_faa\n",
        "        grouped_thaa.columns = new_columns_thaa\n",
        "\n",
        "        for col in new_columns_faa:\n",
        "            if '_Std' in col:\n",
        "                count_col = col.replace('_Std', '_Count')\n",
        "                grouped_faa.loc[grouped_faa[count_col] == 1, col] = 0\n",
        "\n",
        "        for col in new_columns_thaa:\n",
        "            if '_Std' in col:\n",
        "                count_col = col.replace('_Std', '_Count')\n",
        "                grouped_thaa.loc[grouped_thaa[count_col] == 1, col] = 0\n",
        "\n",
        "        return grouped_faa, grouped_thaa\n",
        "\n",
        "    def calculate_amino_acid_distribution(self, df, amino_acid):\n",
        "        \"\"\"Calculate the distribution of a given amino acid.\"\"\"\n",
        "        if df is None:\n",
        "            self.logger(\"No processed data available. Please process data first.\", \"ERROR\")\n",
        "            return None\n",
        "\n",
        "        amino_acid_distributions = []\n",
        "\n",
        "        initial_thaa = df[f\"{amino_acid}_Conc_THAA_Mean\"].iloc[0]\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            thaa = row[f\"{amino_acid}_Conc_THAA_Mean\"]\n",
        "            faa = row[f\"{amino_acid}_Conc_FAA_Mean\"]\n",
        "            thaa_dl = row[f\"{amino_acid}_D/L_THAA_Mean\"]\n",
        "            faa_dl = row[f\"{amino_acid}_D/L_FAA_Mean\"]\n",
        "            time_point = row[\"time\"]\n",
        "\n",
        "            # Calculate BAA, FAA, and losses\n",
        "            baa = thaa - faa\n",
        "            baa_d = baa * thaa_dl / (1 + thaa_dl)\n",
        "            baa_l = baa - baa_d\n",
        "            faa_d = faa * faa_dl / (1 + faa_dl)\n",
        "            faa_l = faa - faa_d\n",
        "            total_loss = max(initial_thaa - thaa, 0)\n",
        "            loss_d = total_loss * faa_dl / (1 + faa_dl) if total_loss > 0 else 0\n",
        "            loss_l = total_loss - loss_d if total_loss > 0 else 0\n",
        "\n",
        "            # Calculate standard deviations\n",
        "            thaa_std = row[f\"{amino_acid}_Conc_THAA_Std\"]\n",
        "            faa_std = row[f\"{amino_acid}_Conc_FAA_Std\"]\n",
        "            baa_std = np.sqrt(thaa_std**2 + faa_std**2)\n",
        "            baa_d_std = baa_std * thaa_dl / (1 + thaa_dl)\n",
        "            baa_l_std = baa_std - baa_d_std\n",
        "            faa_d_std = faa_std * faa_dl / (1 + faa_dl)\n",
        "            faa_l_std = faa_std - faa_d_std\n",
        "            loss_std = np.sqrt(2 * thaa_std**2)\n",
        "            loss_d_std = loss_std * faa_dl / (1 + faa_dl)\n",
        "            loss_l_std = loss_std - loss_d_std\n",
        "\n",
        "            amino_acid_distributions.append({\n",
        "                \"time\": time_point,\n",
        "                \"temp (K)\": row[\"temp (K)\"],\n",
        "                \"BAA_D\": baa_d, \"BAA_L\": baa_l,\n",
        "                \"BAA_D_Std\": baa_d_std, \"BAA_L_Std\": baa_l_std,\n",
        "                \"FAA_D\": faa_d, \"FAA_L\": faa_l,\n",
        "                \"FAA_D_Std\": faa_d_std, \"FAA_L_Std\": faa_l_std,\n",
        "                \"FAA_D_loss\": loss_d, \"FAA_L_loss\": loss_l,\n",
        "                \"FAA_D_loss_Std\": loss_d_std, \"FAA_L_loss_Std\": loss_l_std,\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(amino_acid_distributions)\n",
        "\n",
        "# Usage\n",
        "config = {\n",
        "    'N': 50000,\n",
        "    'fold_water': 8,\n",
        "    'temperature_kelvin': 353.15,\n",
        "    'amino_acid': 'Asx',\n",
        "    'initial_length': 1500,\n",
        "    'rate_params': {\n",
        "        # Add rate parameters here\n",
        "    }\n",
        "}\n",
        "\n",
        "model = ModulAAR(config)\n",
        "\n",
        "# Load data from Google Sheets\n",
        "sheet_info = {'sheet_id': \"1nA6jSAkAf1Ud-kHdaYTMtBPgKhe9nBg_IjM9idLlj8E\", 'gid': \"1259514505\"}\n",
        "df1 = model.load_data(sheet_info, is_gsheet=True)\n",
        "\n",
        "# Process data\n",
        "if df1 is not None:\n",
        "    processed_df1 = model.process_data(df1)\n",
        "    if processed_df1 is not None:\n",
        "        print(processed_df1.head())\n",
        "\n",
        "        # Calculate amino acid distribution for Asx\n",
        "        asx_distribution = model.calculate_amino_acid_distribution(processed_df1, 'Asx')\n",
        "        print(asx_distribution.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXc79ig5EKJ7",
        "outputId": "016917a2-5865-4d31-fb41-b3d1c9d70b41"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-05 18:08:25.235547 - INFO - Data loaded successfully from Google Sheet\n",
            "2024-07-05 18:08:25.238211 - ERROR - Unknown data format. Expected columns for eggshell: ['Pre-heat bleach time (h)', 'temp (°C)', 'pH', 'sample', 'time', '[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]', '[Phe]', '[Ile]', 'Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L', 'Phe D/L', 'Ile D/L'], or beta-lactoglobulin: ['Condition', 'Fraction', 'm']. Found columns: ['Pre-heat bleach time (h)', 'temp (°C)', 'pH', 'sample', 'time (h)', '[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]', '[Phe]', '[Ile]', 'Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L', 'Phe D/L', 'Ile D/L']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YT5-A1b5Nys5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Old stuff\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BdRqMIJmNs_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Global Variables and Constants\n",
        "# Precomputed activation energies in kJ/mol (converted to J/mol)\n",
        "activation_energies = {\n",
        "    'Asx_ConcTHAA': 111.09 * 1000, 'Asx_ConcFAA': 98.93 * 1000, 'Asx_DLTHAA': 126.02 * 1000, 'Asx_DLFAA': 121.99 * 1000,\n",
        "    'Glx_ConcTHAA': 105.46 * 1000, 'Glx_ConcFAA': 109.39 * 1000, 'Glx_DLTHAA': 121.89 * 1000, 'Glx_DLFAA': 100.80 * 1000,\n",
        "    'Ser_ConcTHAA': 127.59 * 1000, 'Ser_ConcFAA': 97.95 * 1000, 'Ser_DLTHAA': 123.03 * 1000, 'Ser_DLFAA': 116.72 * 1000,\n",
        "    'Ala_ConcTHAA': 96.58 * 1000, 'Ala_ConcFAA': 112.12 * 1000, 'Ala_DLTHAA': 113.58 * 1000, 'Ala_DLFAA': 98.88 * 1000,\n",
        "    'Val_ConcTHAA': 111.00 * 1000, 'Val_ConcFAA': 122.81 * 1000, 'Val_DLTHAA': 99.73 * 1000, 'Val_DLFAA': 128.52 * 1000,\n",
        "    'Phe_ConcTHAA': 113.28 * 1000, 'Phe_ConcFAA': 98.88 * 1000, 'Phe_DLTHAA': 128.15 * 1000, 'Phe_DLFAA': 102.27 * 1000,\n",
        "    'Leu_ConcTHAA': 114.09 * 1000, 'Leu_ConcFAA': 103.75 * 1000, 'Leu_DLTHAA': 105.43 * 1000, 'Leu_DLFAA': 105.43 * 1000,\n",
        "    'Ile_ConcTHAA': 114.09 * 1000, 'Ile_ConcFAA': 103.75 * 1000, 'Ile_DLTHAA': 105.43 * 1000, 'Ile_DLFAA': 105.43 * 1000\n",
        "}\n",
        "\n",
        "# Mapping from one-letter to three-letter amino acid codes\n",
        "one_to_three_letter = {\n",
        "    \"I\": \"Ile\", \"V\": \"Val\", \"L\": \"Leu\", \"F\": \"Phe\", \"C\": \"Cys\", \"M\": \"Met\",\n",
        "    \"A\": \"Ala\", \"G\": \"Gly\", \"T\": \"Thr\", \"W\": \"Trp\", \"S\": \"Ser\", \"Y\": \"Tyr\",\n",
        "    \"P\": \"Pro\", \"H\": \"His\", \"Q\": \"Gln\", \"E\": \"Glx\", \"N\": \"Asn\", \"D\": \"Asx\",\n",
        "    \"K\": \"Lys\", \"R\": \"Arg\", \"D\": \"Asx\", \"E\": \"Glx\"\n",
        "}\n",
        "\n",
        "# Mapping three-letter codes to concentration column names\n",
        "three_letter_to_Conc = {\n",
        "    'Ala': '[Ala]', 'Arg': '[Arg]', 'Asn': '[Asx]', 'Asp': '[Asx]', 'Cys': '[Cys]',\n",
        "    'Glu': '[Glx]', 'Gln': '[Glx]', 'Gly': '[Gly]', 'His': '[His]', 'Ile': '[Ile]',\n",
        "    'Leu': '[Leu]', 'Lys': '[Lys]', 'Met': '[Met]', 'Phe': '[Phe]', 'Pro': '[Pro]',\n",
        "    'Ser': '[Ser]', 'Thr': '[Thr]', 'Trp': '[Trp]', 'Tyr': '[Tyr]', 'Val': '[Val]'\n",
        "}\n",
        "\n",
        "\n",
        "# Pre-exponential factors for each type of rate\n",
        "pre_exponential_factors = {\n",
        "    'Asx': {'internal': 1e19, 'terminal': 1e20, 'free': 1e18, 'loss': 1e18},\n",
        "    'Glx': {'internal': 1e17, 'terminal': 1e20, 'free': 1e17, 'loss': 1e16},\n",
        "    'Ser': {'internal': 1e19, 'terminal': 1e20, 'free': 1e20, 'loss': 1e15},\n",
        "    'Ala': {'internal': 1e17, 'terminal': 1e17, 'free': 1e16, 'loss': 1e20},\n",
        "    'Val': {'internal': 1e17, 'terminal': 1e20, 'free': 1e15, 'loss': 1e10},\n",
        "    'Phe': {'internal': 1e17, 'terminal': 1e17, 'free': 1e20, 'loss': 1e15},\n",
        "    'Leu': {'internal': 1e19, 'terminal': 1e18, 'free': 1e13, 'loss': 1e20},\n",
        "    'Ile': {'internal': 1e16, 'terminal': 1e20, 'free': 1e18, 'loss': 1e20}\n",
        "}\n",
        "\n",
        "# Mapping from rate type descriptions to pre-exponential factor keys\n",
        "rate_type_mapping = {\n",
        "    'ConcTHAA': 'internal',\n",
        "    'ConcFAA': 'terminal',\n",
        "    'DLTHAA': 'free',\n",
        "    'DLFAA': 'loss'\n",
        "}\n",
        "\n",
        "# Universal gas constant in J/(mol·K)\n",
        "R = 8.314"
      ],
      "metadata": {
        "id": "jf7rA3hO_iV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions\n",
        "def arrhenius_rate_constant(E_a, T, A):\n",
        "    return A * np.exp(-E_a / (R * T))\n",
        "\n",
        "def adjust_saturation(color, amount=0.5):\n",
        "    color = mcolors.to_rgba(color)\n",
        "    color = mcolors.rgb_to_hsv(color[:3])\n",
        "    color[1] = max(0, min(1, color[1] * amount))\n",
        "    return mcolors.hsv_to_rgb(color)"
      ],
      "metadata": {
        "id": "3ePvMNXf_wOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Color Setup\n",
        "# Load color dictionary\n",
        "color_dict_path = '/content/drive/MyDrive/Colab_Notebooks/Dictionaries/Colours/colors.json'\n",
        "with open(color_dict_path, 'r') as file:\n",
        "    color_dict = json.load(file)\n",
        "\n",
        "amino_acid_colors = color_dict[\"amino_acids_colors\"]\n",
        "\n",
        "# Create dictionaries for three-letter codes and concentration column names\n",
        "amino_acid_colors_three_letter = {one_to_three_letter[k]: v for k, v in amino_acid_colors.items() if k in one_to_three_letter}\n",
        "three_letter_to_Conc = {k: f'[{k}]' for k in amino_acid_colors_three_letter.keys()}\n",
        "amino_acid_colors_conc = {three_letter_to_Conc[k]: v for k, v in amino_acid_colors_three_letter.items() if k in three_letter_to_Conc}\n",
        "\n",
        "# Adjust colors for standard deviations\n",
        "std_colors = {k: adjust_saturation(v, 0.5) for k, v in amino_acid_colors_three_letter.items()}\n"
      ],
      "metadata": {
        "id": "HvjBIyFW_5B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fixed Variables and Amino Acid Selection\n",
        "NUM_TIME_POINTS = 48\n",
        "amino_acid = 'Asx'\n",
        "initial_length = 1500\n",
        "temperature_kelvin = 353.15"
      ],
      "metadata": {
        "id": "1fOKagjh_2du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Load Data from Google Sheets\n",
        "sheet_id = \"1nA6jSAkAf1Ud-kHdaYTMtBPgKhe9nBg_IjM9idLlj8E\"\n",
        "gid = \"1259514505\"\n",
        "export_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(export_url)\n",
        "    print(\"Data successfully loaded into DataFrame!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: Could not access data. {e}\")\n",
        "\n",
        "# Ensure the columns match your Google Sheets structure\n",
        "expected_columns = [\n",
        "    'Pre-heat bleach time (h)', 'temp (°C)', 'pH', 'sample', 'time',\n",
        "    '[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]', '[Phe]', '[Ile]',\n",
        "    'Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L', 'Phe D/L', 'Ile D/L'\n",
        "]\n",
        "\n",
        "# Rename the column (assuming extracted time or existing time format)\n",
        "df = df.rename(columns={'time (h)': 'time'})\n",
        "\n",
        "# Ensure all expected columns are present in the data\n",
        "data = df[expected_columns]\n"
      ],
      "metadata": {
        "id": "iyvX2KV0AH9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Processing Functions\n",
        "def clean_data(df):\n",
        "    # Convert concentration and D/L columns to numeric, coerce errors to NaN\n",
        "    concentration_columns = ['[Asx]', '[Glx]', '[Ser]', '[Ala]', '[Val]', '[Phe]', '[Ile]']\n",
        "    dl_columns = ['Asx D/L', 'Glx D/L', 'Ser D/L', 'Ala D/L', 'Val D/L', 'Phe D/L', 'Ile D/L']\n",
        "\n",
        "    for col in concentration_columns + dl_columns:\n",
        "        df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Filter out rows with comments or non-numeric entries in critical columns\n",
        "    df = df.dropna(subset=concentration_columns + dl_columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_stats(df):\n",
        "    faa_df = df[df['sample'] == 'FAA']\n",
        "    thaa_df = df[df['sample'] == 'THAA']\n",
        "\n",
        "    grouped_faa = faa_df.groupby(['temp (°C)', 'time']).agg({\n",
        "        '[Asx]': ['mean', 'std', 'count'],\n",
        "        '[Glx]': ['mean', 'std', 'count'],\n",
        "        '[Ser]': ['mean', 'std', 'count'],\n",
        "        '[Ala]': ['mean', 'std', 'count'],\n",
        "        '[Val]': ['mean', 'std', 'count'],\n",
        "        '[Phe]': ['mean', 'std', 'count'],\n",
        "        '[Ile]': ['mean', 'std', 'count'],\n",
        "        'Asx D/L': ['mean', 'std', 'count'],\n",
        "        'Glx D/L': ['mean', 'std', 'count'],\n",
        "        'Ser D/L': ['mean', 'std', 'count'],\n",
        "        'Ala D/L': ['mean', 'std', 'count'],\n",
        "        'Val D/L': ['mean', 'std', 'count'],\n",
        "        'Phe D/L': ['mean', 'std', 'count'],\n",
        "        'Ile D/L': ['mean', 'std', 'count']\n",
        "    }).reset_index()\n",
        "\n",
        "    grouped_thaa = thaa_df.groupby(['temp (°C)', 'time']).agg({\n",
        "        '[Asx]': ['mean', 'std', 'count'],\n",
        "        '[Glx]': ['mean', 'std', 'count'],\n",
        "        '[Ser]': ['mean', 'std', 'count'],\n",
        "        '[Ala]': ['mean', 'std', 'count'],\n",
        "        '[Val]': ['mean', 'std', 'count'],\n",
        "        '[Phe]': ['mean', 'std', 'count'],\n",
        "        '[Ile]': ['mean', 'std', 'count'],\n",
        "        'Asx D/L': ['mean', 'std', 'count'],\n",
        "        'Glx D/L': ['mean', 'std', 'count'],\n",
        "        'Ser D/L': ['mean', 'std', 'count'],\n",
        "        'Ala D/L': ['mean', 'std', 'count'],\n",
        "        'Val D/L': ['mean', 'std', 'count'],\n",
        "        'Phe D/L': ['mean', 'std', 'count'],\n",
        "        'Ile D/L': ['mean', 'std', 'count']\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename columns to reflect FAA and THAA\n",
        "    new_columns_faa = ['temp (°C)', 'time'] + [f'{aa}_Conc_FAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']] + \\\n",
        "                      [f'{aa}_D/L_FAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']]\n",
        "    new_columns_thaa = ['temp (°C)', 'time'] + [f'{aa}_Conc_THAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']] + \\\n",
        "                       [f'{aa}_D/L_THAA_{stat}' for aa in ['Asx', 'Glx', 'Ser', 'Ala', 'Val', 'Phe', 'Ile'] for stat in ['Mean', 'Std', 'Count']]\n",
        "\n",
        "    grouped_faa.columns = new_columns_faa\n",
        "    grouped_thaa.columns = new_columns_thaa\n",
        "\n",
        "    for col in new_columns_faa:\n",
        "        if '_Std' in col:\n",
        "            count_col = col.replace('_Std', '_Count')\n",
        "            grouped_faa.loc[grouped_faa[count_col] == 1, col] = 0\n",
        "\n",
        "    for col in new_columns_thaa:\n",
        "        if '_Std' in col:\n",
        "            count_col = col.replace('_Std', '_Count')\n",
        "            grouped_thaa.loc[grouped_thaa[count_col] == 1, col] = 0\n",
        "\n",
        "    return grouped_faa, grouped_thaa\n",
        "\n",
        "def prepare_and_filter_data(df, amino_acid, temperature_kelvin):\n",
        "    relevant_suffixes = ['Mean', 'Std', 'DL_Mean', 'DL_Std']\n",
        "    relevant_columns = [\n",
        "        col for col in df.columns\n",
        "        if amino_acid in col and any(suffix in col for suffix in relevant_suffixes)\n",
        "    ]\n",
        "    relevant_columns.extend(['time', 'temp (K)'])\n",
        "\n",
        "    filtered_data = df[\n",
        "        (df['temp (K)'] == temperature_kelvin)\n",
        "    ][relevant_columns]\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "def calculate_amino_acid_distribution_with_loss(df, amino_acid):\n",
        "    amino_acid_distributions = []\n",
        "\n",
        "    initial_thaa = df[f\"{amino_acid}_Conc_THAA_Mean\"].iloc[0]\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        thaa = row[f\"{amino_acid}_Conc_THAA_Mean\"]\n",
        "        faa = row[f\"{amino_acid}_Conc_FAA_Mean\"]\n",
        "        thaa_dl = row[f\"{amino_acid}_D/L_THAA_Mean\"]\n",
        "        faa_dl = row[f\"{amino_acid}_D/L_FAA_Mean\"]\n",
        "        time_point = row[\"time\"]\n",
        "\n",
        "        # Calculate BAA\n",
        "        baa = thaa - faa\n",
        "        baa_d = baa * thaa_dl / (1 + thaa_dl)\n",
        "        baa_l = baa - baa_d\n",
        "\n",
        "        # Calculate FAA\n",
        "        faa_d = faa * faa_dl / (1 + faa_dl)\n",
        "        faa_l = faa - faa_d\n",
        "\n",
        "        # Calculate total loss\n",
        "        total_loss = max(initial_thaa - thaa, 0)\n",
        "\n",
        "        # Distribute total loss between D and L forms based on current FAA D/L ratio\n",
        "        if total_loss > 0:\n",
        "            loss_d = total_loss * faa_dl / (1 + faa_dl)\n",
        "            loss_l = total_loss - loss_d\n",
        "        else:\n",
        "            loss_d = 0\n",
        "            loss_l = 0\n",
        "\n",
        "        # Calculate standard deviations\n",
        "        thaa_std = row[f\"{amino_acid}_Conc_THAA_Std\"] if f\"{amino_acid}_Conc_THAA_Std\" in df.columns else 0\n",
        "        faa_std = row[f\"{amino_acid}_Conc_FAA_Std\"] if f\"{amino_acid}_Conc_FAA_Std\" in df.columns else 0\n",
        "\n",
        "        baa_std = np.sqrt(thaa_std**2 + faa_std**2)\n",
        "        baa_d_std = baa_std * thaa_dl / (1 + thaa_dl)\n",
        "        baa_l_std = baa_std - baa_d_std\n",
        "\n",
        "        faa_d_std = faa_std * faa_dl / (1 + faa_dl)\n",
        "        faa_l_std = faa_std - faa_d_std\n",
        "\n",
        "        loss_std = np.sqrt(2 * thaa_std**2)  # Using THAA std for loss calculation\n",
        "        loss_d_std = loss_std * faa_dl / (1 + faa_dl)\n",
        "        loss_l_std = loss_std - loss_d_std\n",
        "\n",
        "        amino_acid_distribution = {\n",
        "            \"time\": time_point,\n",
        "            \"temp (K)\": row[\"temp (K)\"],\n",
        "            \"BAA_D\": baa_d,\n",
        "            \"BAA_L\": baa_l,\n",
        "            \"BAA_D_Std\": baa_d_std,\n",
        "            \"BAA_L_Std\": baa_l_std,\n",
        "            \"FAA_D\": faa_d,\n",
        "            \"FAA_L\": faa_l,\n",
        "            \"FAA_D_Std\": faa_d_std,\n",
        "            \"FAA_L_Std\": faa_l_std,\n",
        "            \"FAA_D_loss\": loss_d,\n",
        "            \"FAA_L_loss\": loss_l,\n",
        "            \"FAA_D_loss_Std\": loss_d_std,\n",
        "            \"FAA_L_loss_Std\": loss_l_std,\n",
        "        }\n",
        "\n",
        "        amino_acid_distributions.append(amino_acid_distribution)\n",
        "\n",
        "    return pd.DataFrame(amino_acid_distributions)\n"
      ],
      "metadata": {
        "id": "t3_xMVe8Ba2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Processing and Preparation\n",
        "# Filter and clean data\n",
        "cleaned_data = clean_data(data)\n",
        "print(\" \")\n",
        "print(\"cleaned_data:--------------------------\")\n",
        "print(cleaned_data.head(3))\n",
        "\n",
        "# Calculate statistics\n",
        "faa_stats, thaa_stats = calculate_stats(cleaned_data)\n",
        "\n",
        "# Merge FAA and THAA statistics\n",
        "data_interpolated = pd.merge(faa_stats, thaa_stats, on=['temp (°C)', 'time'], how='outer')\n",
        "\n",
        "# Add 'temp (K)' column to the merged DataFrame\n",
        "data_interpolated['temp (K)'] = data_interpolated['temp (°C)'] + 273.15\n",
        "\n",
        "# Calculate BAA\n",
        "real_data = prepare_and_filter_data(data_interpolated, amino_acid, temperature_kelvin)\n",
        "\n",
        "print(\"real_data:--------------------------\")\n",
        "print(real_data.head(3))\n",
        "\n",
        "# Check prepared data\n",
        "real_DL = calculate_amino_acid_distribution_with_loss(real_data, amino_acid)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(\" \")\n",
        "print(\"real_DL:--------------------------\")\n",
        "print(real_DL.head(2))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "34IMttOZLmsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRFwukZn-8oE"
      },
      "outputs": [],
      "source": [
        "#@title Water Generation Calculation Functions\n",
        "def categorize_amino_acids(df):\n",
        "    concentration_columns = [col for col in df.columns if col.startswith('[') and col.endswith(']')]\n",
        "    categories = {\n",
        "        'THAA_dehyd': ['[Ala]'],  # Alanine for dehydration\n",
        "        'THAA_cyclic': ['[Glx]'],\n",
        "        'THAA_serine': ['[Ser]'],  # Separate category for Serine\n",
        "        'THAA_no_water': [col for col in concentration_columns if col not in ['[Ala]', '[Glx]', '[Ser]']]\n",
        "    }\n",
        "    return categories\n",
        "\n",
        "def initialize_concentrations(df, categories):\n",
        "    concentrations = {}\n",
        "\n",
        "    for category, amino_acids in categories.items():\n",
        "        category_dict = {}\n",
        "        for aa in amino_acids:\n",
        "            stats = df.groupby('time')[aa].agg(['mean', 'std']).reset_index()\n",
        "            category_dict[aa] = stats\n",
        "        concentrations[category] = category_dict\n",
        "\n",
        "    return concentrations\n",
        "\n",
        "def average_initial_concentration(data, column, num_points=3):\n",
        "    return data[column].iloc[:num_points].mean()\n",
        "\n",
        "def calculate_changes_in_concentration(data, initial_alanine, initial_serine):\n",
        "    time_points = data['time'].values\n",
        "    alanine_concentration = data['[Ala]'].values\n",
        "    serine_concentration = data['[Ser]'].values\n",
        "\n",
        "    change_in_alanine = alanine_concentration - initial_alanine\n",
        "    change_in_serine = serine_concentration - initial_serine\n",
        "\n",
        "    return time_points, change_in_alanine, change_in_serine\n",
        "\n",
        "def linear_fit(t, A, B):\n",
        "    return A * t + B\n",
        "\n",
        "def exponential_decay(t, A, k):\n",
        "    return A * np.exp(-k * t)\n",
        "\n",
        "def fit_linear_model(time_points, concentration):\n",
        "    popt, pcov = curve_fit(linear_fit, time_points, concentration['mean'], sigma=concentration['std'], absolute_sigma=True)\n",
        "    return popt, pcov\n",
        "\n",
        "def calculate_r_squared(y_true, y_pred):\n",
        "    residuals = y_true - y_pred\n",
        "    ss_res = np.sum(residuals ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "def calculate_water_generation(concentrations, time_points, categories):\n",
        "    water_generation = np.zeros(len(time_points))\n",
        "\n",
        "    for aa in categories['THAA_dehyd']:\n",
        "        initial_concentration = concentrations['THAA_dehyd'][aa]['mean'].iloc[0]\n",
        "        time_values = concentrations['THAA_dehyd'][aa]['time'].values\n",
        "        concentration_values = concentrations['THAA_dehyd'][aa]['mean'].values\n",
        "\n",
        "        time_values = pd.to_numeric(time_values, errors='coerce')\n",
        "        concentration_values = pd.to_numeric(concentration_values, errors='coerce')\n",
        "\n",
        "        valid_indices = ~np.isnan(time_values) & ~np.isnan(concentration_values)\n",
        "        time_values = time_values[valid_indices]\n",
        "        concentration_values = concentration_values[valid_indices]\n",
        "\n",
        "        concentration_values = np.interp(time_points, time_values, concentration_values)\n",
        "        for i, t in enumerate(time_points):\n",
        "            current_concentration = concentration_values[i]\n",
        "            water_generation[i] += max(initial_concentration - current_concentration, 0)\n",
        "\n",
        "    return water_generation\n",
        "\n",
        "def calculate_cyclic_water(concentrations, time_points, categories):\n",
        "    cyclic_water = np.zeros(len(time_points))\n",
        "\n",
        "    for i, t in enumerate(time_points):\n",
        "        for aa in categories['THAA_cyclic']:\n",
        "            initial_concentration = concentrations['THAA_cyclic'][aa]['mean'].iloc[0]\n",
        "            final_concentration = concentrations['THAA_cyclic'][aa]['mean'].iloc[-1]\n",
        "            cyclic_water[i] = max(initial_concentration - (initial_concentration - final_concentration) * (t / time_points[-1]), 0)\n",
        "\n",
        "    return cyclic_water\n",
        "\n",
        "def process_and_plot(cleaned_data, initial_length, amino_acid_colors):\n",
        "    categories = categorize_amino_acids(cleaned_data)\n",
        "    concentrations = initialize_concentrations(cleaned_data, categories)\n",
        "    initial_alanine_concentration = average_initial_concentration(cleaned_data, '[Ala]')\n",
        "    initial_serine_concentration = average_initial_concentration(cleaned_data, '[Ser]')\n",
        "    time_points, change_in_alanine, change_in_serine = calculate_changes_in_concentration(\n",
        "        cleaned_data, initial_alanine_concentration, initial_serine_concentration)\n",
        "\n",
        "    alanine_stats = cleaned_data.groupby('time')['[Ala]'].agg(['mean', 'std']).reset_index()\n",
        "    serine_stats = cleaned_data.groupby('time')['[Ser]'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "    valid_indices_alanine = np.isin(time_points, alanine_stats['time'])\n",
        "    filtered_time_points_alanine = time_points[valid_indices_alanine]\n",
        "    filtered_change_in_alanine = change_in_alanine[valid_indices_alanine]\n",
        "    filtered_alanine_stats = alanine_stats[alanine_stats['time'].isin(filtered_time_points_alanine)]\n",
        "\n",
        "    alanine_params, alanine_cov = fit_linear_model(filtered_alanine_stats['time'].values, filtered_alanine_stats)\n",
        "\n",
        "    valid_indices_serine = np.isin(time_points, serine_stats['time'])\n",
        "    filtered_time_points_serine = time_points[valid_indices_serine]\n",
        "    filtered_change_in_serine = change_in_serine[valid_indices_serine]\n",
        "    filtered_serine_stats = serine_stats[serine_stats['time'].isin(filtered_time_points_serine)]\n",
        "\n",
        "    serine_params, serine_cov = fit_linear_model(filtered_serine_stats['time'].values, filtered_serine_stats)\n",
        "\n",
        "    alanine_fit = linear_fit(filtered_time_points_alanine, *alanine_params)\n",
        "    serine_fit = linear_fit(filtered_time_points_serine, *serine_params)\n",
        "    alanine_r_squared = calculate_r_squared(filtered_change_in_alanine, alanine_fit)\n",
        "    serine_r_squared = calculate_r_squared(filtered_change_in_serine, serine_fit)\n",
        "\n",
        "    dehydration_rate = serine_params[0]\n",
        "    print(f\"Dehydration Rate: k = {dehydration_rate:.4f}\")\n",
        "\n",
        "    water_generation = calculate_water_generation(concentrations, time_points, categories)\n",
        "    cyclic_water = calculate_cyclic_water(concentrations, time_points, categories)\n",
        "    cumulative_water_release = np.cumsum(water_generation + cyclic_water)\n",
        "\n",
        "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(50, 10))\n",
        "\n",
        "    unique_times = cleaned_data['time'].unique()\n",
        "    bottom = np.zeros(len(unique_times))\n",
        "\n",
        "    for category in concentrations.values():\n",
        "        for aa, stats in category.items():\n",
        "            concentration = pd.to_numeric(stats['mean'].values, errors='coerce')\n",
        "            valid_indices = ~np.isnan(concentration)\n",
        "            concentration = concentration[valid_indices]\n",
        "            color = amino_acid_colors_conc.get(aa, 'gray')\n",
        "            ax1.fill_between(unique_times[valid_indices], bottom[valid_indices], bottom[valid_indices] + concentration, label=aa, color=color)\n",
        "            bottom[valid_indices] += concentration\n",
        "\n",
        "    ax1.set_xlabel('Time (hours)')\n",
        "    ax1.set_ylabel('Concentration')\n",
        "    ax1.set_title('Amino Acid Concentrations Over Time')\n",
        "    ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    ala_color = amino_acid_colors_conc.get('[Ala]', 'blue')\n",
        "    ser_color = amino_acid_colors_conc.get('[Ser]', 'green')\n",
        "    glx_color = amino_acid_colors_conc.get('[Glx]', 'red')\n",
        "\n",
        "    ax1.semilogx(time_points, water_generation, label='Water Generation (Ala)', color=ala_color)\n",
        "    ax1.semilogx(time_points, cyclic_water, label='Cyclic Water (Glx)', color=glx_color)\n",
        "    ax1.set_xlabel('Time (hours)')\n",
        "    ax1.set_ylabel('Water Concentration')\n",
        "    ax1.set_title('Water Generation and Cyclic Water Over Time')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.semilogx(time_points, water_generation, label='Water Generation (Ala)', color=ala_color)\n",
        "    ax2.semilogx(time_points, cyclic_water, label='Cyclic Water (Glx)', color=glx_color)\n",
        "    ax2.set_xlabel('Time (hours)')\n",
        "    ax2.set_ylabel('Water Concentration')\n",
        "    ax2.set_title('Water Generation and Cyclic Water Over Time')\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3.semilogx(filtered_time_points_alanine, filtered_change_in_alanine, 'o', label='Observed Decline in Alanine', color=ala_color)\n",
        "    ax3.semilogx(filtered_time_points_alanine, alanine_fit, '-', label=f'Fitted Alanine: A={alanine_params[0]:.2f}, B={alanine_params[1]:.2f}, $R^2$={alanine_r_squared:.2f}', color=ala_color)\n",
        "    ax3.semilogx(filtered_time_points_serine, filtered_change_in_serine, 'o', label='Observed Decline in Serine', color=ser_color)\n",
        "    ax3.semilogx(filtered_time_points_serine, serine_fit, '-', label=f'Fitted Serine: A={serine_params[0]:.2f}, B={serine_params[1]:.2f}, $R^2$={serine_r_squared:.2f}', color=ser_color)\n",
        "    ax3.set_xlabel('Time (hours)')\n",
        "    ax3.set_ylabel('Decline in Concentration')\n",
        "    ax3.set_title('Decline in Alanine and Serine Concentrations Over Time')\n",
        "    ax3.legend()\n",
        "\n",
        "    ax4.plot(time_points, water_generation, 'o', label='Observed Water Generation', color=ala_color)\n",
        "    ax4.plot(time_points, water_generation, '-', label='Fitted Water Generation', color=ala_color)\n",
        "    ax4.plot(time_points, cyclic_water, 'o', label='Observed Cyclic Water', color=glx_color)\n",
        "    ax4.plot(time_points, cyclic_water, '-', label='Fitted Cyclic Water', color=glx_color)\n",
        "    ax4.set_xlabel('Time (hours)')\n",
        "    ax4.set_ylabel('Water Concentration')\n",
        "    ax4.set_title('Observed and Fitted Water Generation and Cyclic Water')\n",
        "    ax4.legend()\n",
        "\n",
        "    ax5.plot(time_points, cumulative_water_release, 'o-', label='Cumulative Water Release', color='purple')\n",
        "    ax5.set_xlabel('Time (hours)')\n",
        "    ax5.set_ylabel('Cumulative Water Molecules')\n",
        "    ax5.set_title('Cumulative Water Release Over Time')\n",
        "    ax5.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return dehydration_rate\n",
        "\n",
        "def adjusted_rate_for_polymer_length(rate, initial_length):\n",
        "    return rate / initial_length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Process and plot water generation\n",
        "dehydration_rate = process_and_plot(cleaned_data, initial_length, amino_acid_colors)\n",
        "dehydration_rate_adjusted = adjusted_rate_for_polymer_length(dehydration_rate, initial_length)\n",
        "print(f\"Adjusted Serine Dehydration Rate: k = {dehydration_rate_adjusted:.4f}\")\n"
      ],
      "metadata": {
        "id": "76RnerxkM-cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Simulation Functions\n",
        "def second_order_hydrolysis(peptide_conc, water_conc, rate_constant, dt):\n",
        "    effective_rate = rate_constant * water_conc * dt\n",
        "    return peptide_conc * effective_rate / (1 + effective_rate)\n",
        "#@title Updated Simulation Functions\n",
        "\n",
        "def simulate_AAR(state, rates, dehydration_rate):\n",
        "    new_state = state.copy()\n",
        "\n",
        "    # 2nd order Hydrolysis\n",
        "    for source, target in [('int', 'term'), ('term', 'free')]:\n",
        "        if source == 'int':\n",
        "            total_fast = state['L_int_fast'] + state['D_int_fast']\n",
        "            total_slow = state['L_int_slow'] + state['D_int_slow']\n",
        "            total = total_fast + total_slow\n",
        "        else:\n",
        "            total = state[f'L_{source}'] + state[f'D_{source}']\n",
        "\n",
        "        if total > 1 and state['water'] > 0:  # Stop hydrolysis if total is 1 or less\n",
        "            effective_rate = rates[f'k_{source}'] * (state['water'] / (N * fold_water))\n",
        "\n",
        "            if source == 'int':\n",
        "                hydrolyzed_fast = np.random.binomial(total_fast, effective_rate)\n",
        "                hydrolyzed_slow = np.random.binomial(total_slow, rates['slow_hydrolysis_pool_relative_rate'])\n",
        "                total_hydrolyzed = hydrolyzed_fast + hydrolyzed_slow\n",
        "            else:\n",
        "                total_hydrolyzed = np.random.binomial(total, effective_rate)\n",
        "\n",
        "            for chirality in ['L', 'D']:e\n",
        "                if source == 'int':\n",
        "                    if total_fast > 0:\n",
        "                        moved_fast = int(hydrolyzed_fast * (state[f'{chirality}_{source}_fast'] / total_fast))\n",
        "                    else:\n",
        "                        moved_fast = 0\n",
        "\n",
        "                    if total_slow > 0:\n",
        "                        moved_slow = int(hydrolyzed_slow * (state[f'{chirality}_{source}_slow'] / total_slow))\n",
        "                    else:\n",
        "                        moved_slow = 0\n",
        "\n",
        "                    new_state[f'{chirality}_{source}_fast'] -= moved_fast\n",
        "                    new_state[f'{chirality}_{source}_slow'] -= moved_slow\n",
        "                    new_state[f'{chirality}_{target}'] += moved_fast + moved_slow\n",
        "                else:\n",
        "                    moved = int(total_hydrolyzed * (state[f'{chirality}_{source}'] / total))\n",
        "                    new_state[f'{chirality}_{source}'] -= moved\n",
        "                    new_state[f'{chirality}_{target}'] += moved\n",
        "\n",
        "            new_state['water'] -= total_hydrolyzed\n",
        "\n",
        "    # Racemization\n",
        "    for location in ['int_fast', 'int_slow', 'term', 'free']:\n",
        "        if location.startswith('int'):\n",
        "            total = state[f'L_{location}'] + state[f'D_{location}']\n",
        "        else:\n",
        "            total = state[f'L_{location}'] + state[f'D_{location}']\n",
        "\n",
        "        if total > N * 0.005:  # Stop racemization if total is less than 0.05%\n",
        "            rate = rates[f'racemization_rate_{location.split(\"_\")[0]}']\n",
        "            if location == 'int_slow':\n",
        "                rate = rates['slow_racemization_rate_polymer_rate']\n",
        "\n",
        "            for source, target in [('L', 'D'), ('D', 'L')]:\n",
        "                racemized = np.random.binomial(state[f'{source}_{location}'], rate)\n",
        "                new_state[f'{source}_{location}'] -= racemized\n",
        "                new_state[f'{target}_{location}'] += racemized\n",
        "\n",
        "    # Loss of free amino acids\n",
        "    for chirality in ['L', 'D']:\n",
        "        lost = np.random.binomial(state[f'{chirality}_free'], rates['k_loss'])\n",
        "        new_state[f'{chirality}_free'] -= lost\n",
        "\n",
        "    # Add water back into the simulation\n",
        "    new_state['water'] += dehydration_rate\n",
        "\n",
        "    return new_state\n",
        "\n",
        "def run_simulation(initial_state, rates, time_steps, dehydration_rate):\n",
        "    results = {key: [value] for key, value in initial_state.items()}\n",
        "    results['time'] = [0]\n",
        "\n",
        "    state = initial_state.copy()\n",
        "    for t in range(1, time_steps + 1):\n",
        "        state = simulate_AAR(state, rates, dehydration_rate)\n",
        "        for key, value in state.items():\n",
        "            results[key].append(value)\n",
        "        results['time'].append(t)\n",
        "\n",
        "    return results\n",
        "\n",
        "def calculate_ratios(results):\n",
        "    ratios = {}\n",
        "    for location in ['int_fast', 'int_slow', 'term', 'free']:\n",
        "        L = np.array(results[f'L_{location}'])\n",
        "        D = np.array(results[f'D_{location}'])\n",
        "        ratios[f'{location}_D_L_ratio'] = np.divide(D, L, where=L != 0)\n",
        "\n",
        "    total_L = sum(np.array(results[f'L_{loc}']) for loc in ['int_fast', 'int_slow', 'term', 'free'])\n",
        "    total_D = sum(np.array(results[f'D_{loc}']) for loc in ['int_fast', 'int_slow', 'term', 'free'])\n",
        "    ratios['overall_D_L_ratio'] = np.divide(total_D, total_L, where=total_L != 0)\n",
        "\n",
        "    return ratios\n",
        "\n"
      ],
      "metadata": {
        "id": "iCTadFlNCSSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title plotting siumlation\n",
        "\n",
        "# Define colormaps using the correct module and method\n",
        "d_colormap = plt.colormaps['tab20b']\n",
        "l_colormap = plt.colormaps['tab20b']\n",
        "\n",
        "# Function to get color based on D/L and lightness\n",
        "def get_color(is_d, is_predicted, lightness):\n",
        "    if is_d:\n",
        "        colormap = d_colormap\n",
        "    else:\n",
        "        colormap = l_colormap\n",
        "    return colormap(lightness)\n",
        "\n",
        "def plot_results(results, ratios, real_DL):\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Plot internal amino acids (combining fast and slow fractions)\n",
        "    ax = axs[0, 0]\n",
        "    ax.plot(results['time'], np.array(results['L_int_fast']) + np.array(results['L_int_slow']), label='Internal L')\n",
        "    ax.plot(results['time'], np.array(results['D_int_fast']) + np.array(results['D_int_slow']), label='Internal D')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Number of Amino Acids')\n",
        "    ax.set_title('Internal Amino Acids')\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot terminal amino acids\n",
        "    ax = axs[0, 1]\n",
        "    ax.plot(results['time'], results['L_term'], label='Terminal L')\n",
        "    ax.plot(results['time'], results['D_term'], label='Terminal D')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Number of Amino Acids')\n",
        "    ax.set_title('Terminal Amino Acids')\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot free amino acids\n",
        "    ax = axs[0, 2]\n",
        "    ax.plot(results['time'], results['L_free'], label='Free L')\n",
        "    ax.plot(results['time'], results['D_free'], label='Free D')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Number of Amino Acids')\n",
        "    ax.set_title('Free Amino Acids')\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot D/L ratios\n",
        "    ax = axs[1, 0]\n",
        "    ax.plot(results['time'], ratios['int_fast_D_L_ratio'], label='Internal Fast')\n",
        "    ax.plot(results['time'], ratios['int_slow_D_L_ratio'], label='Internal Slow')\n",
        "    ax.plot(results['time'], ratios['term_D_L_ratio'], label='Terminal')\n",
        "    ax.plot(results['time'], ratios['free_D_L_ratio'], label='Free')\n",
        "    ax.plot(results['time'], ratios['overall_D_L_ratio'], label='Overall', linewidth=2)\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('D/L Ratio')\n",
        "    ax.set_title('D/L Ratios')\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot water consumption\n",
        "    ax = axs[1, 1]\n",
        "    ax.plot(results['time'], results['water'], label='Water Molecules')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Number of Water Molecules')\n",
        "    ax.set_title('Water Consumption')\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot total amino acids\n",
        "    ax = axs[1, 2]\n",
        "    total_amino_acids = (np.array(results['L_int_fast']) + np.array(results['D_int_fast']) +\n",
        "                         np.array(results['L_int_slow']) + np.array(results['D_int_slow']) +\n",
        "                         np.array(results['L_term']) + np.array(results['D_term']) +\n",
        "                         np.array(results['L_free']) + np.array(results['D_free']))\n",
        "    ax.plot(results['time'], total_amino_acids, label='Total Amino Acids')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Number of Amino Acids')\n",
        "    ax.set_title('Total Amino Acids')\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot real_DL experimental data\n",
        "    axs[0, 0].scatter(real_DL['time'], real_DL['FAA_D'], label='FAA_D (real)', color=get_color(True, False, 0.7))\n",
        "    axs[0, 0].scatter(real_DL['time'], real_DL['FAA_L'], label='FAA_L (real)', color=get_color(False, False, 0.7))\n",
        "    axs[0, 0].scatter(real_DL['time'], real_DL['BAA_D'], label='BAA_D (real)', color=get_color(True, False, 0.6))\n",
        "    axs[0, 0].scatter(real_DL['time'], real_DL['BAA_L'], label='BAA_L (real)', color=get_color(False, False, 0.6))\n",
        "    axs[0, 0].scatter(real_DL['time'], real_DL['FAA_D_loss'] + real_DL['FAA_L_loss'], label='Loss', color='grey')\n",
        "\n",
        "    thaa_dl_real = (real_DL['FAA_D'] + real_DL['BAA_D']) / (real_DL['FAA_L'] + real_DL['BAA_L'])\n",
        "    axs[0, 0].scatter(real_DL['time'], thaa_dl_real, label='THAA D/L (real)', color=get_color(True, False, 0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T4I0y5BRqIFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Parameters\n",
        "N = 50000  # Initial number of amino acids\n",
        "fold_water = 8  # Initial water concentration relative to amino acids\n",
        "k_internal = 0.04  # Rate constant for internal to terminal hydrolysis\n",
        "k_terminal = 0.01  # Rate constant for terminal to free hydrolysis\n",
        "k_loss = 0.001  # Rate of free amino acid loss\n",
        "racemization_rate_polymer = 0.001  # Racemization rate for internal amino acids\n",
        "racemization_rate_terminal = 0.002  # Racemization rate for terminal amino acids\n",
        "racemization_rate_free = 0.01  # Racemization rate for free amino acids\n",
        "time_steps = 1000  # Number of time steps for the simulation\n",
        "dt = 1  # Time step size\n",
        "user_defined_max_time = 6000  # Example of user-defined max_time\n",
        "# Set the number of time steps\n",
        "number_of_steps = 1000\n",
        "\n",
        "# New parameters for slow fractions\n",
        "slow_internal_hydrolysis_fraction = 0.5\n",
        "slow_internal_hydrolysis_rate = 0.01\n",
        "slow_racemization_rate_polymer_fraction = 0.5\n",
        "slow_racemization_rate_polymer_rate = 0.001\n",
        "\n",
        "# Initial conditions\n",
        "initial_state = {\n",
        "    'L_int_fast': int(N * (1 - slow_internal_hydrolysis_fraction)),\n",
        "    'L_int_slow': int(N * slow_internal_hydrolysis_fraction),\n",
        "    'D_int_fast': 0,\n",
        "    'D_int_slow': 0,\n",
        "    'L_term': 0,\n",
        "    'D_term': 0,\n",
        "    'L_free': 0,\n",
        "    'D_free': 0,\n",
        "    'water': N * fold_water  # Initial water molecules\n",
        "}\n",
        "\n",
        "rate_params = {\n",
        "    'k_internal': 0.04,\n",
        "    'k_terminal': 0.02,\n",
        "    'k_loss': 0.001,\n",
        "    'fixed_rate_free': 0.005,\n",
        "    'slow_hydrolysis_pool_size': 0.5,\n",
        "    'slow_hydrolysis_pool_relative_rate': 0.00013,\n",
        "    'slow_DL_terminal_pool_size': 0.5,\n",
        "    'slow_DL_terminal_pool_relative_rate': 0.1,\n",
        "    'slow_DL_internal_pool_size': 0.34762483392850024,\n",
        "    'racemization_rate_polymer': 0.001,\n",
        "    'racemization_rate_terminal': 0.002,\n",
        "    'racemization_rate_free': 0.01,\n",
        "}\n",
        "\n",
        "rates = {\n",
        "    'k_int': k_internal * dt,\n",
        "    'k_term': k_terminal * dt,\n",
        "    'k_loss': k_loss * dt,\n",
        "    'racemization_rate_int': racemization_rate_polymer * dt,\n",
        "    'racemization_rate_term': racemization_rate_terminal * dt,\n",
        "    'racemization_rate_free': racemization_rate_free * dt,\n",
        "    'slow_hydrolysis_pool_relative_rate': slow_internal_hydrolysis_rate * dt,\n",
        "    'slow_racemization_rate_polymer_rate': slow_racemization_rate_polymer_rate * dt\n",
        "}"
      ],
      "metadata": {
        "id": "VvqKtC5rB3_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Updated Simulation\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the maximum time\n",
        "max_time = min(real_DL['time'].max(), user_defined_max_time)\n",
        "\n",
        "# Calculate the time step size\n",
        "dt = max_time / number_of_steps\n",
        "\n",
        "# Update rate parameters\n",
        "rate_params['k_loss'] = serine_dehydration_rate_adjusted\n",
        "\n",
        "# Convert rate parameters to rates used in simulation\n",
        "rates = {key: value * dt for key, value in rate_params.items()}\n",
        "\n",
        "# Run simulation\n",
        "results = run_simulation(initial_state, rates, number_of_steps, dehydration_rate)\n",
        "ratios = calculate_ratios(results)\n",
        "plot_results(results, ratios, real_DL)"
      ],
      "metadata": {
        "id": "cXyHdXR95NKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run simulation\n",
        "results = run_simulation(initial_state, rates, time_steps)\n",
        "ratios = calculate_ratios(results)\n",
        "plot_results(results, ratios, real_DL)"
      ],
      "metadata": {
        "id": "h41tCDz1KSHw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}